bench_models.py
==================================================
# Minimal bench_models stub for local runs
from __future__ import annotations

def calculate_predicted_flight_constraint(horizontal_mps: float, vertical_mps: float, weight_n: float) -> float:
    """Return a simple predicted flight constraint (watts) approximation.

    This is a lightweight stub so the automation scripts can run without
    the full research model dependency. It approximates required power as
    proportional to horizontal and vertical speed scaled by weight.
    """
    try:
        h = float(abs(horizontal_mps))
        v = float(abs(vertical_mps))
        w = float(max(0.0, weight_n))
    except Exception:
        return 0.0
    # simple physics-inspired proxy: power ~ weight * speed * factor
    speed = (h ** 2 + v ** 2) ** 0.5
    factor = 0.1
    return float(w * speed * factor)

==================================================

check_oqs.py
==================================================
import sys
try:
    import oqs
    print(f"oqs version: {oqs.oqs_version()}")
    print(f"oqs-python version: {oqs.__version__}")
    from oqs import Signature
    print("Signature class available")
    sig = Signature("Falcon-512")
    print("Falcon-512 instantiated")
except Exception as e:
    print(f"Error: {e}")
    sys.exit(1)

==================================================

config.remote.py
==================================================
"""
Core configuration constants for PQC drone-GCS secure proxy.

Single source of truth for all network ports, hosts, and runtime parameters.
"""

import os
from ipaddress import ip_address
from typing import Dict, Any
from core.exceptions import ConfigError


# Baseline host defaults reused throughout the configuration payload.
# Keep both LAN and Tailscale addresses handy so schedulers can pin the
# appropriate interface per testbed. Defaults target the LAN endpoints.
# Localhost-only topology override for smoke/local tests can be applied
# by temporarily pointing *_HOST_LAN values at 127.0.0.1. For normal
# lab runs, keep these set to the actual LAN-facing addresses.
_DRONE_HOST_LAN = "192.168.0.105"   # uavpi drone LAN IP (wlan0 from `ip addr`)
_DRONE_HOST_TAILSCALE = "100.101.93.23"
_GCS_HOST_LAN = "192.168.0.101"    # GCS Windows LAN IP (from ipconfig)
_GCS_HOST_TAILSCALE = "100.106.181.122"

# Default to LAN hosts for operational runs (Tailscale kept for SSH only)
_DEFAULT_DRONE_HOST = _DRONE_HOST_LAN
_DEFAULT_GCS_HOST = _GCS_HOST_LAN

# Environment-sourced default credential to avoid embedding lab passwords in source control.
_LAB_PASSWORD_DEFAULT = os.getenv("PQC_LAB_PASSWORD", "uavpi")


# Default configuration - all required keys with correct types
CONFIG = {
    # Handshake (TCP)
    "TCP_HANDSHAKE_PORT": 46000,

    # Encrypted UDP data-plane (network)
    "UDP_DRONE_RX": 46012,   # drone binds here; GCS sends here
    "UDP_GCS_RX": 46011,     # gcs binds here; Drone sends here

    # Plaintext UDP (local loopback to apps/FC)
    "DRONE_PLAINTEXT_TX": 47003,  # app→drone-proxy (to encrypt out)
    "DRONE_PLAINTEXT_RX": 47004,  # drone-proxy→app (after decrypt)
    "GCS_PLAINTEXT_TX": 47001,    # app→gcs-proxy
    "GCS_PLAINTEXT_RX": 47002,    # gcs-proxy→app
    # Use localhost for plaintext bindings to ensure compatibility with local MAVProxy
    "DRONE_PLAINTEXT_HOST": "127.0.0.1",
    "GCS_PLAINTEXT_HOST": "127.0.0.1",

    # Hosts
    "DRONE_HOST": _DEFAULT_DRONE_HOST,
    "GCS_HOST": _DEFAULT_GCS_HOST,
    "DRONE_HOST_LAN": _DRONE_HOST_LAN,
    "DRONE_HOST_TAILSCALE": _DRONE_HOST_TAILSCALE,
    "GCS_HOST_LAN": _GCS_HOST_LAN,
    "GCS_HOST_TAILSCALE": _GCS_HOST_TAILSCALE,

    # Pre-shared key (hex) for drone authentication during handshake.
    # Default is a placeholder; override in production via environment variable.
    # Intentionally default to empty; require injection via environment in non-dev.
    "DRONE_PSK": "",

    # Crypto/runtime
    "REPLAY_WINDOW": 1024,
    "WIRE_VERSION": 1,      # header version byte (frozen)
    # Allow slower suites to finish the rekey handshake without timing out
    "REKEY_HANDSHAKE_TIMEOUT": 45.0,

    # --- Bare scheduler defaults (scheduler/bare/*) ---
    # Dwell time per suite before automatic rotation (seconds).
    # Both drone_follower and gcs_scheduler read this for consistency.
    "BARE_SUITE_DWELL_S": 10.0,
    # Confirmation timeout for local proxy state change after rekey request.
    "BARE_CONFIRM_TIMEOUT_S": 10.0,
    # Poll interval for status checks during dwell period.
    "BARE_POLL_INTERVAL_S": 2.0,

    # --- Optional hardening / QoS knobs (NOT required; safe defaults) ---
    # Limit TCP handshake attempts accepted per IP at the GCS (server) side.
    # Model: token bucket; BURST tokens max, refilling at REFILL_PER_SEC tokens/sec.
    "HANDSHAKE_RL_BURST": 5,
    "HANDSHAKE_RL_REFILL_PER_SEC": 1,

    # Mark encrypted UDP with DSCP EF (46) to prioritize on WMM-enabled APs.
    # Set to None to disable. Implementation multiplies by 4 to form TOS.
    "ENCRYPTED_DSCP": 46,

    # Feature flag: if True, proxy prefixes app->proxy plaintext with 1 byte packet type.
    # 0x01 = MAVLink/data (forward to local app); 0x02 = control (route to policy engine).
    # When False (default), proxy passes bytes unchanged (backward compatible).
    "ENABLE_PACKET_TYPE": True,

    # Enable exposure of ASCON AEAD variants in suite registry and runtime probing.
    # ENABLE_ASCON gates all Ascon tokens; ENABLE_ASCON128A further enables the 'ascon128a'
    # variant (kept experimental). Both default False to preserve legacy test matrix unless
    # explicitly activated via environment variables.
    # LOCAL TEST OVERRIDE: enable Ascon variants for extended AEAD smoke coverage.
    "ENABLE_ASCON": True,
    "ENABLE_ASCON128A": True,

    # Enforce 16-byte key usage for ASCON-128 when enabled. Default False preserves
    # legacy behaviour while exposing the knob via CONFIG/env overrides.
    "ASCON_STRICT_KEY_SIZE": False,

    # HMAC auth shared secret for MAV schedulers (string, UTF-8). Override via env MAV_AUTH_KEY.
    "MAV_AUTH_KEY": "",
    # Optional allow list for MAV scheduler control channels. Accepts list/tuple or comma string.
    "MAV_ALLOWED_SENDERS": [],

    # Direct MAVProxy wiring defaults for lab setups. These are used by
    # auto/mav helpers to build a plain Pixhawk↔GCS MAVLink path that is
    # independent of the PQC proxy. Adjust these if wiring or tools move.
    "MAV_FC_DEVICE": "/dev/ttyACM0",  # Pixhawk USB serial on the drone Pi
    "MAV_FC_BAUD": 57600,              # Pixhawk serial baud rate
    # GCS-side UDP ports where the drone sends MAVLink (LAN-facing).
    "MAV_GCS_IN_PORT_1": 14550,
    "MAV_GCS_IN_PORT_2": 14551,
    # GCS MAVProxy bind host for incoming MAVLink from the drone.
    "MAV_GCS_LISTEN_HOST": "0.0.0.0",
    # Local loopback host/ports on the GCS for ground tools (e.g., QGC).
    "MAV_LOCAL_HOST": "127.0.0.1",
    "MAV_LOCAL_OUT_PORT_1": 14550,
    "MAV_LOCAL_OUT_PORT_2": 14551,
    "MAVPROXY_BINARY": "/home/dev/cenv/bin/mavproxy.py",

    # Explicit drone host/port for client-style GCS master (two-way heartbeat).
    # Using explicit remote prevents passive listener stalls on some platforms.
    "MAV_DRONE_HOST": _DEFAULT_DRONE_HOST,
    "MAV_DRONE_UDP_PORT": 14550,

    # Enforce strict matching of encrypted UDP peer IP/port with the authenticated handshake peer.
    # Disable (set to False) only when operating behind NAT where source ports may differ.
    "STRICT_UDP_PEER_MATCH": True,
    "STRICT_HANDSHAKE_IP": True,

    # Log real session IDs only when explicitly enabled (default False masks them to hashes).
    "LOG_SESSION_ID": False,

    # --- Simple automation defaults (tools/auto/*_simple.py) ---
    # Optional: enable TCP JSON control listener inside the core proxy.
    # When enabled, each side may expose a listener (DRONE_CONTROL_* / GCS_CONTROL_*).
    # Only the configured CONTROL_COORDINATOR_ROLE will accept "cmd":"rekey";
    # non-coordinator nodes respond with coordinator_only.
    "ENABLE_TCP_CONTROL": False,
    # Rekey coordinator/dominator role for the in-band two-phase commit.
    # This does NOT change the TCP handshake roles (GCS still serves, drone still connects);
    # it only defines which side initiates prepare/commit and triggers the rekey handshake.
    # Allowed: "gcs" (default, legacy) or "drone".
    "CONTROL_COORDINATOR_ROLE": "gcs",
    # Control server bind host for the drone follower. Default to the
    # configured drone host (LAN) so remote GCS schedulers can reach
    # the control RPCs by default. Use env override `DRONE_CONTROL_HOST`
    # for special cases (loopback-only testing).
    "DRONE_CONTROL_HOST": _DEFAULT_DRONE_HOST,
    "DRONE_CONTROL_PORT": 48080,
    # Optional control listener settings for the GCS host.
    # Used by core's TCP control server (when enabled) and by tooling.
    # Bind to 0.0.0.0 by default to accept local + remote commands.
    "GCS_CONTROL_HOST": "0.0.0.0",
    "GCS_CONTROL_PORT": 48080,
        # Encrypted-plane control channel used by certain schedulers to route
        # drone-originated control/status back to the GCS when ENABLE_PACKET_TYPE is set.
        # Keep distinct from the plaintext follower RPC port to avoid conflicts.
     "DRONE_TO_GCS_CTL_PORT": 48181,
    "SIMPLE_VERIFY_TIMEOUT_S": 5.0,
    "SIMPLE_PACKETS_PER_SUITE": 1,
    "SIMPLE_PACKET_DELAY_S": 0.0,
    "SIMPLE_SUITE_DWELL_S": 0.0,
    # Default initial suite used by simple automation drivers and schedulers.
    # Keep suite IDs centralized in core.suites; tooling should fall back to it.
    "SIMPLE_INITIAL_SUITE": None,

    # Primitive benchmark coverage lists used by metrics tools and tests.
    # Keep these aligned with supported algorithms on target hardware.
    "PRIMITIVE_TEST_KEMS": [
        "ML-KEM-768",
        "Kyber512",
        "HQC-192",
    ],
    "PRIMITIVE_TEST_SIGS": [
        "ML-DSA-65",
        "Falcon-512",
        "SPHINCS+-SHA2-128s-simple",
    ],
    "PRIMITIVE_TEST_AEADS": [
        "aesgcm",
        "chacha20poly1305",
    ],
    # Automation defaults for tools/auto orchestration scripts
    "AUTO_DRONE": {
        # Session IDs default to "<prefix>_<unix>" unless DRONE_SESSION_ID env overrides
        "session_prefix": "run",
        # Optional explicit initial suite override (None -> defer to tooling defaults).
        "initial_suite": None,
        # Enable follower monitors (perf/pidstat/psutil) by default
        "monitors_enabled": True,
        # Apply CPU governor tweaks unless disabled
        "cpu_optimize": True,
        # Enable telemetry publisher back to the scheduler
        "telemetry_enabled": True,
        # Optional explicit telemetry host/port (None -> derive from CONTROL_HOST defaults)
        "telemetry_host": None,
        "telemetry_port": 52080,
        # Override monitoring output base directory (None -> DEFAULT_MONITOR_BASE)
        "monitor_output_base": None,
        # Optional environment exports applied before creating the power monitor
        "power_env": {
            # Maintain 1 kHz sampling by default; backend remains auto unless overridden
            "DRONE_POWER_BACKEND": "ina219",
            "DRONE_POWER_SAMPLE_HZ": "1000",
            "INA219_I2C_BUS": "1",
            "INA219_ADDR": "0x40",
            "INA219_SHUNT_OHM": "0.1",
        },
        # Synthetic flight model defaults used when power telemetry
        # translates PQC utilization into flight endurance estimates.
        "mock_mass_kg": 6.5,
        "kinematics_horizontal_mps": 13.0,
        "kinematics_vertical_mps": 3.5,
        "kinematics_cycle_s": 18.0,
        "kinematics_yaw_rate_dps": 45.0,
        # Toggle MAVProxy launch vs. legacy UDP echo helper.
        # MAVProxy stays enabled by default to keep parity with lab setups.
        "mavproxy_enabled": True,
        "udp_echo_enabled": False,
    },

    "AUTO_GCS": {
        # Session IDs default to "<prefix>_<unix>" unless GCS_SESSION_ID env overrides
        "session_prefix": "run",  # string prefix for run IDs
        # Traffic profile: "blast", "constant", "mavproxy", or "saturation"
        "traffic": "constant",  # modes: constant|blast|mavproxy|saturation
        # Traffic engine: "native" (built-in blaster) or "iperf3" (external client)
    "traffic_engine": "native",  # generator: native|iperf3
        # Duration for active traffic window per suite (seconds)
        # For cores+DVFS sweeps we default to short, aggressive 10s windows.
        "duration_s": 10.0,  # positive float seconds
        # Delay after rekey before starting traffic (seconds)
        "pre_gap_s": 1.0,  # non-negative float seconds (keep a short warmup)
        # Delay between suites (seconds). Shorten for faster cores+DVFS sweeps.
        "inter_gap_s": 5.0,  # non-negative float seconds
        # UDP payload size (bytes) for blaster calculations
        # Use a near-MTU payload to stress the data plane.
        "payload_bytes": 1200,  # payload bytes (>0)
        # Sample every Nth send/receive event (0 disables)
        "event_sample": 100,  # packets between samples (>=0)
        # Number of full passes across suite list. For DVFS sweeps, set this
        # to match the number of DVFS combos (e.g. 13 for 600-1800 MHz in
        # 100 MHz steps) so each pass runs the full suite set at a fixed
        # CPU frequency.
        "passes": 13,  # positive integer
        # Explicit packets-per-second override; 0 means best-effort
        "rate_pps": 0,  # packets/sec (>=0)
        # Optional bandwidth target in Mbps (converted to PPS if > 0)
        # Default to ~10 Mbps to exercise realistic airlink load.
        "bandwidth_mbps": 10.0,  # Mbps target (>=0)
        # Max rate explored during saturation sweeps (Mbps)
        "max_rate_mbps": 200.0,  # saturation upper bound Mbps (>0)
        # Optional ordered suite subset (None -> all suites from core.suites, including ChaCha20-Poly1305 and ASCON variants)
        # Set to None to run the full suite matrix
    "suites": None,
        # Launch local GCS proxy under scheduler control
        "launch_proxy": True,  # bool controls local proxy launch
        # Enable local proxy monitors (perf/pidstat/psutil)
        "monitors_enabled": True,  # bool controlling monitor sidecars
        # Start telemetry collector on the scheduler side
        "telemetry_enabled": True,  # bool gating telemetry collector
        # Bind/port for telemetry collector (defaults to CONFIG values)
        "telemetry_bind_host": "0.0.0.0",  # bind address string
        "telemetry_port": 52080,  # telemetry listen port (1-65535)
        # Emit combined Excel workbook when run completes
        "export_combined_excel": True,  # bool to generate combined workbook
        # Optional iperf3 configuration used when traffic_engine == "iperf3"
        "iperf3": {
            "server_host": None,  # override iperf3 server host or None for default
            "server_port": 5201,  # iperf3 UDP port (1-65535)
            "binary": "iperf3",  # iperf3 executable path/name
            "extra_args": [],  # additional CLI args list
            "force_cli": False,  # bool to force CLI output mode
        },
    # Blocklist of AEAD tokens to exclude from automation runs (case-insensitive)
    "aead_exclude_tokens": [],
            # Optional post-run fetch of drone artifacts (logs, power captures)
            "post_fetch": {
                # Legacy remote-fetch pipeline is disabled; artifacts must be synced via Git.
                "enabled": False,
                "host": _DEFAULT_DRONE_HOST,
                "username": "dev",
                "password": os.getenv("AUTO_GCS_POST_FETCH_PASSWORD", _LAB_PASSWORD_DEFAULT),
                "key": None,
                "strategy": "disabled",
                "port": 22,
                "logs_remote": "~/research/logs/auto/drone",
                "logs_local": "logs/auto",
                "output_remote": "~/research/output/drone",
                "output_local": "output/drone",
            },
            # Enable remote power fetch and set the SCP/SFTP target
        # Power fetch now relies on locally synced artifacts instead of remote copy.
        "power_fetch_enabled": False,
        "power_fetch_target": f"dev@{_DEFAULT_DRONE_HOST}",
        "artifact_fetch_strategy": "auto",  # Default fetch strategy for artifacts (auto selects best available)
        "post_report": {
            "enabled": True,  # bool toggling post-run report generation
            "script": "tools/report_constant_run.py",  # reporting script path
            "output_dir": "output/gcs",  # base output directory
            "table_name": "run_summary_table.md",  # Markdown table filename
            "text_name": "run_suite_summaries.txt",  # narrative summary filename
        },
        # Non-interactive SFTP password for POWER fetch (used by gcs_scheduler._sftp_fetch)
        # Set to None to prefer key/agent-based auth. For development convenience we
        # populate it here; in production prefer using an SSH agent or per-run env var.
    "power_fetch_password": os.getenv("AUTO_GCS_POWER_FETCH_PASSWORD", _LAB_PASSWORD_DEFAULT),
        # Optional explicit private key for power fetch operations (overrides agent lookup)
        "power_fetch_key": None,
    },

    # Allow plaintext host bindings to be non-loopback by default so LAN runners work
    # without env overrides. Set to False to force loopback-only bindings.
    "ALLOW_NON_LOOPBACK_PLAINTEXT": True,
}


# Required keys with their expected types
_REQUIRED_KEYS = {
    "TCP_HANDSHAKE_PORT": int,
    "UDP_DRONE_RX": int,
    "UDP_GCS_RX": int,
    "DRONE_PLAINTEXT_TX": int,
    "DRONE_PLAINTEXT_RX": int,
    "GCS_PLAINTEXT_TX": int,
    "GCS_PLAINTEXT_RX": int,
    "DRONE_HOST": str,
    "GCS_HOST": str,
    "DRONE_PLAINTEXT_HOST": str,
    "GCS_PLAINTEXT_HOST": str,
    "REPLAY_WINDOW": int,
    "WIRE_VERSION": int,
    "ENABLE_PACKET_TYPE": bool,
    "ENABLE_ASCON": bool,
    "ENABLE_ASCON128A": bool,
    "STRICT_UDP_PEER_MATCH": bool,
    "STRICT_HANDSHAKE_IP": bool,
    "LOG_SESSION_ID": bool,
    "DRONE_PSK": str,
    "REKEY_HANDSHAKE_TIMEOUT": float,
    "ASCON_STRICT_KEY_SIZE": bool,
    "DRONE_TO_GCS_CTL_PORT": int,
    "DRONE_CONTROL_HOST": str,
    "DRONE_CONTROL_PORT": int,
    "GCS_CONTROL_HOST": str,
    "GCS_CONTROL_PORT": int,
}

# Env-overridable keys that are not part of _REQUIRED_KEYS but still need type parsing.
_ENV_OPTIONAL_TYPES = {
    "ENABLE_TCP_CONTROL": bool,
    "CONTROL_COORDINATOR_ROLE": str,
}

# Keys that can be overridden by environment variables
_ENV_OVERRIDABLE = {
        "ENABLE_TCP_CONTROL",
    "CONTROL_COORDINATOR_ROLE",
    "DRONE_HOST",
    "GCS_HOST",
    "TCP_HANDSHAKE_PORT",
    "UDP_DRONE_RX", 
    "UDP_GCS_RX",
    "DRONE_PLAINTEXT_TX",  # Added for testing/benchmarking flexibility
    "DRONE_PLAINTEXT_RX",  # Added for testing/benchmarking flexibility  
    "GCS_PLAINTEXT_TX",    # Added for testing/benchmarking flexibility
    "GCS_PLAINTEXT_RX",    # Added for testing/benchmarking flexibility
    "DRONE_CONTROL_PORT",
    "DRONE_CONTROL_HOST",
    "GCS_CONTROL_PORT",
    "GCS_CONTROL_HOST",
    "DRONE_TO_GCS_CTL_PORT",
    "ENABLE_PACKET_TYPE",
    "ENABLE_ASCON",
    "ENABLE_ASCON128A",
    "STRICT_UDP_PEER_MATCH",
    "STRICT_HANDSHAKE_IP",
    "LOG_SESSION_ID",
    "DRONE_PSK",
    "ASCON_STRICT_KEY_SIZE",
}


def validate_config(cfg: Dict[str, Any]) -> None:
    """
    Ensure all required keys exist with correct types/ranges.
    Raise NotImplementedError("<reason>") on any violation.
    No return value on success.
    """
    # Check all required keys exist
    missing_keys = set(_REQUIRED_KEYS.keys()) - set(cfg.keys())
    if missing_keys:
        raise ConfigError(f"CONFIG missing required keys: {', '.join(sorted(missing_keys))}")
    
    # Check types for all keys
    for key, expected_type in _REQUIRED_KEYS.items():
        value = cfg[key]
        if key == "REKEY_HANDSHAKE_TIMEOUT":
            if not isinstance(value, (int, float)):
                raise ConfigError(
                    f"CONFIG[{key}] must be float seconds, got {type(value).__name__}"
                )
            continue
        if not isinstance(value, expected_type):
            raise ConfigError(f"CONFIG[{key}] must be {expected_type.__name__}, got {type(value).__name__}")
    
    # Validate port ranges
    for key in _REQUIRED_KEYS:
        if key.endswith("_PORT") or key.endswith("_RX") or key.endswith("_TX"):
            port = cfg[key]
            if not (1 <= port <= 65535):
                raise ConfigError(f"CONFIG[{key}] must be valid port (1-65535), got {port}")
    
    # Validate specific constraints
    if cfg["WIRE_VERSION"] != 1:
        raise ConfigError(f"CONFIG[WIRE_VERSION] must be 1 (frozen), got {cfg['WIRE_VERSION']}")
    
    if cfg["REPLAY_WINDOW"] < 64:
        raise ConfigError(f"CONFIG[REPLAY_WINDOW] must be >= 64, got {cfg['REPLAY_WINDOW']}")
    if cfg["REPLAY_WINDOW"] > 8192:
        raise ConfigError(f"CONFIG[REPLAY_WINDOW] must be <= 8192, got {cfg['REPLAY_WINDOW']}")
    
    # Validate hosts are valid strings (basic check)
    for host_key in ["DRONE_HOST", "GCS_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise ConfigError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        try:
            ip_address(host)
        except ValueError as exc:
            raise ConfigError(f"CONFIG[{host_key}] must be a valid IP address: {exc}")

    # Loopback hosts for plaintext path may remain hostnames (e.g., 127.0.0.1).
    # Allow override via CONFIG key or environment variable for backward compatibility
    allow_non_loopback_plaintext_env = str(os.environ.get("ALLOW_NON_LOOPBACK_PLAINTEXT", "")).strip().lower() in {
        "1",
        "true",
        "yes",
        "on",
    }
    allow_non_loopback_plaintext_cfg = bool(cfg.get("ALLOW_NON_LOOPBACK_PLAINTEXT", False))
    allow_non_loopback_plaintext = allow_non_loopback_plaintext_cfg or allow_non_loopback_plaintext_env
    for host_key in ["DRONE_PLAINTEXT_HOST", "GCS_PLAINTEXT_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise ConfigError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        if allow_non_loopback_plaintext:
            continue
        try:
            parsed = ip_address(host)
            if not parsed.is_loopback:
                raise ConfigError(
                    f"CONFIG[{host_key}] must be a loopback address unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
        except ValueError:
            if host.lower() != "localhost":
                raise ConfigError(
                    f"CONFIG[{host_key}] must be a loopback address (localhost) unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
    
    # Optional keys are intentionally not required; do light validation if present
    if "ENCRYPTED_DSCP" in cfg and cfg["ENCRYPTED_DSCP"] is not None:
        if not (0 <= int(cfg["ENCRYPTED_DSCP"]) <= 63):
            raise ConfigError("CONFIG[ENCRYPTED_DSCP] must be 0..63 or None")

    if "ENABLE_TCP_CONTROL" in cfg:
        if not isinstance(cfg["ENABLE_TCP_CONTROL"], bool):
            raise ConfigError("CONFIG[ENABLE_TCP_CONTROL] must be bool")

    coord = cfg.get("CONTROL_COORDINATOR_ROLE", "gcs")
    if coord is not None:
        if not isinstance(coord, str):
            raise ConfigError("CONFIG[CONTROL_COORDINATOR_ROLE] must be str")
        coord_norm = coord.strip().lower()
        if coord_norm not in {"gcs", "drone"}:
            raise ConfigError("CONFIG[CONTROL_COORDINATOR_ROLE] must be 'gcs' or 'drone'")

    # Validate DRONE_PSK: require in non-dev environments; allow empty in dev.
    psk = cfg.get("DRONE_PSK", "")
    if os.getenv("ENV", "dev") != "dev" and not psk:
        raise ConfigError("CONFIG[DRONE_PSK] must be provided in non-dev environment")
    if psk:
        try:
            psk_bytes = bytes.fromhex(psk)
        except ValueError:
            raise ConfigError("CONFIG[DRONE_PSK] must be a hex string")
        if len(psk_bytes) != 32:
            raise ConfigError("CONFIG[DRONE_PSK] must decode to 32 bytes")


def _apply_env_overrides(cfg: Dict[str, Any]) -> Dict[str, Any]:
    """Apply environment variable overrides to config."""
    result = cfg.copy()
    
    for key in _ENV_OVERRIDABLE:
        env_var = key
        if env_var in os.environ:
            env_value = os.environ[env_var]
            expected_type = _REQUIRED_KEYS.get(key) or _ENV_OPTIONAL_TYPES.get(key)
            if expected_type is None:
                raise ConfigError(f"Unsupported env override key: {env_var}")
            
            try:
                if expected_type == int:
                    result[key] = int(env_value)
                elif expected_type == str:
                    result[key] = str(env_value)
                elif expected_type == bool:
                    lowered = str(env_value).strip().lower()
                    if lowered in {"1", "true", "yes", "on"}:
                        result[key] = True
                    elif lowered in {"0", "false", "no", "off"}:
                        result[key] = False
                    else:
                        raise ValueError(f"invalid boolean literal: {env_value}")
                elif expected_type == float:
                    result[key] = float(env_value)
                else:
                    raise ConfigError(f"Unsupported type for env override: {expected_type}")
            except ValueError:
                raise ConfigError(f"Invalid {expected_type.__name__} value for {env_var}: {env_value}")
    
    return result


# Apply environment overrides and validate
CONFIG = _apply_env_overrides(CONFIG)
validate_config(CONFIG)

==================================================

log_codebase.py
==================================================
#!/usr/bin/env python3
"""
Script to log all .py files in the codebase recursively.
Outputs to 'codebase_log.txt' with filename headings and separators.
"""

import os
import sys

def log_codebase(root_dir, output_file):
    with open(output_file, 'w', encoding='utf-8') as f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            for filename in filenames:
                if filename.endswith('.py'):
                    filepath = os.path.join(dirpath, filename)
                    rel_path = os.path.relpath(filepath, root_dir)
                    f.write(f"{rel_path}\n")
                    f.write("=" * 50 + "\n")
                    try:
                        with open(filepath, 'r', encoding='utf-8') as py_file:
                            content = py_file.read()
                            f.write(content)
                    except Exception as e:
                        f.write(f"Error reading file: {e}\n")
                    f.write("\n" + "=" * 50 + "\n\n")

if __name__ == "__main__":
    root_dir = os.getcwd()
    output_file = "codebase_log.txt"
    log_codebase(root_dir, output_file)
    print(f"Codebase logged to {output_file}")
==================================================

modify_config.py
==================================================
from pathlib import Path
p = Path('config.remote.py')
s = p.read_text()
needle = '"MAV_LOCAL_OUT_PORT_2": 14551,'
ins = '    "MAVPROXY_BINARY": "/home/dev/cenv/bin/mavproxy.py",\n'
if needle in s:
    s = s.replace(needle, needle + '\n' + ins)
    p.write_text(s)
    print('inserted')
else:
    print('needle not found')

==================================================

sdrone.remote.py
==================================================
#!/usr/bin/env python3
"""
Simplified Drone Scheduler (CONTROLLER) - sscheduler/sdrone.py

REVERSED CONTROL: Drone is the controller, GCS follows.
- Drone decides suite order, timing, rekey
- Drone sends commands to GCS
- Drone runs echo server (receives traffic from GCS)
- Drone starts its proxy first, then tells GCS to start

Usage:
    python -m sscheduler.sdrone [options]

Environment:
    DRONE_HOST          Drone IP (default: from config)
    GCS_HOST            GCS IP (default: from config)
    GCS_CONTROL_HOST    GCS control server IP (default: GCS_HOST)
"""

import os
import sys
import time
import json
import socket
import signal
import argparse
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timezone

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import CONFIG
from core.suites import get_suite, list_suites
from tools.mavproxy_manager import MavProxyManager

# Extract config values (use CONFIG as single source of truth)
DRONE_HOST = str(CONFIG.get("DRONE_HOST"))
GCS_HOST = str(CONFIG.get("GCS_HOST"))
DRONE_PLAIN_RX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
DRONE_PLAIN_TX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_TX", 47003))

# Control endpoint for GCS: use configured GCS_HOST and GCS_CONTROL_PORT
GCS_CONTROL_HOST = str(CONFIG.get("GCS_HOST"))
GCS_CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", 48080))

# Derived internal proxy control port to avoid collisions
PROXY_INTERNAL_CONTROL_PORT = GCS_CONTROL_PORT + 100

DEFAULT_SUITE = "cs-mlkem768-aesgcm-mldsa65"
SECRETS_DIR = Path(__file__).parent.parent / "secrets" / "matrix"

# Traffic settings (for telling GCS how long to run)
DEFAULT_DURATION = 10.0  # seconds per suite
DEFAULT_RATE_MBPS = 110.0
PAYLOAD_SIZE = 1200

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# --------------------
LOCAL_DURATION = None  # override DEFAULT_DURATION if set, e.g. 10.0
LOCAL_RATE_MBPS = None  # override DEFAULT_RATE_MBPS if set, e.g. 110.0
LOCAL_MAX_SUITES = None  # limit suites run, e.g. 2
LOCAL_SUITES = None  # list of suite names to run, or None

# Get all suites (list_suites returns dict, convert to list of dicts)
_suites_dict = list_suites()
SUITES = [{"name": k, **v} for k, v in _suites_dict.items()]

ROOT = Path(__file__).resolve().parents[1]
LOGS_DIR = ROOT / "logs" / "sscheduler" / "drone"
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# Logging
# ============================================================

def log(msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [sdrone-ctrl] {msg}", flush=True)

# ============================================================
# UDP Echo Server (drone receives traffic from GCS)
# ============================================================

class UdpEchoServer:
    """Echoes UDP packets: receives on DRONE_PLAIN_RX, sends back on DRONE_PLAIN_TX"""
    
    def __init__(self):
        self.rx_sock = None
        self.tx_sock = None
        self.running = False
        self.thread = None
        self.rx_count = 0
        self.tx_count = 0
        self.rx_bytes = 0
        self.tx_bytes = 0
        self.lock = threading.Lock()
    
    def start(self):
        if self.running:
            return
        
        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        self.rx_sock.bind((DRONE_HOST, DRONE_PLAIN_RX_PORT))
        self.rx_sock.settimeout(1.0)
        
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)
        
        self.running = True
        self.thread = threading.Thread(target=self._echo_loop, daemon=True)
        self.thread.start()
        
        log(f"Echo server listening on {DRONE_HOST}:{DRONE_PLAIN_RX_PORT}")
    
    def _echo_loop(self):
        while self.running:
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                with self.lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
                
                self.tx_sock.sendto(data, (DRONE_HOST, DRONE_PLAIN_TX_PORT))
                with self.lock:
                    self.tx_count += 1
                    self.tx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Echo error: {e}")
    
    def get_stats(self):
        with self.lock:
            return {
                "rx_count": self.rx_count,
                "tx_count": self.tx_count,
                "rx_bytes": self.rx_bytes,
                "tx_bytes": self.tx_bytes,
            }
    
    def reset_stats(self):
        with self.lock:
            self.rx_count = 0
            self.tx_count = 0
            self.rx_bytes = 0
            self.tx_bytes = 0
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.rx_sock:
            self.rx_sock.close()
        if self.tx_sock:
            self.tx_sock.close()


# MavProxyManager imported from tools.mavproxy_manager

# ============================================================
# GCS Control Client (drone sends commands to GCS)
# ============================================================

def send_gcs_command(cmd: str, **params) -> dict:
    """Send command to GCS control server"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(30.0)
        sock.connect((GCS_CONTROL_HOST, GCS_CONTROL_PORT))
        
        request = {"cmd": cmd, **params}
        sock.sendall(json.dumps(request).encode() + b"\n")
        
        response = b""
        while True:
            chunk = sock.recv(4096)
            if not chunk:
                break
            response += chunk
            if b"\n" in response:
                break
        
        sock.close()
        return json.loads(response.decode().strip())
    except Exception as e:
        return {"status": "error", "message": str(e)}

def wait_for_gcs(timeout: float = 30.0) -> bool:
    """Wait for GCS control server to be ready"""
    start = time.time()
    while time.time() - start < timeout:
        result = send_gcs_command("ping")
        if result.get("status") == "ok":
            return True
        time.sleep(0.5)
    return False

# ============================================================
# Drone Proxy Management
# ============================================================

class DroneProxyManager:
    """Manages drone proxy subprocess"""
    
    def __init__(self):
        self.process = None
        self.current_suite = None
    
    def start(self, suite_name: str) -> bool:
        """Start drone proxy with given suite"""
        if self.process and self.process.poll() is None:
            self.stop()
        
        suite = get_suite(suite_name)
        if not suite:
            log(f"Unknown suite: {suite_name}")
            return False
        
        secret_dir = SECRETS_DIR / suite_name
        peer_pubkey = secret_dir / "gcs_signing.pub"
        
        if not peer_pubkey.exists():
            log(f"Missing key: {peer_pubkey}")
            return False
        
        cmd = [
            sys.executable, "-m", "core.run_proxy", "drone",
            "--suite", suite_name,
            "--peer-pubkey-file", str(peer_pubkey),
            "--quiet"
        ]

        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_path = LOGS_DIR / f"drone_{suite_name}_{timestamp}.log"
        log(f"Launching: {' '.join(cmd)} (log: {log_path})")
        log_handle = open(log_path, "w", encoding="utf-8")
        self.process = subprocess.Popen(
            cmd,
            stdout=log_handle,
            stderr=subprocess.STDOUT,
            bufsize=1,
            universal_newlines=True,
        )
        self._last_log = log_path
        self.current_suite = suite_name
        
        # Wait for proxy to initialize
        time.sleep(3.0)

        if self.process.poll() is not None:
            log(f"Proxy exited early with code {self.process.returncode}")
            # Print tail of log for diagnosis
            try:
                with open(self._last_log, "r", encoding="utf-8") as fh:
                    lines = fh.read().splitlines()[-30:]
                    log("--- proxy log tail ---")
                    for l in lines:
                        log(l)
                    log("--- end log tail ---")
            except Exception:
                pass
            return False
        
        return True
    
    def stop(self):
        """Stop drone proxy"""
        if self.process:
            self.process.terminate()
            try:
                self.process.wait(timeout=5.0)
            except subprocess.TimeoutExpired:
                self.process.kill()
            self.process = None
            self.current_suite = None
            try:
                # close last log handle if exists by leaving file closed (we opened in start)
                pass
            except Exception:
                pass
    
    def is_running(self) -> bool:
        return self.process is not None and self.process.poll() is None

# ============================================================
# Suite Runner
# ============================================================

def run_suite(proxy: DroneProxyManager, mavproxy, 
              suite_name: str, duration: float, is_first: bool = False) -> dict:
    """Run a single suite test - drone controls the flow.
    
    NOTE: Even though drone is the controller, GCS proxy must start first
    because the TCP handshake requires GCS to listen and drone to connect.
    Drone controls WHEN to start, but GCS proxy goes up first.
    """
    
    result = {
        "suite": suite_name,
        "status": "unknown",
        "echo_rx": 0,
        "echo_tx": 0,
    }
    
    # Ensure mavproxy (application-layer relay) is available
    try:
        mav_running = bool(mavproxy.is_running())
    except Exception:
        mav_running = False
    
    if not is_first:
        # Rekey: tell GCS to prepare (stop its proxy)
        log("Preparing GCS for rekey...")
        resp = send_gcs_command("prepare_rekey")
        if resp.get("status") != "ok":
            log(f"GCS prepare_rekey failed: {resp}")
            result["status"] = "gcs_prepare_failed"
            return result
        
        # Stop our proxy too
        proxy.stop()
        time.sleep(0.5)
    
    # Tell GCS to start its proxy first (GCS listens, drone connects)
    log(f"Telling GCS to start proxy for {suite_name}...")
    resp = send_gcs_command("start_proxy", suite=suite_name)
    log(f"GCS start_proxy response: {resp}")
    if resp.get("status") != "ok":
        log(f"GCS start_proxy failed: {resp}")
        result["status"] = "gcs_start_failed"
        return result

    # Wait for GCS proxy to be ready by polling status
    log("Waiting for GCS proxy to report ready...")
    start_wait = time.time()
    ready = False
    while time.time() - start_wait < 20.0:
        time.sleep(0.5)
        try:
            st = send_gcs_command("status")
            if st.get("proxy_running"):
                ready = True
                break
        except Exception:
            pass

    if not ready:
        log("GCS proxy did not become ready in time")
        result["status"] = "gcs_not_ready"
        return result
    
    # Now start drone proxy (it will connect to GCS)
    log(f"Starting drone proxy for {suite_name}...")
    if not proxy.start(suite_name):
        result["status"] = "proxy_start_failed"
        # include last log path if available
        try:
            tail = getattr(proxy, "_last_log", None)
            if tail:
                result["log"] = str(tail)
        except Exception:
            pass
        return result
    
    # Wait for handshake
    time.sleep(1.0)
    
    # Tell GCS to start traffic
    log("Telling GCS to start traffic...")
    resp = send_gcs_command("start_traffic", duration=duration)
    if resp.get("status") != "ok":
        log(f"GCS start_traffic failed: {resp}")
        result["status"] = "gcs_traffic_failed"
        return result
    
    log("Traffic started, waiting for completion... (mavproxy relaying MAVLink)")
    
    # Wait for GCS to finish traffic generation
    # Poll GCS status
    traffic_done = False
    start_time = time.time()
    max_wait = duration + 30  # Extra buffer
    
    while time.time() - start_time < max_wait:
        time.sleep(2.0)
        
        # Log mavproxy status periodically
        try:
            log(f"mavproxy running: {mavproxy.is_running()}")
        except Exception:
            pass
        
        # Check GCS status
        status = send_gcs_command("status")
        if status.get("traffic_complete"):
            traffic_done = True
            break
        
        # Check if proxy died
        if not proxy.is_running():
            log("Proxy exited unexpectedly")
            result["status"] = "proxy_exited"
            return result
    
    if not traffic_done:
        log("Traffic did not complete in time")
        result["status"] = "timeout"
        return result
    
    # Indicate mavproxy and proxy status in result
    result["mavproxy_running"] = bool(mavproxy.is_running())
    result["proxy_running"] = bool(proxy.is_running())
    result["status"] = "pass"
    
    return result

# ============================================================
# Main
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="Drone Scheduler (Controller)")
    parser.add_argument("--mav-master", default=str(CONFIG.get("MAV_MASTER", "/dev/ttyACM0")), help="Primary MAVLink master (e.g. /dev/ttyACM0 or tcp:host:port)")
    parser.add_argument("--suite", default=None, help="Single suite to run")
    parser.add_argument("--nist-level", choices=["L1", "L3", "L5"], help="Run suites for NIST level")
    parser.add_argument("--all", action="store_true", help="Run all suites")
    parser.add_argument("--duration", type=float, default=DEFAULT_DURATION, help="Seconds per suite")
    parser.add_argument("--rate", type=float, default=DEFAULT_RATE_MBPS, help="Traffic rate Mbps")
    parser.add_argument("--max-suites", type=int, default=None, help="Max suites to run")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Simplified Drone Scheduler (CONTROLLER) - sscheduler")
    print("=" * 60)
    # Configuration dump for debugging
    cfg = {
        "DRONE_HOST": DRONE_HOST,
        "GCS_HOST": GCS_HOST,
        "GCS_CONTROL": f"{GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}",
        "PROXY_INTERNAL_CONTROL_PORT": PROXY_INTERNAL_CONTROL_PORT,
        "DRONE_PLAINTEXT_RX": DRONE_PLAIN_RX_PORT,
        "DRONE_PLAINTEXT_TX": DRONE_PLAIN_TX_PORT,
    }
    log("Configuration Dump:")
    for k, v in cfg.items():
        log(f"  {k}: {v}")
    log(f"Duration: {args.duration}s per suite, Rate: {args.rate} Mbps")
    
    # Determine suites to run
    if args.suite:
        suites_to_run = [args.suite]
    elif args.nist_level:
        suites_to_run = [s["name"] for s in SUITES if s.get("nist_level") == args.nist_level]
    elif args.all:
        suites_to_run = [s["name"] for s in SUITES]
    else:
        # Default: run all available suites
        suites_to_run = [s["name"] for s in SUITES]
    
    if args.max_suites:
        suites_to_run = suites_to_run[:args.max_suites]

    # Apply local in-file overrides
    if LOCAL_RATE_MBPS is not None:
        args.rate = float(LOCAL_RATE_MBPS)
    if LOCAL_DURATION is not None:
        args.duration = float(LOCAL_DURATION)
    if LOCAL_SUITES:
        suites_to_run = [s for s in LOCAL_SUITES if s in [x["name"] for x in SUITES]]
    if LOCAL_MAX_SUITES:
        suites_to_run = suites_to_run[: int(LOCAL_MAX_SUITES)]
    
    log(f"Suites to run: {len(suites_to_run)}")
    
    # Initialize components
    # Start mavproxy manager instead of UDP echo
    mavproxy = MavProxyManager("drone")
    proxy = DroneProxyManager()
    
    # Wait for GCS
    log("Waiting for GCS scheduler...")
    if not wait_for_gcs(timeout=60.0):
        log("ERROR: GCS scheduler not responding")
        return 1
    log("GCS scheduler is ready")
    
    # Tell GCS the test parameters
    send_gcs_command("configure", rate_mbps=args.rate, duration=args.duration)
    
    # Run suites
    results = []
    
    def signal_handler(sig, frame):
        log("Interrupted, cleaning up...")
        proxy.stop()
        try:
            mavproxy.stop()
        except Exception:
            pass
        send_gcs_command("stop")
        sys.exit(1)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        for i, suite_name in enumerate(suites_to_run):
            log("=" * 60)
            log(f"Running suite {i+1}/{len(suites_to_run)}: {suite_name}")
            log("=" * 60)
            
            # Ensure MAVProxy is running on drone side before starting suite
            # Primary master is the flight controller; also listen for tunnel RX
            drone_rx = int(CONFIG.get("DRONE_PLAINTEXT_RX", DRONE_PLAIN_RX_PORT))
            drone_tx = int(CONFIG.get("DRONE_PLAINTEXT_TX", DRONE_PLAIN_TX_PORT))
            # master from args (FCU) and extra listens for tunnel RX
            master = args.mav_master
            extra = [f"--master=udpin:0.0.0.0:{drone_rx}"]
            if not mavproxy.is_running():
                ok = mavproxy.start(master, 115200, "127.0.0.1", drone_tx, extra_args=extra)
                if not ok:
                    log("Failed to start local mavproxy; aborting suite")
                    result = {"suite": suite_name, "status": "mavproxy_start_failed"}
                    results.append(result)
                    break

            result = run_suite(proxy, mavproxy, suite_name, args.duration, is_first=(i == 0))
            results.append(result)
            
            if result["status"] == "pass":
                log(f"Suite PASSED: mavproxy_running={result.get('mavproxy_running')} proxy_running={result.get('proxy_running')}")
            else:
                log(f"Suite FAILED: {result['status']}")
            
            # Wait between suites
            if i < len(suites_to_run) - 1:
                log("Waiting 2s before next suite...")
                time.sleep(2.0)
    
    finally:
        proxy.stop()
        try:
            mavproxy.stop()
        except Exception:
            pass
        send_gcs_command("stop")
    
    # Summary
    print()
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for r in results if r["status"] == "pass")
    failed = len(results) - passed
    
    for r in results:
        status = "[PASS]" if r["status"] == "pass" else "[FAIL]"
        print(
            f"  {status} {r['suite']}: {r['status']}, mavproxy={r.get('mavproxy_running')} proxy={r.get('proxy_running')}"
        )
    
    print("-" * 60)
    print(f"Total: {len(results)}, Passed: {passed}, Failed: {failed}")
    print("=" * 60)
    
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())

==================================================

sdrone.remote2.py
==================================================
#!/usr/bin/env python3
"""
Simplified Drone Scheduler (CONTROLLER) - sscheduler/sdrone.py

REVERSED CONTROL: Drone is the controller, GCS follows.
- Drone decides suite order, timing, rekey
- Drone sends commands to GCS
- Drone runs echo server (receives traffic from GCS)
- Drone starts its proxy first, then tells GCS to start

Usage:
    python -m sscheduler.sdrone [options]

Environment:
    DRONE_HOST          Drone IP (default: from config)
    GCS_HOST            GCS IP (default: from config)
    GCS_CONTROL_HOST    GCS control server IP (default: GCS_HOST)
"""

import os
import sys
import time
import json
import socket
import signal
import argparse
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timezone

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import CONFIG
from core.suites import get_suite, list_suites
from tools.mavproxy_manager import MavProxyManager

# Extract config values (use CONFIG as single source of truth)
DRONE_HOST = str(CONFIG.get("DRONE_HOST"))
GCS_HOST = str(CONFIG.get("GCS_HOST"))
DRONE_PLAIN_RX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
DRONE_PLAIN_TX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_TX", 47003))

# Control endpoint for GCS: use configured GCS_HOST and GCS_CONTROL_PORT
GCS_CONTROL_HOST = str(CONFIG.get("GCS_HOST"))
GCS_CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", 48080))

# Derived internal proxy control port to avoid collisions
PROXY_INTERNAL_CONTROL_PORT = GCS_CONTROL_PORT + 100

DEFAULT_SUITE = "cs-mlkem768-aesgcm-mldsa65"
SECRETS_DIR = Path(__file__).parent.parent / "secrets" / "matrix"

# Traffic settings (for telling GCS how long to run)
DEFAULT_DURATION = 10.0  # seconds per suite
DEFAULT_RATE_MBPS = 110.0
PAYLOAD_SIZE = 1200

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# --------------------
LOCAL_DURATION = None  # override DEFAULT_DURATION if set, e.g. 10.0
LOCAL_RATE_MBPS = None  # override DEFAULT_RATE_MBPS if set, e.g. 110.0
LOCAL_MAX_SUITES = None  # limit suites run, e.g. 2
LOCAL_SUITES = None  # list of suite names to run, or None

# Get all suites (list_suites returns dict, convert to list of dicts)
_suites_dict = list_suites()
SUITES = [{"name": k, **v} for k, v in _suites_dict.items()]

ROOT = Path(__file__).resolve().parents[1]
LOGS_DIR = ROOT / "logs" / "sscheduler" / "drone"
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# Logging
# ============================================================

def log(msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [sdrone-ctrl] {msg}", flush=True)

# ============================================================
# UDP Echo Server (drone receives traffic from GCS)
# ============================================================

class UdpEchoServer:
    """Echoes UDP packets: receives on DRONE_PLAIN_RX, sends back on DRONE_PLAIN_TX"""
    
    def __init__(self):
        self.rx_sock = None
        self.tx_sock = None
        self.running = False
        self.thread = None
        self.rx_count = 0
        self.tx_count = 0
        self.rx_bytes = 0
        self.tx_bytes = 0
        self.lock = threading.Lock()
    
    def start(self):
        if self.running:
            return
        
        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        self.rx_sock.bind((DRONE_HOST, DRONE_PLAIN_RX_PORT))
        self.rx_sock.settimeout(1.0)
        
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)
        
        self.running = True
        self.thread = threading.Thread(target=self._echo_loop, daemon=True)
        self.thread.start()
        
        log(f"Echo server listening on {DRONE_HOST}:{DRONE_PLAIN_RX_PORT}")
    
    def _echo_loop(self):
        while self.running:
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                with self.lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
                
                self.tx_sock.sendto(data, (DRONE_HOST, DRONE_PLAIN_TX_PORT))
                with self.lock:
                    self.tx_count += 1
                    self.tx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Echo error: {e}")
    
    def get_stats(self):
        with self.lock:
            return {
                "rx_count": self.rx_count,
                "tx_count": self.tx_count,
                "rx_bytes": self.rx_bytes,
                "tx_bytes": self.tx_bytes,
            }
    
    def reset_stats(self):
        with self.lock:
            self.rx_count = 0
            self.tx_count = 0
            self.rx_bytes = 0
            self.tx_bytes = 0
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.rx_sock:
            self.rx_sock.close()
        if self.tx_sock:
            self.tx_sock.close()


# MavProxyManager imported from tools.mavproxy_manager

# ============================================================
# GCS Control Client (drone sends commands to GCS)
# ============================================================

def send_gcs_command(cmd: str, **params) -> dict:
    """Send command to GCS control server"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(30.0)
        sock.connect((GCS_CONTROL_HOST, GCS_CONTROL_PORT))
        
        request = {"cmd": cmd, **params}
        sock.sendall(json.dumps(request).encode() + b"\n")
        
        response = b""
        while True:
            chunk = sock.recv(4096)
            if not chunk:
                break
            response += chunk
            if b"\n" in response:
                break
        
        sock.close()
        return json.loads(response.decode().strip())
    except Exception as e:
        return {"status": "error", "message": str(e)}

def wait_for_gcs(timeout: float = 30.0) -> bool:
    """Wait for GCS control server to be ready"""
    start = time.time()
    while time.time() - start < timeout:
        result = send_gcs_command("ping")
        if result.get("status") == "ok":
            return True
        time.sleep(0.5)
    return False

# ============================================================
# Drone Proxy Management
# ============================================================

class DroneProxyManager:
    """Manages drone proxy subprocess"""
    
    def __init__(self):
        self.process = None
        self.current_suite = None
    
    def start(self, suite_name: str) -> bool:
        """Start drone proxy with given suite"""
        if self.process and self.process.poll() is None:
            self.stop()
        
        suite = get_suite(suite_name)
        if not suite:
            log(f"Unknown suite: {suite_name}")
            return False
        
        secret_dir = SECRETS_DIR / suite_name
        peer_pubkey = secret_dir / "gcs_signing.pub"
        
        if not peer_pubkey.exists():
            log(f"Missing key: {peer_pubkey}")
            return False
        
        cmd = [
            sys.executable, "-m", "core.run_proxy", "drone",
            "--suite", suite_name,
            "--peer-pubkey-file", str(peer_pubkey),
            "--quiet"
        ]

        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_path = LOGS_DIR / f"drone_{suite_name}_{timestamp}.log"
        log(f"Launching: {' '.join(cmd)} (log: {log_path})")
        log_handle = open(log_path, "w", encoding="utf-8")
        self.process = subprocess.Popen(
            cmd,
            stdout=log_handle,
            stderr=subprocess.STDOUT,
            bufsize=1,
            universal_newlines=True,
        )
        self._last_log = log_path
        self.current_suite = suite_name
        
        # Wait for proxy to initialize
        time.sleep(3.0)

        if self.process.poll() is not None:
            log(f"Proxy exited early with code {self.process.returncode}")
            # Print tail of log for diagnosis
            try:
                with open(self._last_log, "r", encoding="utf-8") as fh:
                    lines = fh.read().splitlines()[-30:]
                    log("--- proxy log tail ---")
                    for l in lines:
                        log(l)
                    log("--- end log tail ---")
            except Exception:
                pass
            return False
        
        return True
    
    def stop(self):
        """Stop drone proxy"""
        if self.process:
            self.process.terminate()
            try:
                self.process.wait(timeout=5.0)
            except subprocess.TimeoutExpired:
                self.process.kill()
            self.process = None
            self.current_suite = None
            try:
                # close last log handle if exists by leaving file closed (we opened in start)
                pass
            except Exception:
                pass
    
    def is_running(self) -> bool:
        return self.process is not None and self.process.poll() is None

# ============================================================
# Suite Runner
# ============================================================

def run_suite(proxy: DroneProxyManager, mavproxy, 
              suite_name: str, duration: float, is_first: bool = False) -> dict:
    """Run a single suite test - drone controls the flow.
    
    NOTE: Even though drone is the controller, GCS proxy must start first
    because the TCP handshake requires GCS to listen and drone to connect.
    Drone controls WHEN to start, but GCS proxy goes up first.
    """
    
    result = {
        "suite": suite_name,
        "status": "unknown",
        "echo_rx": 0,
        "echo_tx": 0,
    }
    
    # Ensure mavproxy (application-layer relay) is available
    try:
        mav_running = bool(mavproxy.is_running())
    except Exception:
        mav_running = False
    
    if not is_first:
        # Rekey: tell GCS to prepare (stop its proxy)
        log("Preparing GCS for rekey...")
        resp = send_gcs_command("prepare_rekey")
        if resp.get("status") != "ok":
            log(f"GCS prepare_rekey failed: {resp}")
            result["status"] = "gcs_prepare_failed"
            return result
        
        # Stop our proxy too
        proxy.stop()
        time.sleep(0.5)
    
    # Tell GCS to start its proxy first (GCS listens, drone connects)
    log(f"Telling GCS to start proxy for {suite_name}...")
    resp = send_gcs_command("start_proxy", suite=suite_name)
    log(f"GCS start_proxy response: {resp}")
    if resp.get("status") != "ok":
        log(f"GCS start_proxy failed: {resp}")
        result["status"] = "gcs_start_failed"
        return result

    # Wait for GCS proxy to be ready by polling status
    log("Waiting for GCS proxy to report ready...")
    start_wait = time.time()
    ready = False
    while time.time() - start_wait < 20.0:
        time.sleep(0.5)
        try:
            st = send_gcs_command("status")
            if st.get("proxy_running"):
                ready = True
                break
        except Exception:
            pass

    if not ready:
        log("GCS proxy did not become ready in time")
        result["status"] = "gcs_not_ready"
        return result
    
    # Now start drone proxy (it will connect to GCS)
    log(f"Starting drone proxy for {suite_name}...")
    if not proxy.start(suite_name):
        result["status"] = "proxy_start_failed"
        # include last log path if available
        try:
            tail = getattr(proxy, "_last_log", None)
            if tail:
                result["log"] = str(tail)
        except Exception:
            pass
        return result
    
    # Wait for handshake
    time.sleep(1.0)
    
    # Tell GCS to start traffic
    log("Telling GCS to start traffic...")
    resp = send_gcs_command("start_traffic", duration=duration)
    if resp.get("status") != "ok":
        log(f"GCS start_traffic failed: {resp}")
        result["status"] = "gcs_traffic_failed"
        return result
    
    log("Traffic started, waiting for completion... (mavproxy relaying MAVLink)")
    
    # Wait for GCS to finish traffic generation
    # Poll GCS status
    traffic_done = False
    start_time = time.time()
    max_wait = duration + 30  # Extra buffer
    
    while time.time() - start_time < max_wait:
        time.sleep(2.0)
        
        # Log mavproxy status periodically
        try:
            log(f"mavproxy running: {mavproxy.is_running()}")
        except Exception:
            pass
        
        # Check GCS status
        status = send_gcs_command("status")
        if status.get("traffic_complete"):
            traffic_done = True
            break
        
        # Check if proxy died
        if not proxy.is_running():
            log("Proxy exited unexpectedly")
            result["status"] = "proxy_exited"
            return result
    
    if not traffic_done:
        log("Traffic did not complete in time")
        result["status"] = "timeout"
        return result
    
    # Indicate mavproxy and proxy status in result
    result["mavproxy_running"] = bool(mavproxy.is_running())
    result["proxy_running"] = bool(proxy.is_running())
    result["status"] = "pass"
    
    return result

# ============================================================
# Main
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="Drone Scheduler (Controller)")
    parser.add_argument("--mav-master", default=str(CONFIG.get("MAV_MASTER", "/dev/ttyACM0")), help="Primary MAVLink master (e.g. /dev/ttyACM0 or tcp:host:port)")
    parser.add_argument("--suite", default=None, help="Single suite to run")
    parser.add_argument("--nist-level", choices=["L1", "L3", "L5"], help="Run suites for NIST level")
    parser.add_argument("--all", action="store_true", help="Run all suites")
    parser.add_argument("--duration", type=float, default=DEFAULT_DURATION, help="Seconds per suite")
    parser.add_argument("--rate", type=float, default=DEFAULT_RATE_MBPS, help="Traffic rate Mbps")
    parser.add_argument("--max-suites", type=int, default=None, help="Max suites to run")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Simplified Drone Scheduler (CONTROLLER) - sscheduler")
    print("=" * 60)
    # Configuration dump for debugging
    cfg = {
        "DRONE_HOST": DRONE_HOST,
        "GCS_HOST": GCS_HOST,
        "GCS_CONTROL": f"{GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}",
        "PROXY_INTERNAL_CONTROL_PORT": PROXY_INTERNAL_CONTROL_PORT,
        "DRONE_PLAINTEXT_RX": DRONE_PLAIN_RX_PORT,
        "DRONE_PLAINTEXT_TX": DRONE_PLAIN_TX_PORT,
    }
    log("Configuration Dump:")
    for k, v in cfg.items():
        log(f"  {k}: {v}")
    log(f"Duration: {args.duration}s per suite, Rate: {args.rate} Mbps")
    
    # Determine suites to run
    if args.suite:
        suites_to_run = [args.suite]
    elif args.nist_level:
        suites_to_run = [s["name"] for s in SUITES if s.get("nist_level") == args.nist_level]
    elif args.all:
        suites_to_run = [s["name"] for s in SUITES]
    else:
        # Default: run all available suites
        suites_to_run = [s["name"] for s in SUITES]
    
    if args.max_suites:
        suites_to_run = suites_to_run[:args.max_suites]

    # Apply local in-file overrides
    if LOCAL_RATE_MBPS is not None:
        args.rate = float(LOCAL_RATE_MBPS)
    if LOCAL_DURATION is not None:
        args.duration = float(LOCAL_DURATION)
    if LOCAL_SUITES:
        suites_to_run = [s for s in LOCAL_SUITES if s in [x["name"] for x in SUITES]]
    if LOCAL_MAX_SUITES:
        suites_to_run = suites_to_run[: int(LOCAL_MAX_SUITES)]
    
    log(f"Suites to run: {len(suites_to_run)}")
    
    # Initialize components
    # Start mavproxy manager instead of UDP echo
    mavproxy = MavProxyManager("drone")
    proxy = DroneProxyManager()
    
    # Wait for GCS
    log("Waiting for GCS scheduler...")
    if not wait_for_gcs(timeout=60.0):
        log("ERROR: GCS scheduler not responding")
        return 1
    log("GCS scheduler is ready")
    
    # Tell GCS the test parameters
    send_gcs_command("configure", rate_mbps=args.rate, duration=args.duration)
    
    # Run suites
    results = []
    
    def signal_handler(sig, frame):
        log("Interrupted, cleaning up...")
        proxy.stop()
        try:
            mavproxy.stop()
        except Exception:
            pass
        send_gcs_command("stop")
        sys.exit(1)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        for i, suite_name in enumerate(suites_to_run):
            log("=" * 60)
            log(f"Running suite {i+1}/{len(suites_to_run)}: {suite_name}")
            log("=" * 60)
            
            # Ensure MAVProxy is running on drone side before starting suite
            # Primary master is the flight controller; also listen for tunnel RX
            drone_rx = int(CONFIG.get("DRONE_PLAINTEXT_RX", DRONE_PLAIN_RX_PORT))
            drone_tx = int(CONFIG.get("DRONE_PLAINTEXT_TX", DRONE_PLAIN_TX_PORT))
            # master from args (FCU) and extra listens for tunnel RX
            master = args.mav_master
            extra = [f"--master=udpin:0.0.0.0:{drone_rx}"]
            if not mavproxy.is_running():
                ok = mavproxy.start(master, 115200, "127.0.0.1", drone_tx, extra_args=extra)
                if not ok:
                    log("Failed to start local mavproxy; aborting suite")
                    result = {"suite": suite_name, "status": "mavproxy_start_failed"}
                    results.append(result)
                    break

            result = run_suite(proxy, mavproxy, suite_name, args.duration, is_first=(i == 0))
            results.append(result)
            
            if result["status"] == "pass":
                log(f"Suite PASSED: mavproxy_running={result.get('mavproxy_running')} proxy_running={result.get('proxy_running')}")
            else:
                log(f"Suite FAILED: {result['status']}")
            
            # Wait between suites
            if i < len(suites_to_run) - 1:
                log("Waiting 2s before next suite...")
                time.sleep(2.0)
    
    finally:
        proxy.stop()
        try:
            mavproxy.stop()
        except Exception:
            pass
        send_gcs_command("stop")
    
    # Summary
    print()
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for r in results if r["status"] == "pass")
    failed = len(results) - passed
    
    for r in results:
        status = "[PASS]" if r["status"] == "pass" else "[FAIL]"
        print(
            f"  {status} {r['suite']}: {r['status']}, mavproxy={r.get('mavproxy_running')} proxy={r.get('proxy_running')}"
        )
    
    print("-" * 60)
    print(f"Total: {len(results)}, Passed: {passed}, Failed: {failed}")
    print("=" * 60)
    
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())

==================================================

test_all_complete_loop.py
==================================================
#!/usr/bin/env python3
"""
Run complete localhost loop for all available suites in secrets/matrix.
For each suite:
 - start GCS proxy
 - start Drone proxy
 - start drone plaintext echo server
 - send UDP traffic from a sender at target bandwidth for duration
 - measure delivery (GCS plaintext RX receives)

Usage: python test_all_complete_loop.py --duration 10 --rate 110
"""
import os
import sys
import time
import socket
import struct
import threading
import subprocess
from pathlib import Path
import argparse

ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(ROOT))

from core.config import CONFIG
from core.suites import list_suites

# Ports
GCS_PLAINTEXT_TX = CONFIG["GCS_PLAINTEXT_TX"]
GCS_PLAINTEXT_RX = CONFIG["GCS_PLAINTEXT_RX"]
DRONE_PLAINTEXT_TX = CONFIG["DRONE_PLAINTEXT_TX"]
DRONE_PLAINTEXT_RX = CONFIG["DRONE_PLAINTEXT_RX"]
HOST = "127.0.0.1"

SECRETS_DIR = ROOT / "secrets" / "matrix"

PAYLOAD_SIZE = 1200


def run_echo(stop_event, stats):
    rx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    rx.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    rx.bind((HOST, DRONE_PLAINTEXT_RX))
    rx.settimeout(0.5)
    tx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        while not stop_event.is_set():
            try:
                data, addr = rx.recvfrom(65535)
                stats['rx'] += 1
                tx.sendto(data, (HOST, DRONE_PLAINTEXT_TX))
                stats['tx'] += 1
            except socket.timeout:
                continue
    finally:
        rx.close(); tx.close()


def traffic_sender(duration_s, rate_mbps, result):
    """Send UDP packets to GCS_PLAINTEXT_TX for duration seconds and count responses at GCS_PLAINTEXT_RX."""
    tx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    rx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    rx.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    rx.bind((HOST, GCS_PLAINTEXT_RX))
    rx.settimeout(1.0)

    pps = (rate_mbps * 1_000_000) / (8 * PAYLOAD_SIZE)
    if pps < 1:
        pps = 1
    interval = 1.0 / pps

    sent = 0
    received = 0
    end = time.time() + duration_s
    seq = 0

    while time.time() < end:
        send_time = time.time_ns()
        payload = struct.pack("!IQ", seq, send_time) + b"PQC-ALL-LOOP"
        try:
            tx.sendto(payload, (HOST, GCS_PLAINTEXT_TX))
            sent += 1
        except Exception:
            pass
        # try receive (non-blocking-ish)
        t0 = time.time()
        while time.time() - t0 < interval:
            try:
                data, addr = rx.recvfrom(65535)
                if len(data) >= 12:
                    received += 1
            except socket.timeout:
                break
            except Exception:
                break
        seq += 1
    tx.close(); rx.close()
    result['sent'] = sent
    result['received'] = received


def run_suite(suite_name, duration, rate):
    print('\n' + '='*60)
    print(f"Suite: {suite_name}")
    suite_dir = SECRETS_DIR / suite_name
    gcs_key = suite_dir / 'gcs_signing.key'
    gcs_pub = suite_dir / 'gcs_signing.pub'
    if not gcs_key.exists() or not gcs_pub.exists():
        print(f"  Skipping {suite_name}: missing keys in {suite_dir}")
        return {'suite': suite_name, 'status': 'skipped'}

    # Start GCS proxy
    gcs_cmd = [sys.executable, '-m', 'core.run_proxy', 'gcs', '--suite', suite_name, '--gcs-secret-file', str(gcs_key), '--quiet']
    gcs_proc = subprocess.Popen(gcs_cmd, cwd=ROOT, env=os.environ.copy())
    time.sleep(2.0)
    if gcs_proc.poll() is not None:
        print('  GCS proxy exited early')
        return {'suite': suite_name, 'status': 'gcs_failed'}

    # Start Drone proxy
    drone_cmd = [sys.executable, '-m', 'core.run_proxy', 'drone', '--suite', suite_name, '--peer-pubkey-file', str(gcs_pub), '--quiet']
    drone_proc = subprocess.Popen(drone_cmd, cwd=ROOT, env=os.environ.copy())
    time.sleep(3.0)
    if drone_proc.poll() is not None:
        print('  Drone proxy exited early')
        gcs_proc.terminate(); gcs_proc.wait(timeout=2)
        return {'suite': suite_name, 'status': 'drone_failed'}

    # Start echo
    stop_event = threading.Event()
    echo_stats = {'rx': 0, 'tx': 0}
    echo_thread = threading.Thread(target=run_echo, args=(stop_event, echo_stats), daemon=True)
    echo_thread.start()
    time.sleep(0.5)

    # Start traffic sender and measure
    result = {'sent': 0, 'received': 0}
    traffic_thread = threading.Thread(target=traffic_sender, args=(duration, rate, result), daemon=True)
    traffic_thread.start()
    traffic_thread.join(timeout=duration + 5)

    # Stop
    stop_event.set()
    time.sleep(0.2)

    # Cleanup procs
    if drone_proc.poll() is None:
        drone_proc.terminate();
        try: drone_proc.wait(timeout=2)
        except: drone_proc.kill()
    if gcs_proc.poll() is None:
        gcs_proc.terminate();
        try: gcs_proc.wait(timeout=2)
        except: gcs_proc.kill()

    sent = result.get('sent', 0)
    received = result.get('received', 0)
    delivery = 0.0 if sent==0 else 100.0 * received / sent
    print(f"  Sent: {sent}, Received: {received}, Delivery: {delivery:.1f}%")
    print(f"  Echo stats: RX={echo_stats['rx']}, TX={echo_stats['tx']}")
    status = 'pass' if received>0 else 'fail'
    return {'suite': suite_name, 'status': status, 'sent': sent, 'received': received, 'delivery': delivery}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--duration', type=float, default=10.0)
    parser.add_argument('--rate', type=float, default=110.0)
    parser.add_argument('--limit', type=int, default=None)
    args = parser.parse_args()

    suites = list_suites()
    suite_names = list(suites.keys())
    if args.limit:
        suite_names = suite_names[:args.limit]

    results = []
    for s in suite_names:
        res = run_suite(s, args.duration, args.rate)
        results.append(res)

    # Summary
    print('\n' + '='*60)
    print('SUMMARY')
    for r in results:
        print(f"{r['suite']}: {r.get('status')} - sent={r.get('sent',0)} rx={r.get('received',0)} delivery={r.get('delivery',0):.1f}%")

    return 0

if __name__ == '__main__':
    sys.exit(main())

==================================================

test_complete_loop.py
==================================================
#!/usr/bin/env python3
"""
Complete localhost loop test for PQC secure tunnel - all in one script.

This script:
1. Starts GCS proxy (server) in a subprocess
2. Starts Drone proxy (client) in a subprocess  
3. Runs a UDP echo server simulating the drone application
4. Sends test packets through the complete encrypt/decrypt loop
5. Validates the round-trip

Run with: python test_complete_loop.py
"""

import os
import sys
import socket
import struct
import time
import threading
import subprocess

# Configure localhost for both sides
os.environ["DRONE_HOST"] = "127.0.0.1"
os.environ["GCS_HOST"] = "127.0.0.1"
os.environ["DRONE_CONTROL_HOST"] = "127.0.0.1"
# Disable packet type prefix for simple test (raw passthrough)
os.environ["ENABLE_PACKET_TYPE"] = "0"

# Add parent directory to path for imports
ROOT = os.path.dirname(os.path.abspath(__file__))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

from core.config import CONFIG

# Ports from config
GCS_PLAINTEXT_TX = CONFIG["GCS_PLAINTEXT_TX"]   # 47001 - GCS app sends here
GCS_PLAINTEXT_RX = CONFIG["GCS_PLAINTEXT_RX"]   # 47002 - GCS app receives here
DRONE_PLAINTEXT_TX = CONFIG["DRONE_PLAINTEXT_TX"]  # 47003 - Drone app sends here
DRONE_PLAINTEXT_RX = CONFIG["DRONE_PLAINTEXT_RX"]  # 47004 - Drone app receives here

HOST = "127.0.0.1"
TEST_DURATION = 30  # seconds


def wait_for_port(port: int, host: str = "127.0.0.1", timeout: float = 10.0) -> bool:
    """Wait for a port to become unavailable (i.e., something is listening)."""
    deadline = time.time() + timeout
    while time.time() < deadline:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(0.5)
        try:
            result = sock.connect_ex((host, port))
            sock.close()
            if result == 0:
                return True
        except Exception:
            pass
        time.sleep(0.2)
    return False


def run_drone_echo(stop_event: threading.Event, stats: dict):
    """Simple echo server on drone side - receives decrypted, sends back."""
    try:
        rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        rx_sock.bind((HOST, DRONE_PLAINTEXT_RX))
        rx_sock.settimeout(0.5)
        
        tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        
        print(f"[Echo] Started on {HOST}:{DRONE_PLAINTEXT_RX} -> {HOST}:{DRONE_PLAINTEXT_TX}")
        
        while not stop_event.is_set():
            try:
                data, addr = rx_sock.recvfrom(65535)
                stats["rx"] += 1
                # Echo back through the encrypt path
                tx_sock.sendto(data, (HOST, DRONE_PLAINTEXT_TX))
                stats["tx"] += 1
            except socket.timeout:
                continue
            except Exception as e:
                if not stop_event.is_set():
                    print(f"[Echo] Error: {e}")
                    
    except Exception as e:
        print(f"[Echo] Failed to start: {e}")
    finally:
        try: rx_sock.close()
        except: pass
        try: tx_sock.close()
        except: pass


def run_test_client(packet_count: int, delay_s: float) -> tuple:
    """Send packets and measure round-trip through encrypted tunnel."""
    
    tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    rx_sock.bind((HOST, GCS_PLAINTEXT_RX))
    rx_sock.settimeout(2.0)
    
    sent = 0
    received = 0
    rtt_samples = []
    
    print(f"\n[Test] Sending {packet_count} packets to {HOST}:{GCS_PLAINTEXT_TX}")
    print(f"[Test] Expecting responses on {HOST}:{GCS_PLAINTEXT_RX}")
    
    for seq in range(packet_count):
        send_time = time.time_ns()
        packet = struct.pack("!IQ", seq, send_time) + b"PQC-LOCALHOST-TEST"
        
        tx_sock.sendto(packet, (HOST, GCS_PLAINTEXT_TX))
        sent += 1
        
        try:
            response, addr = rx_sock.recvfrom(65535)
            recv_time = time.time_ns()
            
            if len(response) >= 12:
                resp_seq, resp_ts = struct.unpack("!IQ", response[:12])
                rtt_ns = recv_time - resp_ts
                rtt_ms = rtt_ns / 1_000_000
                rtt_samples.append(rtt_ms)
                received += 1
                
                if seq % 10 == 0 or seq < 5:
                    print(f"[Test] Packet {seq}: RTT = {rtt_ms:.2f} ms")
        except socket.timeout:
            if seq < 5 or seq % 10 == 0:
                print(f"[Test] Packet {seq}: TIMEOUT")
        
        if delay_s > 0:
            time.sleep(delay_s)
    
    tx_sock.close()
    rx_sock.close()
    
    return sent, received, rtt_samples


def main():
    print("=" * 70)
    print("PQC SECURE TUNNEL - COMPLETE LOCALHOST LOOP TEST")
    print("=" * 70)
    print(f"\nUsing suite: cs-mlkem768-aesgcm-mldsa65")
    print(f"Hosts: DRONE_HOST={os.environ['DRONE_HOST']}, GCS_HOST={os.environ['GCS_HOST']}")
    print(f"\nData flow:")
    print(f"  GCS Client ({GCS_PLAINTEXT_TX}) -> GCS Proxy (encrypt)")
    print(f"    -> UDP Network (46011/46012)")
    print(f"    -> Drone Proxy (decrypt) -> Echo Server ({DRONE_PLAINTEXT_RX})")
    print(f"    -> Echo ({DRONE_PLAINTEXT_TX}) -> Drone Proxy (encrypt)")
    print(f"    -> UDP Network")  
    print(f"    -> GCS Proxy (decrypt) -> GCS Client ({GCS_PLAINTEXT_RX})")
    print("")
    
    gcs_proc = None
    drone_proc = None
    stop_event = threading.Event()
    echo_stats = {"rx": 0, "tx": 0}
    
    try:
        # Start GCS proxy (server side)
        print("[Main] Starting GCS proxy...")
        gcs_cmd = [
            sys.executable, "-m", "core.run_proxy", "gcs",
            "--gcs-secret-file", "secrets/localtest/gcs_signing.key",
            "--suite", "cs-mlkem768-aesgcm-mldsa65",
            "--stop-seconds", str(TEST_DURATION)
        ]
        gcs_proc = subprocess.Popen(
            gcs_cmd,
            stdout=None,  # Let output go to console
            stderr=None,
            cwd=ROOT,
            env=os.environ.copy()
        )
        
        # Wait for GCS to start listening
        print("[Main] Waiting for GCS proxy to start...")
        time.sleep(2)
        
        if gcs_proc.poll() is not None:
            output = gcs_proc.stdout.read().decode() if gcs_proc.stdout else ""
            print(f"[Main] ERROR: GCS proxy exited early! Output:\n{output}")
            return 1
        
        # Start Drone proxy (client side)
        print("[Main] Starting Drone proxy...")
        drone_cmd = [
            sys.executable, "-m", "core.run_proxy", "drone",
            "--peer-pubkey-file", "secrets/localtest/gcs_signing.pub",
            "--suite", "cs-mlkem768-aesgcm-mldsa65",
            "--stop-seconds", str(TEST_DURATION - 2),
            "--quiet"
        ]
        drone_proc = subprocess.Popen(
            drone_cmd,
            stdout=None,  # Let output go to console
            stderr=None,
            cwd=ROOT,
            env=os.environ.copy()
        )
        
        # Wait for handshake
        print("[Main] Waiting for PQC handshake...")
        time.sleep(3)
        
        if drone_proc.poll() is not None:
            output = drone_proc.stdout.read().decode() if drone_proc.stdout else ""
            print(f"[Main] ERROR: Drone proxy exited! Output:\n{output}")
            return 1
        
        print("[Main] OK - Proxies started successfully!")
        
        # Start echo server
        echo_thread = threading.Thread(
            target=run_drone_echo,
            args=(stop_event, echo_stats),
            daemon=True
        )
        echo_thread.start()
        time.sleep(0.5)
        
        # Run the test
        print("\n" + "-" * 70)
        sent, received, rtt_samples = run_test_client(packet_count=30, delay_s=0.1)
        print("-" * 70)
        
        # Results
        print("\n" + "=" * 70)
        print("TEST RESULTS")
        print("=" * 70)
        print(f"Packets sent:       {sent}")
        print(f"Packets received:   {received}")
        print(f"Delivery rate:      {100.0 * received / max(1, sent):.1f}%")
        print(f"Echo server stats:  RX={echo_stats['rx']}, TX={echo_stats['tx']}")
        
        if rtt_samples:
            avg_rtt = sum(rtt_samples) / len(rtt_samples)
            min_rtt = min(rtt_samples)
            max_rtt = max(rtt_samples)
            print(f"RTT (min/avg/max):  {min_rtt:.2f} / {avg_rtt:.2f} / {max_rtt:.2f} ms")
        
        print("=" * 70)
        
        if received > 0:
            print("\n[OK] SUCCESS: Complete PQC encryption/decryption loop verified!")
            print("   Packets successfully traveled:")
            print("   1. GCS plaintext -> GCS proxy (ML-KEM-768 + AES-256-GCM encrypt)")
            print("   2. UDP encrypted tunnel (localhost)")
            print("   3. Drone proxy (AES-256-GCM decrypt) -> Echo server")
            print("   4. Echo server -> Drone proxy (encrypt)")
            print("   5. UDP encrypted tunnel (localhost)")
            print("   6. GCS proxy (decrypt) -> GCS client")
            return 0
        else:
            print("\n[FAIL] No packets completed the round-trip")
            return 1
            
    except Exception as e:
        print(f"\n[ERROR]: {e}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        stop_event.set()
        
        if drone_proc and drone_proc.poll() is None:
            drone_proc.terminate()
            try:
                drone_proc.wait(timeout=3)
            except:
                drone_proc.kill()
                
        if gcs_proc and gcs_proc.poll() is None:
            gcs_proc.terminate()
            try:
                gcs_proc.wait(timeout=3)
            except:
                gcs_proc.kill()
        
        print("\n[Main] Cleanup complete.")


if __name__ == "__main__":
    sys.exit(main())

==================================================

test_localhost_loop.py
==================================================
#!/usr/bin/env python3
"""
Localhost loop test for PQC secure tunnel.

This script sends UDP packets through the complete encryption/decryption loop:
1. GCS plaintext sender (port 47001) -> GCS proxy
2. GCS proxy encrypts -> network (port 46011) -> Drone proxy  
3. Drone proxy decrypts -> Drone plaintext (port 47004)
4. Echo server returns packet via same path in reverse

We also run a simple echo server on the drone side to simulate the flight controller.
"""

import socket
import struct
import time
import threading

# Ports from core/config.py
GCS_PLAINTEXT_TX = 47001   # GCS app sends plaintext here -> GCS proxy
GCS_PLAINTEXT_RX = 47002   # GCS proxy returns decrypted packets here
DRONE_PLAINTEXT_TX = 47003  # Drone app sends plaintext here
DRONE_PLAINTEXT_RX = 47004  # Drone proxy returns decrypted packets here

HOST = "127.0.0.1"

def run_drone_echo_server(stop_event: threading.Event, stats: dict):
    """Run an echo server on the drone side to return packets."""
    
    # Bind to drone RX port (receives decrypted packets from drone proxy)
    rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    rx_sock.bind((HOST, DRONE_PLAINTEXT_RX))
    rx_sock.settimeout(0.5)
    
    # Create send socket to send echo back to drone proxy
    tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    
    print(f"[Echo] Drone echo server started on {HOST}:{DRONE_PLAINTEXT_RX}")
    print(f"[Echo] Will echo back to {HOST}:{DRONE_PLAINTEXT_TX}")
    
    while not stop_event.is_set():
        try:
            data, addr = rx_sock.recvfrom(65535)
            stats["echo_received"] += 1
            
            # Echo the data back through the drone TX port
            tx_sock.sendto(data, (HOST, DRONE_PLAINTEXT_TX))
            stats["echo_sent"] += 1
            
            if stats["echo_received"] % 10 == 0:
                print(f"[Echo] Received and echoed {stats['echo_received']} packets")
        except socket.timeout:
            continue
        except Exception as e:
            print(f"[Echo] Error: {e}")
    
    rx_sock.close()
    tx_sock.close()
    print("[Echo] Echo server stopped")


def run_gcs_sender_receiver(packet_count: int, delay_ms: float):
    """Send packets from GCS side and receive echoed responses."""
    
    # Create sender socket (sends to GCS proxy plaintext input)
    tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    
    # Create receiver socket (receives from GCS proxy after decryption)
    rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
    rx_sock.bind((HOST, GCS_PLAINTEXT_RX))
    rx_sock.settimeout(2.0)
    
    print(f"\n[GCS] Sending {packet_count} packets to {HOST}:{GCS_PLAINTEXT_TX}")
    print(f"[GCS] Expecting responses on {HOST}:{GCS_PLAINTEXT_RX}")
    
    sent = 0
    received = 0
    rtt_samples = []
    
    for seq in range(packet_count):
        # Create packet with sequence number and timestamp
        send_time_ns = time.time_ns()
        packet = struct.pack("!IQ", seq, send_time_ns) + b"PQC-TEST-PAYLOAD-" + str(seq).encode()
        
        # Send to GCS proxy plaintext input
        tx_sock.sendto(packet, (HOST, GCS_PLAINTEXT_TX))
        sent += 1
        
        # Try to receive echoed response
        try:
            response, addr = rx_sock.recvfrom(65535)
            recv_time_ns = time.time_ns()
            
            if len(response) >= 12:
                resp_seq, resp_ts = struct.unpack("!IQ", response[:12])
                rtt_ns = recv_time_ns - resp_ts
                rtt_ms = rtt_ns / 1_000_000
                rtt_samples.append(rtt_ms)
                received += 1
                
                if seq % 10 == 0:
                    print(f"[GCS] Packet {seq}: RTT={rtt_ms:.2f}ms")
        except socket.timeout:
            print(f"[GCS] Packet {seq}: TIMEOUT (no response)")
        
        if delay_ms > 0:
            time.sleep(delay_ms / 1000.0)
    
    tx_sock.close()
    rx_sock.close()
    
    # Print summary
    print("\n" + "=" * 60)
    print("LOCALHOST LOOP TEST SUMMARY")
    print("=" * 60)
    print(f"Packets sent:     {sent}")
    print(f"Packets received: {received}")
    print(f"Delivery rate:    {100.0 * received / max(1, sent):.1f}%")
    
    if rtt_samples:
        avg_rtt = sum(rtt_samples) / len(rtt_samples)
        min_rtt = min(rtt_samples)
        max_rtt = max(rtt_samples)
        print(f"RTT (min/avg/max): {min_rtt:.2f} / {avg_rtt:.2f} / {max_rtt:.2f} ms")
    
    print("=" * 60)
    return sent, received


def main():
    print("=" * 60)
    print("PQC SECURE TUNNEL - LOCALHOST LOOP TEST")
    print("=" * 60)
    print("\nData flow:")
    print(f"  GCS plaintext ({GCS_PLAINTEXT_TX}) -> GCS proxy")
    print(f"  -> Encrypted UDP (46011/46012)")
    print(f"  -> Drone proxy -> Drone plaintext ({DRONE_PLAINTEXT_RX})")
    print(f"  -> Echo server -> Drone plaintext ({DRONE_PLAINTEXT_TX})")
    print(f"  -> Drone proxy -> Encrypted UDP")
    print(f"  -> GCS proxy -> GCS plaintext ({GCS_PLAINTEXT_RX})")
    print("")
    
    # Wait for proxies to be ready
    print("Waiting 2 seconds for proxies to be ready...")
    time.sleep(2)
    
    # Start echo server in background
    stop_event = threading.Event()
    echo_stats = {"echo_received": 0, "echo_sent": 0}
    echo_thread = threading.Thread(
        target=run_drone_echo_server, 
        args=(stop_event, echo_stats),
        daemon=True
    )
    echo_thread.start()
    
    time.sleep(0.5)  # Give echo server time to bind
    
    try:
        # Run the test - send 50 packets with 50ms delay
        sent, received = run_gcs_sender_receiver(packet_count=50, delay_ms=50)
        
        # Print echo stats
        print(f"\nEcho server stats:")
        print(f"  Packets received at drone: {echo_stats['echo_received']}")
        print(f"  Packets echoed back: {echo_stats['echo_sent']}")
        
        if received > 0:
            print("\n✅ SUCCESS: Complete encryption/decryption loop working!")
        else:
            print("\n❌ FAILED: No packets received back through the loop")
            print("   Check that both GCS and Drone proxies are running")
            
    finally:
        stop_event.set()
        echo_thread.join(timeout=2.0)


if __name__ == "__main__":
    main()

==================================================

test_multiple_suites.py
==================================================
#!/usr/bin/env python3
"""Test multiple PQC cipher suites on localhost."""

import os
import sys
import socket
import subprocess
import threading
import time

# Set environment
os.environ["DRONE_HOST"] = "127.0.0.1"
os.environ["GCS_HOST"] = "127.0.0.1"
os.environ["ENABLE_PACKET_TYPE"] = "0"

ROOT = os.path.dirname(os.path.abspath(__file__))
PYTHON = sys.executable

# Ports
GCS_TX = 47001
GCS_RX = 47002
DRONE_RX = 47004
DRONE_TX = 47003

# Test suites with different algorithms (all have keys in secrets/matrix/)
TEST_SUITES = [
    "cs-mlkem512-aesgcm-falcon512",        # ML-KEM-512 + AES-GCM + Falcon-512 (NIST L1)
    "cs-hqc128-aesgcm-mldsa44",            # HQC-128 + AES-GCM + ML-DSA-44 (NIST L1)
    "cs-hqc192-aesgcm-mldsa65",            # HQC-192 + AES-GCM + ML-DSA-65 (NIST L3)
]


def wait_for_port_free(port, timeout=10):
    """Wait for a port to be free."""
    start = time.time()
    while time.time() - start < timeout:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            sock.bind(("127.0.0.1", port))
            sock.close()
            return True
        except OSError:
            time.sleep(0.5)
    return False


def test_suite(suite_id):
    """Test a single cipher suite."""
    gcs_proc = None
    drone_proc = None
    echo_running = threading.Event()
    echo_running.set()
    echo_stats = {"rx": 0, "tx": 0}
    
    # Check for keys
    key_file = os.path.join(ROOT, f"secrets/matrix/{suite_id}/gcs_signing.key")
    pub_file = os.path.join(ROOT, f"secrets/matrix/{suite_id}/gcs_signing.pub")
    
    if not os.path.exists(key_file) or not os.path.exists(pub_file):
        return None, f"Missing keys for {suite_id}"
    
    def run_echo():
        rx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        rx.bind(("127.0.0.1", DRONE_RX))
        rx.settimeout(1.0)
        tx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        while echo_running.is_set():
            try:
                data, addr = rx.recvfrom(65535)
                echo_stats["rx"] += 1
                tx.sendto(data, ("127.0.0.1", DRONE_TX))
                echo_stats["tx"] += 1
            except socket.timeout:
                continue
        rx.close()
        tx.close()
    
    try:
        # Start GCS proxy
        gcs_proc = subprocess.Popen(
            [PYTHON, "-m", "core.run_proxy", "gcs",
             "--gcs-secret-file", key_file,
             "--suite", suite_id,
             "--stop-seconds", "30", "--quiet"],
            env=os.environ.copy(),
            cwd=ROOT,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT
        )
        time.sleep(2)
        
        if gcs_proc.poll() is not None:
            out = gcs_proc.stdout.read().decode() if gcs_proc.stdout else ""
            return False, f"GCS proxy exited: {out[:200]}"
        
        # Start Drone proxy
        drone_proc = subprocess.Popen(
            [PYTHON, "-m", "core.run_proxy", "drone",
             "--peer-pubkey-file", pub_file,
             "--suite", suite_id,
             "--stop-seconds", "25", "--quiet"],
            env=os.environ.copy(),
            cwd=ROOT,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT
        )
        time.sleep(3)
        
        if drone_proc.poll() is not None:
            out = drone_proc.stdout.read().decode() if drone_proc.stdout else ""
            return False, f"Drone proxy exited: {out[:200]}"
        
        # Start echo server
        echo_thread = threading.Thread(target=run_echo, daemon=True)
        echo_thread.start()
        time.sleep(0.5)
        
        # Run test packets
        tx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        rx.bind(("127.0.0.1", GCS_RX))
        rx.settimeout(2.0)
        
        sent = 0
        received = 0
        
        for i in range(10):
            payload = f"TEST-{i:04d}".encode()
            tx.sendto(payload, ("127.0.0.1", GCS_TX))
            sent += 1
            
            try:
                data, addr = rx.recvfrom(65535)
                received += 1
            except socket.timeout:
                pass
            
            time.sleep(0.05)
        
        tx.close()
        rx.close()
        
        # Stop echo
        echo_running.clear()
        time.sleep(0.3)
        
        if received >= 8:  # Allow some packet loss
            return True, f"OK ({received}/{sent} packets)"
        else:
            return False, f"Low delivery ({received}/{sent} packets)"
        
    except Exception as e:
        return False, str(e)
    finally:
        echo_running.clear()
        
        if drone_proc and drone_proc.poll() is None:
            drone_proc.terminate()
            try:
                drone_proc.wait(timeout=3)
            except:
                drone_proc.kill()
        
        if gcs_proc and gcs_proc.poll() is None:
            gcs_proc.terminate()
            try:
                gcs_proc.wait(timeout=3)
            except:
                gcs_proc.kill()


def main():
    print("=" * 70)
    print("PQC Secure Tunnel - Multi-Suite Localhost Test")
    print("=" * 70)
    print()
    
    results = []
    
    for suite_id in TEST_SUITES:
        print(f"Testing: {suite_id}")
        print("-" * 70)
        
        # Wait for ports to be free
        for port in [GCS_RX, GCS_TX, DRONE_RX, DRONE_TX]:
            if not wait_for_port_free(port, timeout=5):
                print(f"  [WARN] Port {port} still in use, proceeding anyway...")
        
        success, message = test_suite(suite_id)
        
        if success is None:
            status = "SKIP"
            results.append((suite_id, "SKIP", message))
        elif success:
            status = "PASS"
            results.append((suite_id, "PASS", message))
        else:
            status = "FAIL"
            results.append((suite_id, "FAIL", message))
        
        print(f"  [{status}] {message}")
        print()
        
        # Wait between suites
        time.sleep(3)
    
    # Summary
    print("=" * 70)
    print("SUMMARY")
    print("=" * 70)
    
    passed = sum(1 for r in results if r[1] == "PASS")
    failed = sum(1 for r in results if r[1] == "FAIL")
    skipped = sum(1 for r in results if r[1] == "SKIP")
    
    for suite_id, status, message in results:
        print(f"  [{status:4}] {suite_id}")
    
    print()
    print(f"Total: {len(results)}, Passed: {passed}, Failed: {failed}, Skipped: {skipped}")
    print("=" * 70)
    
    if failed > 0:
        return 1
    return 0


if __name__ == "__main__":
    time.sleep(2)  # Wait for ports to release
    sys.exit(main())

==================================================

test_schedulers.py
==================================================
#!/usr/bin/env python3
"""Test scheduler pair on localhost - runs both sdrone and sgcs."""

import os
import sys
import subprocess
import time
import signal

# Set environment for localhost
os.environ["DRONE_HOST"] = "127.0.0.1"
os.environ["GCS_HOST"] = "127.0.0.1"
os.environ["DRONE_CONTROL_HOST"] = "127.0.0.1"
os.environ["ENABLE_PACKET_TYPE"] = "0"

ROOT = os.path.dirname(os.path.abspath(__file__))
PYTHON = sys.executable


def main():
    print("=" * 70)
    print("Scheduler Test - Running sdrone + sgcs on localhost")
    print("=" * 70)
    
    drone_proc = None
    gcs_proc = None
    
    try:
        # Start drone scheduler first (it waits for GCS)
        print("\n[Test] Starting drone scheduler...")
        drone_proc = subprocess.Popen(
            [PYTHON, "scheduler/sdrone.py"],
            env=os.environ.copy(),
            cwd=ROOT,
        )
        
        # Wait for drone control server to be ready
        time.sleep(3)
        
        if drone_proc.poll() is not None:
            print("[Test] ERROR: Drone scheduler exited early!")
            return 1
        
        print("[Test] Drone scheduler running")
        
        # Run GCS scheduler (blocking)
        print("\n[Test] Starting GCS scheduler...")
        gcs_proc = subprocess.Popen(
            [
                PYTHON, "scheduler/sgcs.py",
                "--suites", "cs-mlkem768-aesgcm-mldsa65,cs-mlkem512-aesgcm-mldsa44",
                "--duration", "10",
                "--bandwidth", "110",
            ],
            env=os.environ.copy(),
            cwd=ROOT,
        )
        
        # Wait for GCS to complete
        gcs_proc.wait()
        
        print("\n[Test] GCS scheduler completed with code:", gcs_proc.returncode)
        
        return gcs_proc.returncode
        
    except KeyboardInterrupt:
        print("\n[Test] Interrupted")
        return 1
    finally:
        # Cleanup
        print("\n[Test] Cleaning up...")
        
        if gcs_proc and gcs_proc.poll() is None:
            gcs_proc.terminate()
            try:
                gcs_proc.wait(timeout=5)
            except:
                gcs_proc.kill()
        
        if drone_proc and drone_proc.poll() is None:
            drone_proc.terminate()
            try:
                drone_proc.wait(timeout=5)
            except:
                drone_proc.kill()
        
        print("[Test] Done")


if __name__ == "__main__":
    sys.exit(main())

==================================================

test_simple_loop.py
==================================================
#!/usr/bin/env python3
"""Simple all-in-one localhost loop test."""

import os
import sys
import socket
import subprocess
import threading
import time

# Set environment
os.environ["DRONE_HOST"] = "127.0.0.1"
os.environ["GCS_HOST"] = "127.0.0.1"
os.environ["ENABLE_PACKET_TYPE"] = "0"

ROOT = os.path.dirname(os.path.abspath(__file__))
PYTHON = sys.executable

# Ports
GCS_TX = 47001
GCS_RX = 47002
DRONE_RX = 47004
DRONE_TX = 47003

def main():
    print("=" * 60)
    print("PQC Secure Tunnel - Localhost Loop Test")
    print("=" * 60)
    
    gcs_proc = None
    drone_proc = None
    echo_running = threading.Event()
    echo_running.set()
    echo_stats = {"rx": 0, "tx": 0}
    
    def run_echo():
        rx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        rx.bind(("127.0.0.1", DRONE_RX))
        rx.settimeout(1.0)
        tx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        print(f"[Echo] Listening on {DRONE_RX}")
        while echo_running.is_set():
            try:
                data, addr = rx.recvfrom(65535)
                echo_stats["rx"] += 1
                tx.sendto(data, ("127.0.0.1", DRONE_TX))
                echo_stats["tx"] += 1
                if echo_stats["rx"] <= 3:
                    print(f"[Echo] Echoed {len(data)} bytes")
            except socket.timeout:
                continue
        rx.close()
        tx.close()
    
    try:
        # Start GCS proxy
        print("[Main] Starting GCS proxy...")
        gcs_proc = subprocess.Popen(
            [PYTHON, "-m", "core.run_proxy", "gcs",
             "--gcs-secret-file", "secrets/localtest/gcs_signing.key",
             "--suite", "cs-mlkem768-aesgcm-mldsa65",
             "--stop-seconds", "45", "--quiet"],
            env=os.environ.copy(),
            cwd=ROOT,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT
        )
        time.sleep(2)
        
        if gcs_proc.poll() is not None:
            print("[Main] ERROR: GCS proxy exited early!")
            out = gcs_proc.stdout.read().decode() if gcs_proc.stdout else ""
            print(out)
            return 1
        
        # Start Drone proxy
        print("[Main] Starting Drone proxy...")
        drone_proc = subprocess.Popen(
            [PYTHON, "-m", "core.run_proxy", "drone",
             "--peer-pubkey-file", "secrets/localtest/gcs_signing.pub",
             "--suite", "cs-mlkem768-aesgcm-mldsa65",
             "--stop-seconds", "40", "--quiet"],
            env=os.environ.copy(),
            cwd=ROOT,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT
        )
        time.sleep(3)
        
        if drone_proc.poll() is not None:
            print("[Main] ERROR: Drone proxy exited early!")
            out = drone_proc.stdout.read().decode() if drone_proc.stdout else ""
            print(out)
            return 1
        
        print("[Main] Proxies running - checking handshake...")
        
        # Wait a moment for handshake
        time.sleep(2)
        
        # Check if still running
        if gcs_proc.poll() is not None or drone_proc.poll() is not None:
            print("[Main] ERROR: A proxy exited during handshake!")
            return 1
        
        print("[Main] Handshake complete - starting echo server...")
        
        # Start echo server
        echo_thread = threading.Thread(target=run_echo, daemon=True)
        echo_thread.start()
        time.sleep(0.5)
        
        # Run the test
        print("\n" + "-" * 60)
        print(f"[Test] Sending 20 packets to GCS TX port {GCS_TX}")
        print(f"[Test] Expecting replies on GCS RX port {GCS_RX}")
        print("-" * 60)
        
        tx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        rx.bind(("127.0.0.1", GCS_RX))
        rx.settimeout(2.0)
        
        sent = 0
        received = 0
        
        for i in range(20):
            payload = f"PACKET-{i:04d}".encode()
            tx.sendto(payload, ("127.0.0.1", GCS_TX))
            sent += 1
            
            try:
                data, addr = rx.recvfrom(65535)
                received += 1
                if i < 5:
                    print(f"[Test] Pkt {i}: OK - {data.decode()}")
            except socket.timeout:
                if i < 5:
                    print(f"[Test] Pkt {i}: TIMEOUT")
            
            time.sleep(0.05)
        
        tx.close()
        rx.close()
        
        # Stop echo
        echo_running.clear()
        time.sleep(0.5)
        
        print("\n" + "=" * 60)
        print("RESULTS")
        print("=" * 60)
        print(f"Packets sent:     {sent}")
        print(f"Packets received: {received}")
        print(f"Echo server:      RX={echo_stats['rx']}, TX={echo_stats['tx']}")
        print("=" * 60)
        
        if received > 0:
            print("\n[OK] SUCCESS! PQC encrypted loop verified!")
            print("Data path: GCS -> encrypt (ML-KEM-768 + AES-GCM) -> network")
            print("           -> Drone decrypt -> Echo -> encrypt -> network")
            print("           -> GCS decrypt -> received!")
            return 0
        else:
            print("\n[FAIL] No packets completed the round trip")
            return 1
        
    except Exception as e:
        print(f"\n[ERROR] {e}")
        import traceback
        traceback.print_exc()
        return 1
    finally:
        echo_running.clear()
        
        if drone_proc and drone_proc.poll() is None:
            drone_proc.terminate()
            try:
                drone_proc.wait(timeout=3)
            except:
                drone_proc.kill()
        
        if gcs_proc and gcs_proc.poll() is None:
            gcs_proc.terminate()
            try:
                gcs_proc.wait(timeout=3)
            except:
                gcs_proc.kill()
        
        print("\n[Main] Cleanup complete.")


if __name__ == "__main__":
    # Wait for any ports to be released
    time.sleep(2)
    sys.exit(main())

==================================================

test_sscheduler.py
==================================================
#!/usr/bin/env python3
"""
Test script for sscheduler (drone-controlled scheduler)

Runs sgcs (follower) first, then sdrone (controller) to test
the reversed control flow where drone commands GCS.
"""

import os
import sys
import time
import subprocess
import signal

def main():
    print("=" * 70)
    print("sScheduler Test - Drone Controller + GCS Follower on localhost")
    print("=" * 70)
    print()
    
    # Set environment
    env = os.environ.copy()
    env["DRONE_HOST"] = "127.0.0.1"
    env["GCS_HOST"] = "127.0.0.1"
    env["GCS_CONTROL_HOST"] = "127.0.0.1"
    env["ENABLE_PACKET_TYPE"] = "0"
    
    # Start GCS scheduler (follower) first - it needs to be listening
    print("[Test] Starting GCS scheduler (follower)...")
    gcs_proc = subprocess.Popen(
        [sys.executable, "-m", "sscheduler.sgcs"],
        env=env,
        cwd=os.path.dirname(os.path.abspath(__file__)),
    )
    time.sleep(3)
    
    if gcs_proc.poll() is not None:
        print(f"[Test] ERROR: GCS scheduler exited with code {gcs_proc.returncode}")
        return 1
    
    print("[Test] GCS scheduler running")
    print()
    
    # Start drone scheduler (controller)
    # Use fast ML-KEM suites (ClassicMcEliece are too slow)
    print("[Test] Starting drone scheduler (controller)...")
    drone_proc = subprocess.Popen(
        [sys.executable, "-m", "sscheduler.sdrone", "--max-suites", "2", "--nist-level", "L3"],
        env=env,
        cwd=os.path.dirname(os.path.abspath(__file__)),
    )
    
    # Wait for drone to complete (it's the controller)
    try:
        drone_exit = drone_proc.wait(timeout=300)
        print()
        print(f"[Test] Drone scheduler completed with code: {drone_exit}")
    except subprocess.TimeoutExpired:
        print("[Test] Drone scheduler timed out")
        drone_proc.terminate()
        drone_exit = 1
    
    print()
    print("[Test] Cleaning up...")
    
    # Stop GCS
    if gcs_proc.poll() is None:
        gcs_proc.terminate()
        try:
            gcs_proc.wait(timeout=5)
        except subprocess.TimeoutExpired:
            gcs_proc.kill()
    
    print("[Test] Done")
    return drone_exit

if __name__ == "__main__":
    sys.exit(main())

==================================================

auto\drone_follower.py
==================================================
#!/usr/bin/env python3
"""Drone follower/loopback agent driven entirely by core configuration.

This script launches the drone proxy, exposes the TCP control channel for the
GCS scheduler, and runs the plaintext UDP echo used to validate the encrypted
path. All network endpoints originate from :mod:`core.config`. Test behaviour
can be tuned via optional CLI flags (e.g. to disable perf monitors), but no
network parameters are duplicated here.
"""

from __future__ import annotations

import sys
from pathlib import Path


def _ensure_core_importable() -> Path:
    """Guarantee the repository root is on sys.path before importing core."""

    root = Path(__file__).resolve().parents[1]
    root_str = str(root)
    if root_str not in sys.path:
        sys.path.insert(0, root_str)
    try:
        __import__("core")
    except ModuleNotFoundError as exc:
        raise RuntimeError(
            f"Unable to import 'core'; repo root {root} missing from sys.path."
        ) from exc
    return root


ROOT = _ensure_core_importable()

import argparse
import csv
import json
import math
import os
import platform
import shlex
import signal
import socket
import struct
import subprocess
import threading
import time
import queue
from collections import deque
from datetime import datetime, timezone
from copy import deepcopy
from typing import IO, Callable, Dict, Iterable, Optional, Tuple

from dataclasses import dataclass


def optimize_cpu_performance(target_khz: int = 1800000) -> None:
    governors = list(Path("/sys/devices/system/cpu").glob("cpu[0-9]*/cpufreq"))
    for governor_dir in governors:
        gov = governor_dir / "scaling_governor"
        min_freq = governor_dir / "scaling_min_freq"
        max_freq = governor_dir / "scaling_max_freq"
        try:
            if gov.exists():
                gov.write_text("performance\n", encoding="utf-8")
            if min_freq.exists():
                min_freq.write_text(f"{target_khz}\n", encoding="utf-8")
            if max_freq.exists():
                current_max = int(max_freq.read_text().strip())
                if current_max < target_khz:
                    max_freq.write_text(f"{target_khz}\n", encoding="utf-8")
        except PermissionError:
            print("[follower] insufficient permissions to adjust CPU governor")
        except Exception as exc:
            print(f"[follower] governor tuning failed: {exc}")


import psutil

from core.config import CONFIG
from core import suites as suites_mod
from core.power_monitor import (
    PowerMonitor,
    PowerMonitorUnavailable,
    PowerSummary,
    create_power_monitor,
)

from bench_models import calculate_predicted_flight_constraint


_CONTROL_HOST_FALLBACK = CONFIG.get("DRONE_HOST", "127.0.0.1")
CONTROL_HOST = str(
    CONFIG.get("DRONE_CONTROL_HOST")
    or os.getenv("DRONE_CONTROL_HOST")
    or _CONTROL_HOST_FALLBACK
).strip() or str(_CONTROL_HOST_FALLBACK)
CONTROL_PORT = int(CONFIG.get("DRONE_CONTROL_PORT", 48080))

APP_BIND_HOST = CONFIG.get("DRONE_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
APP_SEND_HOST = CONFIG.get("DRONE_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("DRONE_PLAINTEXT_TX", 47003))

DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

TELEMETRY_DEFAULT_HOST = (
    CONFIG.get("DRONE_TELEMETRY_HOST")
    or CONFIG.get("GCS_HOST")
    or "127.0.0.1"
)
TELEMETRY_DEFAULT_PORT = int(
    CONFIG.get("DRONE_TELEMETRY_PORT")
    or CONFIG.get("GCS_TELEMETRY_PORT")
    or 52080
)

OUTDIR = ROOT / "logs/auto/drone"
MARK_DIR = OUTDIR / "marks"
SECRETS_DIR = ROOT / "secrets/matrix"

PI4_TARGET_KHZ = 1_800_000
PI5_TARGET_KHZ = 2_400_000

DEFAULT_MONITOR_BASE = Path(
    CONFIG.get("DRONE_MONITOR_OUTPUT_BASE")
    or os.getenv("DRONE_MONITOR_OUTPUT_BASE", "/home/dev/research/output/drone")
)
LOG_INTERVAL_MS = 100

GRAVITY = 9.80665  # m/s^2, standard gravity for synthetic flight modeling

PERF_EVENTS = "task-clock,cycles,instructions,cache-misses,branch-misses,context-switches,branches"

_VCGENCMD_WARNING_EMITTED = False


def _warn_vcgencmd_unavailable() -> None:
    global _VCGENCMD_WARNING_EMITTED
    if not _VCGENCMD_WARNING_EMITTED:
        print("[monitor] vcgencmd not available; thermal metrics disabled")
        _VCGENCMD_WARNING_EMITTED = True


def _merge_defaults(defaults: dict, override: Optional[dict]) -> dict:
    result = deepcopy(defaults)
    if isinstance(override, dict):
        for key, value in override.items():
            if isinstance(value, dict) and isinstance(result.get(key), dict):
                merged = result[key].copy()
                merged.update(value)
                result[key] = merged
            else:
                result[key] = value
    return result

AUTO_DRONE_DEFAULTS = {
    "session_prefix": "session",
    "monitors_enabled": True,
    "cpu_optimize": True,
    "telemetry_enabled": True,
    "telemetry_host": None,
    "telemetry_port": TELEMETRY_DEFAULT_PORT,
    "monitor_output_base": None,
    "power_env": {},
    "initial_suite": None,
    "mock_mass_kg": 6.5,
    "kinematics_horizontal_mps": 13.0,
    "kinematics_vertical_mps": 3.5,
    "kinematics_cycle_s": 18.0,
    "kinematics_yaw_rate_dps": 45.0,
}

AUTO_DRONE_CONFIG = _merge_defaults(AUTO_DRONE_DEFAULTS, CONFIG.get("AUTO_DRONE"))


def _collect_capabilities_snapshot() -> dict:
    """Probe local crypto/telemetry capabilities for scheduler negotiation."""

    timestamp_ns = time.time_ns()

    try:
        enabled_kems = {name for name in suites_mod.enabled_kems()}
        kem_probe_error = ""
    except Exception as exc:  # pragma: no cover - depends on oqs installation
        enabled_kems = set()
        kem_probe_error = str(exc)

    try:
        enabled_sigs = {name for name in suites_mod.enabled_sigs()}
        sig_probe_error = ""
    except Exception as exc:  # pragma: no cover - depends on oqs installation
        enabled_sigs = set()
        sig_probe_error = str(exc)

    available_aeads = set(suites_mod.available_aead_tokens())
    missing_aead_reasons = suites_mod.unavailable_aead_reasons()

    suite_map = suites_mod.list_suites()
    supported_suites: list[str] = []
    unsupported_suites: list[dict[str, object]] = []

    all_kems = set()
    all_sigs = set()

    for suite_id, info in sorted(suite_map.items()):
        kem_name = info.get("kem_name")
        sig_name = info.get("sig_name")
        aead_token = info.get("aead_token")

        if kem_name:
            all_kems.add(kem_name)
        if sig_name:
            all_sigs.add(sig_name)

        reasons: list[str] = []
        details: dict[str, object] = {
            "kem_name": kem_name,
            "sig_name": sig_name,
            "aead_token": aead_token,
        }

        if enabled_kems and kem_name not in enabled_kems:
            reasons.append("kem_unavailable")
        if enabled_sigs and sig_name not in enabled_sigs:
            reasons.append("sig_unavailable")
        if available_aeads and aead_token not in available_aeads:
            reasons.append("aead_unavailable")
            hint = missing_aead_reasons.get(str(aead_token))
            if hint:
                details["aead_hint"] = hint

        if reasons:
            unsupported_suites.append(
                {
                    "suite": suite_id,
                    "reasons": reasons,
                    "details": details,
                }
            )
            continue

        supported_suites.append(suite_id)

    missing_kems = sorted(kem for kem in (all_kems - enabled_kems)) if enabled_kems else sorted(all_kems)
    missing_sigs = sorted(sig for sig in (all_sigs - enabled_sigs)) if enabled_sigs else sorted(all_sigs)

    oqs_info: dict[str, object] = {}
    try:  # pragma: no cover - depends on oqs availability
        import oqs  # type: ignore

        oqs_info["python_version"] = getattr(oqs, "__version__", "unknown")
        get_version = getattr(oqs, "get_version", None)
        if callable(get_version):
            oqs_info["library_version"] = get_version()
        get_build_config = getattr(oqs, "get_build_config", None)
        if callable(get_build_config):
            try:
                build_cfg = get_build_config()
                oqs_info["build_config"] = build_cfg if isinstance(build_cfg, dict) else repr(build_cfg)
            except Exception as exc:  # pragma: no cover - defensive path
                oqs_info["build_config_error"] = str(exc)
    except Exception as exc:  # pragma: no cover - oqs missing
        oqs_info["error"] = str(exc)

    return {
        "timestamp_ns": timestamp_ns,
        "supported_suites": supported_suites,
        "unsupported_suites": unsupported_suites,
        "enabled_kems": sorted(enabled_kems),
        "enabled_sigs": sorted(enabled_sigs),
        "available_aeads": sorted(available_aeads),
        "missing_aead_reasons": missing_aead_reasons,
        "missing_kems": missing_kems,
        "missing_sigs": missing_sigs,
        "kem_probe_error": kem_probe_error,
        "sig_probe_error": sig_probe_error,
        "suite_registry_size": len(suite_map),
        "oqs": oqs_info,
    }


def _parse_args(argv: Optional[list[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Drone follower controller")
    parser.add_argument(
        "--5",
        "--pi5",
        dest="pi5",
        action="store_true",
        help="Treat hardware as Raspberry Pi 5 (defaults to Pi 4 governor settings)",
    )
    parser.add_argument(
        "--pi4",
        dest="pi5",
        action="store_false",
        help=argparse.SUPPRESS,
    )
    parser.set_defaults(pi5=False)
    return parser.parse_args(argv)


def ts() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def log_runtime_environment(component: str) -> None:
    """Emit interpreter context to help debug sudo/venv mismatches."""

    preview = ";".join(sys.path[:5])
    print(f"[{ts()}] {component} python_exe={sys.executable}")
    print(f"[{ts()}] {component} cwd={Path.cwd()}")
    print(f"[{ts()}] {component} sys.path_prefix={preview}")


def _collect_hardware_context() -> dict:
    """Gather hardware, OS, and toolchain context for reproducibility logs."""

    info: dict[str, object] = {
        "platform": platform.platform(),
        "machine": platform.machine(),
        "processor": platform.processor(),
        "python_version": platform.python_version(),
        "python_compiler": platform.python_compiler(),
        "python_build": platform.python_build(),
        "executable": sys.executable,
    }

    try:
        uname = os.uname()  # type: ignore[attr-defined]
    except AttributeError:
        uname = None
    if uname is not None:
        info["uname"] = {
            "sysname": uname.sysname,
            "nodename": uname.nodename,
            "release": uname.release,
            "version": uname.version,
            "machine": uname.machine,
        }

    # Capture relevant environment hints for compiler optimisation flags.
    flag_env_vars = {
        key: os.environ.get(key)
        for key in (
            "CFLAGS",
            "CXXFLAGS",
            "LDFLAGS",
            "OQS_OPT_FLAGS",
            "OQS_CFLAGS",
            "OQS_LDFLAGS",
            "OQS_OPT_LEVEL",
            "OQS_OPTIMIZATION",
        )
        if os.environ.get(key)
    }
    if flag_env_vars:
        info["build_flags"] = flag_env_vars

    try:
        import oqs  # type: ignore

        info["oqs_python_version"] = getattr(oqs, "__version__", "unknown")
        get_version = getattr(oqs, "get_version", None)
        if callable(get_version):
            info["oqs_library_version"] = get_version()
        get_build_config = getattr(oqs, "get_build_config", None)
        if callable(get_build_config):
            build_config = get_build_config()
            try:
                json.dumps(build_config)
                info["oqs_build_config"] = build_config
            except TypeError:
                info["oqs_build_config"] = repr(build_config)

            optimization_hint: Optional[str] = None
            if isinstance(build_config, dict):
                for candidate_key in (
                    "OQS_OPT_FLAG",
                    "OQS_OPT_FLAGS",
                    "OPT_FLAGS",
                    "OPTIMIZATION_FLAGS",
                    "CFLAGS",
                    "CMAKE_C_FLAGS",
                    "CMAKE_CXX_FLAGS",
                ):
                    value = build_config.get(candidate_key)
                    if isinstance(value, str) and value.strip():
                        optimization_hint = value.strip()
                        break
                if optimization_hint is None:
                    cmake_cache = build_config.get("CMAKE_ARGS")
                    if isinstance(cmake_cache, str) and cmake_cache:
                        for token in cmake_cache.split():
                            if token.startswith("-O"):
                                optimization_hint = token
                                break
            if optimization_hint is None and flag_env_vars:
                for key in ("OQS_OPT_FLAGS", "CFLAGS", "OQS_CFLAGS"):
                    candidate = flag_env_vars.get(key)
                    if candidate:
                        optimization_hint = candidate
                        break
            if optimization_hint:
                info["oqs_optimization_hint"] = optimization_hint
    except Exception as exc:  # pragma: no cover - diagnostic only
        info["oqs_info_error"] = str(exc)

    return info


def _record_hardware_context(session_dir: Path, telemetry: Optional[TelemetryPublisher]) -> None:
    """Persist hardware context to disk and telemetry for audit trails."""

    context = _collect_hardware_context()
    try:
        session_dir.mkdir(parents=True, exist_ok=True)
        target = session_dir / "hardware_context.json"
        target.write_text(json.dumps(context, indent=2), encoding="utf-8")
        print(f"[follower] hardware context -> {target}")
    except Exception as exc:
        print(f"[follower] failed to write hardware context: {exc}")

    if telemetry is not None:
        try:
            telemetry.publish("hardware_context", {"timestamp_ns": time.time_ns(), **context})
        except Exception:
            pass


@dataclass
class _TelemetryClient:
    conn: socket.socket
    writer: IO[str]
    peer: str


class TelemetryPublisher:
    """Server-side telemetry broadcaster that mirrors the control channel semantics."""

    def __init__(self, host: str, port: int, session_id: str) -> None:
        self.host = host
        self.port = port
        self.session_id = session_id
        self.stop_event = threading.Event()
        self.lock = threading.Lock()
        self.clients: Dict[socket.socket, _TelemetryClient] = {}
        self.server: Optional[socket.socket] = None
        self.accept_thread: Optional[threading.Thread] = None
        self._status_path: Optional[Path] = None
        self._last_status_flush = 0.0
        self._connected_once = False

    def start(self) -> None:
        if self.server is not None:
            return
        self._start_server()

    def publish(self, kind: str, payload: dict) -> None:
        if self.stop_event.is_set():
            return
        message = {
            "session_id": self.session_id,
            "kind": kind,
            **payload,
        }
        message["component"] = "drone_follower"
        message.setdefault("timestamp_ns", time.time_ns())
        text = json.dumps(message) + "\n"
        with self.lock:
            clients = list(self.clients.values())
        for client in clients:
            try:
                client.writer.write(text)
                client.writer.flush()
            except Exception:
                self._remove_client(client, reason="send_error")

    def stop(self) -> None:
        self.stop_event.set()
        if self.accept_thread and self.accept_thread.is_alive():
            self.accept_thread.join(timeout=2.0)
        with self.lock:
            clients = list(self.clients.values())
            self.clients.clear()
        for client in clients:
            try:
                client.writer.close()
            except Exception:
                pass
            try:
                client.conn.close()
            except Exception:
                pass
        if self.server is not None:
            try:
                self.server.close()
            except Exception:
                pass
            self.server = None
        self._emit_status("stopped", active_clients=0)

    def configure_status_sink(self, path: Path) -> None:
        self._status_path = path
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass
        self._emit_status("init", active_clients=len(self.clients))

    def _emit_status(self, event: str, **extra: object) -> None:
        if self._status_path is None:
            return
        payload = {
            "event": event,
            "timestamp_ns": time.time_ns(),
            "session_id": self.session_id,
            "host": self.host,
            "port": self.port,
            "connected_once": self._connected_once,
            "active_clients": len(self.clients),
        }
        if extra:
            payload.update(extra)
        try:
            self._status_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
            self._last_status_flush = time.monotonic()
        except Exception:
            pass

    def _start_server(self) -> None:
        try:
            addrinfo = socket.getaddrinfo(
                self.host,
                self.port,
                0,
                socket.SOCK_STREAM,
                proto=0,
                flags=socket.AI_PASSIVE if not self.host else 0,
            )
        except socket.gaierror as exc:
            raise OSError(f"telemetry bind failed for {self.host}:{self.port}: {exc}") from exc

        last_exc: Optional[Exception] = None
        for family, socktype, proto, _canon, sockaddr in addrinfo:
            try:
                srv = socket.socket(family, socktype, proto)
                try:
                    srv.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    if family == socket.AF_INET6:
                        try:
                            srv.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
                        except OSError:
                            pass
                    srv.bind(sockaddr)
                    srv.listen(5)
                    srv.settimeout(0.5)
                except Exception:
                    srv.close()
                    raise
            except Exception as exc:
                last_exc = exc
                continue
            self.server = srv
            break

        if self.server is None:
            message = last_exc or RuntimeError("no suitable address family")
            raise OSError(f"telemetry bind failed for {self.host}:{self.port}: {message}")

        print(f"[follower] telemetry listening on {self.host}:{self.port}", flush=True)
        self._emit_status("listening", active_clients=0)
        self.accept_thread = threading.Thread(target=self._accept_loop, daemon=True)
        self.accept_thread.start()

    def _accept_loop(self) -> None:
        assert self.server is not None
        while not self.stop_event.is_set():
            try:
                conn, addr = self.server.accept()
            except socket.timeout:
                continue
            except OSError:
                if self.stop_event.is_set():
                    break
                continue
            peer = f"{addr[0]}:{addr[1]}"
            client = self._register_client(conn, peer)
            if client is None:
                continue
            threading.Thread(target=self._monitor_client, args=(client,), daemon=True).start()

    def _register_client(self, conn: socket.socket, peer: str) -> Optional[_TelemetryClient]:
        try:
            writer = conn.makefile("w", encoding="utf-8", buffering=1)
        except Exception:
            conn.close()
            return None
        hello = {
            "session_id": self.session_id,
            "kind": "telemetry_hello",
            "timestamp_ns": time.time_ns(),
        }
        try:
            writer.write(json.dumps(hello) + "\n")
            writer.flush()
        except Exception:
            try:
                writer.close()
            except Exception:
                pass
            conn.close()
            return None
        client = _TelemetryClient(conn=conn, writer=writer, peer=peer)
        with self.lock:
            self.clients[conn] = client
        self._connected_once = True
        print(f"[follower] telemetry client {peer} connected", flush=True)
        self._emit_status("connected", peer=peer, active_clients=len(self.clients))
        return client

    def _monitor_client(self, client: _TelemetryClient) -> None:
        conn = client.conn
        try:
            while not self.stop_event.is_set():
                data = conn.recv(1024)
                if not data:
                    break
        except Exception:
            pass
        finally:
            self._remove_client(client, reason="disconnect")

    def _remove_client(self, client: _TelemetryClient, *, reason: str) -> None:
        with self.lock:
            existing = self.clients.pop(client.conn, None)
        if existing is None:
            return
        try:
            existing.writer.close()
        except Exception:
            pass
        try:
            existing.conn.close()
        except Exception:
            pass
        print(f"[follower] telemetry client {existing.peer} closed ({reason})", flush=True)
        self._emit_status("disconnected", peer=existing.peer, reason=reason, active_clients=len(self.clients))


class SyntheticKinematicsModel:
    """Deterministic mock flight profile used for telemetry and PFC estimation."""

    def __init__(
        self,
        *,
        weight_n: float,
        horizontal_peak_mps: float,
        vertical_peak_mps: float,
        yaw_rate_dps: float,
        cycle_s: float,
    ) -> None:
        self.weight_n = max(0.0, weight_n)
        self.horizontal_peak_mps = max(0.0, horizontal_peak_mps)
        self.vertical_peak_mps = float(vertical_peak_mps)
        self.yaw_rate_dps = float(yaw_rate_dps)
        self.cycle_s = max(4.0, float(cycle_s))
        self._start_monotonic = time.monotonic()
        self._last_monotonic = self._start_monotonic
        self._altitude_m = 30.0
        self._heading_rad = 0.0
        self._prev_horizontal_mps = 0.0
        self._prev_vertical_mps = 0.0
        self._sequence = 0

    def _phase(self, now: float) -> float:
        elapsed = now - self._start_monotonic
        return (elapsed % self.cycle_s) / self.cycle_s

    def step(self, timestamp_ns: int) -> dict:
        now = time.monotonic()
        dt = max(0.0, now - self._last_monotonic)
        self._last_monotonic = now
        phase = self._phase(now)
        phase_rad = 2.0 * math.pi * phase

        horiz_mps = self.horizontal_peak_mps * math.sin(phase_rad)
        vert_mps = self.vertical_peak_mps * math.sin(phase_rad + math.pi / 3.0)
        speed_mps = math.hypot(horiz_mps, vert_mps)

        yaw_rate_rps = math.radians(self.yaw_rate_dps) * math.cos(phase_rad + math.pi / 6.0)
        self._heading_rad = (self._heading_rad + yaw_rate_rps * dt) % (2.0 * math.pi)
        self._altitude_m = max(0.0, self._altitude_m + vert_mps * dt)

        horiz_accel = 0.0 if dt == 0.0 else (horiz_mps - self._prev_horizontal_mps) / dt
        vert_accel = 0.0 if dt == 0.0 else (vert_mps - self._prev_vertical_mps) / dt
        self._prev_horizontal_mps = horiz_mps
        self._prev_vertical_mps = vert_mps

        pfc_w = calculate_predicted_flight_constraint(abs(horiz_mps), vert_mps, self.weight_n)
        tilt_deg = math.degrees(math.atan2(abs(vert_mps), max(0.1, abs(horiz_mps))))

        self._sequence += 1
        return {
            "timestamp_ns": timestamp_ns,
            "sequence": self._sequence,
            "velocity_horizontal_mps": horiz_mps,
            "velocity_vertical_mps": vert_mps,
            "speed_mps": speed_mps,
            "horizontal_accel_mps2": horiz_accel,
            "vertical_accel_mps2": vert_accel,
            "yaw_rate_dps": math.degrees(yaw_rate_rps),
            "heading_deg": math.degrees(self._heading_rad),
            "altitude_m": self._altitude_m,
            "tilt_deg": tilt_deg,
            "predicted_flight_constraint_w": pfc_w,
        }


def _summary_to_dict(
    summary: PowerSummary,
    *,
    suite: str,
    session_id: str,
    session_dir: Optional[Path] = None,
    monitor_manifest: Optional[Path] = None,
    telemetry_status: Optional[Path] = None,
) -> dict:
    data = {
        "timestamp_ns": summary.end_ns,
        "suite": suite,
        "label": summary.label,
        "session_id": session_id,
        "duration_s": summary.duration_s,
        "samples": summary.samples,
        "avg_current_a": summary.avg_current_a,
        "avg_voltage_v": summary.avg_voltage_v,
        "avg_power_w": summary.avg_power_w,
        "energy_j": summary.energy_j,
        "sample_rate_hz": summary.sample_rate_hz,
        "csv_path": summary.csv_path,
        "start_ns": summary.start_ns,
        "end_ns": summary.end_ns,
    }
    if session_dir is not None:
        data["session_dir"] = str(session_dir)
    if monitor_manifest is not None:
        data["monitor_manifest_path"] = str(monitor_manifest)
    if telemetry_status is not None:
        data["telemetry_status_path"] = str(telemetry_status)
    return data


class PowerCaptureManager:
    """Coordinates power captures for control commands."""

    def __init__(
        self,
        output_dir: Path,
        session_id: str,
        telemetry: Optional[TelemetryPublisher],
    ) -> None:
        self.telemetry = telemetry
        self.session_id = session_id
        self.lock = threading.Lock()
        self._thread: Optional[threading.Thread] = None
        self._last_summary: Optional[dict] = None
        self._last_error: Optional[str] = None
        self._pending_suite: Optional[str] = None
        self.monitor: Optional[PowerMonitor] = None
        self.monitor_backend: Optional[str] = None
        self.session_dir = output_dir.parent
        self._monitor_manifest: Optional[Path] = None
        self._telemetry_status: Optional[Path] = None
        self._artifact_sink: Optional[Callable[[Iterable[Path]], None]] = None

        def _parse_int_env(name: str, default: int) -> int:
            raw = os.getenv(name)
            if not raw:
                return default
            try:
                return int(raw)
            except ValueError:
                print(f"[follower] invalid {name}={raw!r}, using {default}")
                return default

        def _parse_float_env(name: str, default: float) -> float:
            raw = os.getenv(name)
            if not raw:
                return default
            try:
                return float(raw)
            except ValueError:
                print(f"[follower] invalid {name}={raw!r}, using {default}")
                return default

        def _parse_float_optional(name: str) -> Optional[float]:
            raw = os.getenv(name)
            if raw is None or raw == "":
                return None
            try:
                return float(raw)
            except ValueError:
                print(f"[follower] invalid {name}={raw!r}, ignoring")
                return None

        backend = os.getenv("DRONE_POWER_BACKEND", "auto")
        sample_hz = _parse_int_env("DRONE_POWER_SAMPLE_HZ", 1000)
        shunt_ohm = _parse_float_env("DRONE_POWER_SHUNT_OHM", 0.1)
        sign_mode = os.getenv("DRONE_POWER_SIGN_MODE", "auto")
        hwmon_path = os.getenv("DRONE_POWER_HWMON_PATH")
        hwmon_name_hint = os.getenv("DRONE_POWER_HWMON_NAME")
        voltage_file = os.getenv("DRONE_POWER_VOLTAGE_FILE")
        current_file = os.getenv("DRONE_POWER_CURRENT_FILE")
        power_file = os.getenv("DRONE_POWER_POWER_FILE")
        voltage_scale = _parse_float_optional("DRONE_POWER_VOLTAGE_SCALE")
        current_scale = _parse_float_optional("DRONE_POWER_CURRENT_SCALE")
        power_scale = _parse_float_optional("DRONE_POWER_POWER_SCALE")

        try:
            self.monitor = create_power_monitor(
                output_dir,
                backend=backend,
                sample_hz=sample_hz,
                shunt_ohm=shunt_ohm,
                sign_mode=sign_mode,
                hwmon_path=hwmon_path,
                hwmon_name_hint=hwmon_name_hint,
                voltage_file=voltage_file,
                current_file=current_file,
                power_file=power_file,
                voltage_scale=voltage_scale,
                current_scale=current_scale,
                power_scale=power_scale,
            )
            self.available = True
            self.monitor_backend = getattr(self.monitor, "backend_name", self.monitor.__class__.__name__)
            print(f"[follower] power monitor backend: {self.monitor_backend}")
        except PowerMonitorUnavailable as exc:
            self.monitor = None
            self.available = False
            self._last_error = str(exc)
            print(f"[follower] power monitor disabled: {exc}")
        except ValueError as exc:
            self.monitor = None
            self.available = False
            self._last_error = str(exc)
            print(f"[follower] power monitor configuration invalid: {exc}")

    def start_capture(self, suite: str, duration_s: float, start_ns: Optional[int]) -> tuple[bool, Optional[str]]:
        if not self.available or self.monitor is None:
            return False, self._last_error or "power_monitor_unavailable"
        if duration_s <= 0:
            return False, "invalid_duration"
        with self.lock:
            if self._thread and self._thread.is_alive():
                return False, "busy"
            self._last_error = None
            self._pending_suite = suite

            def worker() -> None:
                try:
                    summary = self.monitor.capture(label=suite, duration_s=duration_s, start_ns=start_ns)
                    summary_dict = _summary_to_dict(
                        summary,
                        suite=suite,
                        session_id=self.session_id,
                        session_dir=self.session_dir,
                        monitor_manifest=self._monitor_manifest,
                        telemetry_status=self._telemetry_status,
                    )
                    summary_json_path = Path(summary.csv_path).with_suffix(".json")
                    try:
                        summary_json_path.parent.mkdir(parents=True, exist_ok=True)
                        summary_json_path.write_text(json.dumps(summary_dict, indent=2), encoding="utf-8")
                        summary_dict["summary_json_path"] = str(summary_json_path)
                        self._notify_artifacts([Path(summary.csv_path), summary_json_path])
                    except Exception as exc_json:
                        print(f"[follower] power summary write failed: {exc_json}")
                        self._notify_artifacts([Path(summary.csv_path)])
                    print(
                        f"[follower] power summary suite={suite} avg={summary.avg_power_w:.3f} W "
                        f"energy={summary.energy_j:.3f} J duration={summary.duration_s:.3f}s"
                    )
                    with self.lock:
                        self._last_summary = summary_dict
                        self._pending_suite = None
                    if self.telemetry:
                        self.telemetry.publish("power_summary", dict(summary_dict))
                except Exception as exc:  # pragma: no cover - depends on hardware
                    with self.lock:
                        self._last_error = str(exc)
                        self._pending_suite = None
                    print(f"[follower] power capture failed: {exc}")
                    if self.telemetry:
                        self.telemetry.publish(
                            "power_summary_error",
                            {
                                "timestamp_ns": time.time_ns(),
                                "suite": suite,
                                "error": str(exc),
                            },
                        )
                finally:
                    with self.lock:
                        self._thread = None

            self._thread = threading.Thread(target=worker, daemon=True)
            self._thread.start()
        return True, None

    def status(self) -> dict:
        with self.lock:
            busy = bool(self._thread and self._thread.is_alive())
            summary = dict(self._last_summary) if self._last_summary else None
            error = self._last_error
            pending_suite = self._pending_suite
        return {
            "available": self.available,
            "busy": busy,
            "last_summary": summary,
            "error": error,
            "pending_suite": pending_suite,
            "session_dir": str(self.session_dir) if self.session_dir else "",
            "monitor_manifest_path": str(self._monitor_manifest) if self._monitor_manifest else "",
            "telemetry_status_path": str(self._telemetry_status) if self._telemetry_status else "",
        }

    def register_monitor_manifest(self, manifest_path: Path) -> None:
        self._monitor_manifest = manifest_path

    def register_telemetry_status(self, status_path: Path) -> None:
        self._telemetry_status = status_path

    def register_artifact_sink(self, sink: Callable[[Iterable[Path]], None]) -> None:
        self._artifact_sink = sink

    def _notify_artifacts(self, paths: Iterable[Path]) -> None:
        if not paths:
            return
        sink = self._artifact_sink
        if sink is None:
            return
        try:
            sink(list(paths))
        except Exception:
            pass



def popen(cmd, **kw) -> subprocess.Popen:
    if isinstance(cmd, (list, tuple)):
        display = " ".join(shlex.quote(str(part)) for part in cmd)
    else:
        display = str(cmd)
    print(f"[{ts()}] exec: {display}", flush=True)
    return subprocess.Popen(cmd, **kw)


def killtree(proc: Optional[subprocess.Popen]) -> None:
    if not proc or proc.poll() is not None:
        return
    try:
        proc.terminate()
        proc.wait(timeout=3)
    except Exception:
        try:
            proc.kill()
        except Exception:
            pass


def discover_initial_suite() -> str:
    configured = CONFIG.get("SIMPLE_INITIAL_SUITE")
    if configured:
        return configured

    suite_map = suites_mod.list_suites()
    if suite_map:
        return sorted(suite_map.keys())[0]

    if SECRETS_DIR.exists():
        for path in sorted(SECRETS_DIR.iterdir()):
            if (path / "gcs_signing.pub").exists():
                return path.name

    return "cs-mlkem768-aesgcm-mldsa65"


def suite_outdir(suite: str) -> Path:
    path = OUTDIR / suite
    path.mkdir(parents=True, exist_ok=True)
    return path


def _tail_file_lines(path: Path, limit: int = 120) -> list[str]:
    limit = max(1, min(int(limit), 500))
    try:
        with open(path, encoding="utf-8", errors="replace") as handle:
            lines = list(deque(handle, maxlen=limit))
    except FileNotFoundError:
        return []
    except OSError:
        return []
    return [line.rstrip("\n") for line in lines]


def suite_secrets_dir(suite: str) -> Path:
    return SECRETS_DIR / suite


def write_marker(suite: str) -> None:
    MARK_DIR.mkdir(parents=True, exist_ok=True)
    marker = MARK_DIR / f"{int(time.time())}_{suite}.json"
    with open(marker, "w", encoding="utf-8") as handle:
        json.dump({"ts": ts(), "suite": suite}, handle)


def start_drone_proxy(suite: str) -> tuple[subprocess.Popen, IO[str]]:
    suite_dir = suite_secrets_dir(suite)
    if not suite_dir.exists():
        raise FileNotFoundError(f"Suite directory missing: {suite_dir}")
    pub = suite_dir / "gcs_signing.pub"
    if not pub.exists() or not os.access(pub, os.R_OK):
        print(f"[follower] ERROR: missing {pub}", file=sys.stderr)
        sys.exit(2)

    os.environ["DRONE_HOST"] = DRONE_HOST
    os.environ["GCS_HOST"] = GCS_HOST
    os.environ["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    os.environ["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"

    suite_path = Path("logs/auto/drone") / suite
    status = suite_path / "drone_status.json"
    summary = suite_path / "drone_summary.json"
    status.parent.mkdir(parents=True, exist_ok=True)
    summary.parent.mkdir(parents=True, exist_ok=True)
    OUTDIR.mkdir(parents=True, exist_ok=True)
    log_path = OUTDIR / f"drone_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_path.parent.mkdir(parents=True, exist_ok=True)
    log_handle: IO[str] = open(log_path, "w", encoding="utf-8")

    env = os.environ.copy()
    root_str = str(ROOT)
    existing_py_path = env.get("PYTHONPATH")
    if existing_py_path:
        if root_str not in existing_py_path.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing_py_path
    else:
        env["PYTHONPATH"] = root_str

    print(f"[follower] launching drone proxy on suite {suite}", flush=True)
    proc = popen([
        sys.executable,
        "-m",
        "core.run_proxy",
        "drone",
        "--suite",
        suite,
        "--peer-pubkey-file",
        str(pub),
        "--status-file",
        str(status),
        "--json-out",
        str(summary),
    ], stdout=log_handle, stderr=subprocess.STDOUT, text=True, env=env, cwd=str(ROOT))
    return proc, log_handle


class HighSpeedMonitor(threading.Thread):
    def __init__(
        self,
        output_dir: Path,
        session_id: str,
        publisher: Optional[TelemetryPublisher],
    ):
        super().__init__(daemon=True)
        self.output_dir = output_dir
        self.session_id = session_id
        self.stop_event = threading.Event()
        self.current_suite = "unknown"
        self.pending_suite: Optional[str] = None
        self.proxy_pid: Optional[int] = None
        self.rekey_start_ns: Optional[int] = None
        self.csv_handle: Optional[object] = None
        self.csv_writer: Optional[csv.writer] = None
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.csv_path = self.output_dir / f"system_monitoring_{session_id}.csv"
        self.publisher = publisher
        self._vcgencmd_available = True
        self.rekey_marks_path = self.output_dir / f"rekey_marks_{session_id}.csv"
        self._rekey_marks_lock = threading.Lock()
        self._summary_lock = threading.Lock()
        self._max_pfc_w = 0.0
        self._last_pfc_w = 0.0
        self._last_kin_sample_ns = 0
        auto_cfg = AUTO_DRONE_CONFIG
        mass_kg = auto_cfg.get("mock_mass_kg", 6.5)
        horiz_mps = auto_cfg.get("kinematics_horizontal_mps", 13.0)
        vert_mps = auto_cfg.get("kinematics_vertical_mps", 3.5)
        yaw_rate_dps = auto_cfg.get("kinematics_yaw_rate_dps", 45.0)
        cycle_s = auto_cfg.get("kinematics_cycle_s", 18.0)
        try:
            weight_n = max(0.0, float(mass_kg) * GRAVITY)
        except (TypeError, ValueError):
            weight_n = 0.0
        try:
            horiz_peak = float(horiz_mps)
        except (TypeError, ValueError):
            horiz_peak = 0.0
        try:
            vert_peak = float(vert_mps)
        except (TypeError, ValueError):
            vert_peak = 0.0
        try:
            yaw_peak = float(yaw_rate_dps)
        except (TypeError, ValueError):
            yaw_peak = 0.0
        try:
            cycle = float(cycle_s)
        except (TypeError, ValueError):
            cycle = 18.0
        self._kinematics_model = SyntheticKinematicsModel(
            weight_n=weight_n,
            horizontal_peak_mps=max(0.0, horiz_peak),
            vertical_peak_mps=vert_peak,
            yaw_rate_dps=yaw_peak,
            cycle_s=cycle,
        ) if weight_n > 0.0 else None

    def attach_proxy(self, pid: int) -> None:
        self.proxy_pid = pid

    def start_rekey(self, old_suite: str, new_suite: str) -> None:
        self.pending_suite = new_suite
        self.rekey_start_ns = time.time_ns()
        print(f"[monitor] rekey transition {old_suite} -> {new_suite}")
        if self.publisher:
            self.publisher.publish(
                "rekey_transition_start",
                {
                    "timestamp_ns": self.rekey_start_ns,
                    "old_suite": old_suite,
                    "new_suite": new_suite,
                    "pending_suite": new_suite,
                },
            )
        self._append_rekey_mark([
            "start",
            str(self.rekey_start_ns),
            old_suite or "",
            new_suite or "",
            self.pending_suite or "",
        ])

    def end_rekey(self, *, success: bool, new_suite: Optional[str]) -> None:
        if self.rekey_start_ns is None:
            self.pending_suite = None
            return
        duration_ms = (time.time_ns() - self.rekey_start_ns) / 1_000_000
        target_suite = new_suite or self.pending_suite or self.current_suite
        if success and new_suite:
            self.current_suite = new_suite
        status_text = "completed" if success else "failed"
        print(f"[monitor] rekey {status_text} in {duration_ms:.2f} ms (target={target_suite})")
        if self.publisher:
            payload = {
                "timestamp_ns": time.time_ns(),
                "suite": self.current_suite,
                "duration_ms": duration_ms,
                "success": success,
            }
            if target_suite:
                payload["requested_suite"] = target_suite
            if self.pending_suite:
                payload["pending_suite"] = self.pending_suite
            self.publisher.publish("rekey_transition_end", payload)
        end_timestamp = time.time_ns()
        self._append_rekey_mark([
            "end",
            str(end_timestamp),
            "ok" if success else "fail",
            target_suite or "",
            f"{duration_ms:.3f}",
        ])
        self.rekey_start_ns = None
        self.pending_suite = None

    def _append_rekey_mark(self, row: list[str]) -> None:
        try:
            self.rekey_marks_path.parent.mkdir(parents=True, exist_ok=True)
            with self._rekey_marks_lock:
                new_file = not self.rekey_marks_path.exists()
                with self.rekey_marks_path.open("a", newline="", encoding="utf-8") as handle:
                    writer = csv.writer(handle)
                    if new_file:
                        writer.writerow(["kind", "timestamp_ns", "field1", "field2", "field3"])
                    writer.writerow(row)
        except Exception as exc:
            print(f"[monitor] rekey mark append failed: {exc}")

    def run(self) -> None:
        self.csv_handle = open(self.csv_path, "w", newline="", encoding="utf-8")
        self.csv_writer = csv.writer(self.csv_handle)
        self.csv_writer.writerow(
            [
                "timestamp_iso",
                "timestamp_ns",
                "suite",
                "proxy_pid",
                "cpu_percent",
                "cpu_freq_mhz",
                "cpu_temp_c",
                "mem_used_mb",
                "mem_percent",
                "rekey_duration_ms",
            ]
        )
        interval = LOG_INTERVAL_MS / 1000.0
        while not self.stop_event.is_set():
            start = time.time()
            self._sample()
            elapsed = time.time() - start
            sleep_for = max(0.0, interval - elapsed)
            if sleep_for:
                time.sleep(sleep_for)

    def _sample(self) -> None:
        timestamp_ns = time.time_ns()
        timestamp_iso = datetime.fromtimestamp(
            timestamp_ns / 1e9,
            tz=timezone.utc,
        ).strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
        cpu_percent = psutil.cpu_percent(interval=None)
        try:
            with open("/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq", "r", encoding="utf-8") as handle:
                cpu_freq_mhz = int(handle.read().strip()) / 1000.0
        except Exception:
            cpu_freq_mhz = 0.0
        cpu_temp_c = 0.0
        try:
            if self._vcgencmd_available:
                result = subprocess.run(["vcgencmd", "measure_temp"], capture_output=True, text=True)
                if result.returncode == 0 and "=" in result.stdout:
                    cpu_temp_c = float(result.stdout.split("=")[1].split("'")[0])
                else:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()
        except Exception:
            if self._vcgencmd_available:
                self._vcgencmd_available = False
                _warn_vcgencmd_unavailable()
        mem = psutil.virtual_memory()
        rekey_ms = ""
        if self.rekey_start_ns is not None:
            rekey_ms = f"{(timestamp_ns - self.rekey_start_ns) / 1_000_000:.2f}"
        if self.csv_writer is None:
            return
        self.csv_writer.writerow(
            [
                timestamp_iso,
                str(timestamp_ns),
                self.current_suite,
                self.proxy_pid or "",
                f"{cpu_percent:.1f}",
                f"{cpu_freq_mhz:.1f}",
                f"{cpu_temp_c:.1f}",
                f"{mem.used / (1024 * 1024):.1f}",
                f"{mem.percent:.1f}",
                rekey_ms,
            ]
        )
        self.csv_handle.flush()
        kin_payload: Optional[dict] = None
        if self._kinematics_model is not None:
            kin = self._kinematics_model.step(timestamp_ns)
            kin_payload = dict(kin)
            kin_payload.setdefault("suite", self.current_suite)
            kin_payload.setdefault("weight_n", self._kinematics_model.weight_n)
            kin_payload.setdefault("mass_kg", self._kinematics_model.weight_n / GRAVITY if GRAVITY else 0.0)
            pfc_value = kin_payload.get("predicted_flight_constraint_w")
            if isinstance(pfc_value, (int, float)):
                with self._summary_lock:
                    self._last_pfc_w = float(pfc_value)
                    self._last_kin_sample_ns = timestamp_ns
                    if pfc_value > self._max_pfc_w:
                        self._max_pfc_w = float(pfc_value)

        if self.publisher:
            sample = {
                "timestamp_ns": timestamp_ns,
                "timestamp_iso": timestamp_iso,
                "suite": self.current_suite,
                "proxy_pid": self.proxy_pid,
                "cpu_percent": cpu_percent,
                "cpu_freq_mhz": cpu_freq_mhz,
                "cpu_temp_c": cpu_temp_c,
                "mem_used_mb": mem.used / (1024 * 1024),
                "mem_percent": mem.percent,
            }
            if self.rekey_start_ns is not None:
                sample["rekey_elapsed_ms"] = (timestamp_ns - self.rekey_start_ns) / 1_000_000
            self.publisher.publish("system_sample", sample)
            if kin_payload is not None:
                self.publisher.publish("kinematics", kin_payload)

    def kinematics_summary(self) -> dict:
        with self._summary_lock:
            return {
                "last_sample_ns": self._last_kin_sample_ns,
                "last_predicted_flight_constraint_w": self._last_pfc_w,
                "peak_predicted_flight_constraint_w": self._max_pfc_w,
            }

    def stop(self) -> None:
        self.stop_event.set()
        if self.is_alive():
            self.join(timeout=2.0)
        if self.csv_handle:
            self.csv_handle.close()


class UdpEcho(threading.Thread):
    def __init__(
        self,
        bind_host: str,
        recv_port: int,
        send_host: str,
        send_port: int,
        stop_event: threading.Event,
        monitor: Optional[HighSpeedMonitor],
        session_dir: Path,
        publisher: Optional[TelemetryPublisher],
    ):
        super().__init__(daemon=True)
        self.bind_host = bind_host
        self.recv_port = recv_port
        self.send_host = send_host
        self.send_port = send_port
        self.stop_event = stop_event
        self.monitor = monitor
        self.session_dir = session_dir
        self.publisher = publisher
        def _bind_socket(host: str, port: int) -> socket.socket:
            flags = socket.AI_PASSIVE if not host else 0
            try:
                addrinfo = socket.getaddrinfo(host, port, 0, socket.SOCK_DGRAM, 0, flags)
            except socket.gaierror as exc:
                raise OSError(f"UDP echo bind failed for {host}:{port}: {exc}") from exc

            last_exc: Optional[Exception] = None
            for family, socktype, proto, _canon, sockaddr in addrinfo:
                sock: Optional[socket.socket] = None
                try:
                    sock = socket.socket(family, socktype, proto)
                    try:
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    except OSError:
                        pass
                    if family == socket.AF_INET6:
                        try:
                            sock.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
                        except OSError:
                            pass
                    sock.bind(sockaddr)
                    return sock
                except Exception as exc:
                    last_exc = exc
                    if sock is not None:
                        try:
                            sock.close()
                        except Exception:
                            pass
                    continue

            message = last_exc or RuntimeError("no suitable address family")
            raise OSError(f"UDP echo bind failed for {host}:{port}: {message}")

        def _connect_tuple(host: str, port: int, preferred_family: int) -> tuple[socket.socket, tuple]:
            addrinfo: list[tuple] = []
            try:
                addrinfo = socket.getaddrinfo(host, port, preferred_family, socket.SOCK_DGRAM)
            except socket.gaierror:
                pass
            if not addrinfo:
                try:
                    addrinfo = socket.getaddrinfo(host, port, 0, socket.SOCK_DGRAM)
                except socket.gaierror as exc:
                    raise OSError(f"UDP echo resolve failed for {host}:{port}: {exc}") from exc

            last_exc: Optional[Exception] = None
            for family, socktype, proto, _canon, sockaddr in addrinfo:
                sock: Optional[socket.socket] = None
                try:
                    sock = socket.socket(family, socktype, proto)
                    try:
                        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    except OSError:
                        pass
                    return sock, sockaddr
                except Exception as exc:
                    last_exc = exc
                    if sock is not None:
                        try:
                            sock.close()
                        except Exception:
                            pass
                    continue

            message = last_exc or RuntimeError("no suitable address family")
            raise OSError(f"UDP echo socket creation failed for {host}:{port}: {message}")

        self.rx_sock = _bind_socket(self.bind_host, self.recv_port)
        self.tx_sock, self.send_addr = _connect_tuple(self.send_host, self.send_port, self.rx_sock.family)
        try:
            sndbuf = int(os.getenv("DRONE_SOCK_SNDBUF", str(16 << 20)))
            rcvbuf = int(os.getenv("DRONE_SOCK_RCVBUF", str(16 << 20)))
            self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, rcvbuf)
            self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, sndbuf)
            actual_snd = self.tx_sock.getsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF)
            actual_rcv = self.rx_sock.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF)
            print(
                f"[{ts()}] follower UDP socket buffers: snd={actual_snd} rcv={actual_rcv}",
                flush=True,
            )
        except Exception:
            pass
        self.packet_log_path = self.session_dir / "packet_timing.csv"
        self.packet_log_handle: Optional[object] = None
        self.packet_writer: Optional[csv.writer] = None
        self.samples = 0
        self.log_every_packet = False

    def run(self) -> None:
        print(
            f"[follower] UDP echo up: recv:{self.bind_host}:{self.recv_port} -> send:{self.send_host}:{self.send_port}",
            flush=True,
        )
        self.packet_log_handle = open(self.packet_log_path, "w", newline="", encoding="utf-8")
        self.packet_writer = csv.writer(self.packet_log_handle)
        self.packet_writer.writerow([
            "recv_timestamp_ns",
            "send_timestamp_ns",
            "processing_ns",
            "processing_ms",
            "sequence",
            "payload_len",
        ])
        self.rx_sock.settimeout(0.001)
        while not self.stop_event.is_set():
            try:
                data, _ = self.rx_sock.recvfrom(65535)
                recv_ns = time.time_ns()
                enhanced = self._annotate_packet(data, recv_ns)
                send_ns = time.time_ns()
                self.tx_sock.sendto(enhanced, self.send_addr)
                self._record_packet(data, recv_ns, send_ns)
            except socket.timeout:
                continue
            except Exception as exc:
                print(f"[follower] UDP echo error: {exc}", flush=True)
        self.rx_sock.close()
        self.tx_sock.close()
        if self.packet_log_handle:
            self.packet_log_handle.close()

    def _annotate_packet(self, data: bytes, recv_ns: int) -> bytes:
        # Last 8 bytes carry drone receive timestamp for upstream OWD inference.
        if len(data) >= 20:
            return data[:-8] + recv_ns.to_bytes(8, "big")
        return data + recv_ns.to_bytes(8, "big")

    def _record_packet(self, data: bytes, recv_ns: int, send_ns: int) -> None:
        if self.packet_writer is None or len(data) < 4:
            return
        try:
            seq, = struct.unpack("!I", data[:4])
        except struct.error:
            return
        processing_ns = send_ns - recv_ns
        monitor_active = bool(self.monitor and self.monitor.rekey_start_ns is not None)
        if monitor_active and not self.log_every_packet:
            self.log_every_packet = True
        elif not monitor_active and self.log_every_packet:
            self.log_every_packet = False

        should_log = self.log_every_packet or (seq % 100 == 0)
        if should_log:
            self.packet_writer.writerow([
                recv_ns,
                send_ns,
                processing_ns,
                f"{processing_ns / 1_000_000:.6f}",
                seq,
                len(data),
            ])
            # Always flush to prevent data loss on crashes
            if self.packet_log_handle:
                self.packet_log_handle.flush()
            if self.publisher:
                suite = self.monitor.current_suite if self.monitor else "unknown"
                self.publisher.publish(
                    "udp_echo_sample",
                    {
                        "recv_timestamp_ns": recv_ns,
                        "send_timestamp_ns": send_ns,
                        "processing_ns": processing_ns,
                        "sequence": seq,
                        "suite": suite,
                    },
                )



class Monitors:
    """Structured performance/telemetry collectors for the drone proxy."""

    PERF_FIELDS = [
        "ts_unix_ns",
        "t_offset_ms",
        "instructions",
        "cycles",
        "cache-misses",
        "branch-misses",
        "task-clock",
        "context-switches",
        "branches",
    ]

    def __init__(self, enabled: bool, telemetry: Optional[TelemetryPublisher], session_dir: Path):
        self.enabled = enabled
        self.telemetry = telemetry
        self.perf: Optional[subprocess.Popen] = None
        self.pidstat: Optional[subprocess.Popen] = None
        self.perf_thread: Optional[threading.Thread] = None
        self.perf_stop = threading.Event()
        self.perf_csv_handle: Optional[object] = None
        self.perf_writer: Optional[csv.DictWriter] = None
        self.perf_start_ns = 0
        self.current_suite = "unknown"

        self.psutil_thread: Optional[threading.Thread] = None
        self.psutil_stop = threading.Event()
        self.psutil_csv_handle: Optional[object] = None
        self.psutil_writer: Optional[csv.DictWriter] = None
        self.psutil_proc: Optional[psutil.Process] = None
        self._stats_lock = threading.Lock()
        self._max_cpu_percent = 0.0
        self._max_rss_bytes = 0
        self._last_cpu_percent = 0.0
        self._last_rss_bytes = 0
        self._last_num_threads = 0
        self._last_sample_ns = 0

        self.temp_thread: Optional[threading.Thread] = None
        self.temp_stop = threading.Event()
        self.temp_csv_handle: Optional[object] = None
        self.temp_writer: Optional[csv.DictWriter] = None
        self.pidstat_out: Optional[IO[str]] = None
        self._vcgencmd_available = True

        self.session_dir = session_dir
        self.manifest_path = session_dir / "monitor_manifest.json"
        self._artifact_lock = threading.Lock()
        self._artifact_paths: set[str] = set()
        self._write_manifest()

    def start(self, pid: int, outdir: Path, suite: str, *, session_dir: Optional[Path] = None) -> None:
        if not self.enabled:
            return
        outdir.mkdir(parents=True, exist_ok=True)
        self.current_suite = suite
        self._vcgencmd_available = True
        if session_dir is not None:
            self.session_dir = session_dir
            self.manifest_path = self.session_dir / "monitor_manifest.json"
            self._write_manifest()

        # Structured perf samples
        perf_path = outdir / f"perf_samples_{suite}.csv"
        self.perf_csv_handle = open(perf_path, "w", newline="", encoding="utf-8")
        self.perf_writer = csv.DictWriter(self.perf_csv_handle, fieldnames=self.PERF_FIELDS)
        self.perf_writer.writeheader()
        self.perf_start_ns = time.time_ns()

        if platform.system() == 'Linux':
            perf_cmd = [
                "perf",
                "stat",
                "-I",
                "1000",
                "-x",
                ",",
                "-e",
                PERF_EVENTS,
                "-p",
                str(pid),
                "--log-fd",
                "1",
            ]
            self.perf = popen(
                perf_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
            )
            self.perf_stop.clear()
            self.perf_thread = threading.Thread(
                target=self._consume_perf,
                args=(self.perf.stdout,),
                daemon=True,
            )
            self.perf_thread.start()

            # pidstat baseline dump for parity with legacy tooling
            self.pidstat_out = open(outdir / f"pidstat_{suite}.txt", "w", encoding="utf-8")
            self.pidstat = popen(
                ["pidstat", "-hlur", "-p", str(pid), "1"],
                stdout=self.pidstat_out,
                stderr=subprocess.STDOUT,
            )
        else:
            print("[monitor] skipping perf and pidstat on non-Linux platform", flush=True)

        # psutil metrics (CPU%, RSS, threads)
        self.psutil_proc = psutil.Process(pid)
        self.psutil_proc.cpu_percent(interval=None)
        psutil_path = outdir / f"psutil_proc_{suite}.csv"
        self.psutil_csv_handle = open(psutil_path, "w", newline="", encoding="utf-8")
        self.psutil_writer = csv.DictWriter(
            self.psutil_csv_handle,
            fieldnames=["ts_unix_ns", "cpu_percent", "rss_bytes", "num_threads"],
        )
        self.psutil_writer.writeheader()
        self.psutil_stop.clear()
        self.psutil_thread = threading.Thread(target=self._psutil_loop, daemon=True)
        self.psutil_thread.start()

        # Temperature / frequency / throttled flags
        temp_path = outdir / f"sys_telemetry_{suite}.csv"
        self.temp_csv_handle = open(temp_path, "w", newline="", encoding="utf-8")
        self.temp_writer = csv.DictWriter(
            self.temp_csv_handle,
            fieldnames=["ts_unix_ns", "temp_c", "freq_hz", "throttled_hex"],
        )
        self.temp_writer.writeheader()
        self.temp_stop.clear()
        self.temp_thread = threading.Thread(target=self._telemetry_loop, daemon=True)
        self.temp_thread.start()

        if self.telemetry:
            self.telemetry.publish(
                "monitors_started",
                {
                    "timestamp_ns": time.time_ns(),
                    "suite": suite,
                    "proxy_pid": pid,
                },
            )
        artifacts = [psutil_path, temp_path]
        if platform.system() == 'Linux':
            artifacts.insert(0, perf_path)
            if self.pidstat_out:
                artifacts.insert(1, self.pidstat_out.name)
        self._record_artifacts(*artifacts)

    def _consume_perf(self, stream) -> None:
        if not self.perf_writer:
            return
        current_ms = None
        row = None
        try:
            for line in iter(stream.readline, ""):
                if self.perf_stop.is_set():
                    break
                parts = [part.strip() for part in line.strip().split(",")]
                if len(parts) < 4:
                    continue
                try:
                    offset_ms = float(parts[0])
                except ValueError:
                    continue
                event = parts[3]
                if event.startswith("#"):
                    continue
                raw_value = parts[1].replace(",", "")
                if event == "task-clock":
                    try:
                        value = float(raw_value)
                    except Exception:
                        value = ""
                else:
                    try:
                        value = int(raw_value)
                    except Exception:
                        value = ""

                if current_ms is None or abs(offset_ms - current_ms) >= 0.5:
                    if row:
                        self.perf_writer.writerow(row)
                        self.perf_csv_handle.flush()
                    current_ms = offset_ms
                    row = {field: "" for field in self.PERF_FIELDS}
                    row["t_offset_ms"] = f"{offset_ms:.0f}"
                    row["ts_unix_ns"] = str(self.perf_start_ns + int(offset_ms * 1_000_000))

                key_map = {
                    "instructions": "instructions",
                    "cycles": "cycles",
                    "cache-misses": "cache-misses",
                    "branch-misses": "branch-misses",
                    "task-clock": "task-clock",
                    "context-switches": "context-switches",
                    "branches": "branches",
                }
                column = key_map.get(event)
                if row is not None and column:
                    row[column] = value

            if row:
                self.perf_writer.writerow(row)
                self.perf_csv_handle.flush()
                if self.telemetry:
                    sample = {k: row.get(k, "") for k in self.PERF_FIELDS}
                    sample["suite"] = self.current_suite
                    self.telemetry.publish("perf_sample", sample)
        finally:
            try:
                stream.close()
            except Exception:
                pass

    def _psutil_loop(self) -> None:
        while not self.psutil_stop.is_set():
            try:
                assert self.psutil_writer is not None
                ts_now = time.time_ns()
                cpu_percent = self.psutil_proc.cpu_percent(interval=None)  # type: ignore[arg-type]
                rss_bytes = self.psutil_proc.memory_info().rss  # type: ignore[union-attr]
                num_threads = self.psutil_proc.num_threads()  # type: ignore[union-attr]
                self.psutil_writer.writerow({
                    "ts_unix_ns": ts_now,
                    "cpu_percent": cpu_percent,
                    "rss_bytes": rss_bytes,
                    "num_threads": num_threads,
                })
                self.psutil_csv_handle.flush()
                with self._stats_lock:
                    self._last_sample_ns = ts_now
                    self._last_cpu_percent = cpu_percent
                    self._last_rss_bytes = rss_bytes
                    self._last_num_threads = num_threads
                    if cpu_percent > self._max_cpu_percent:
                        self._max_cpu_percent = cpu_percent
                    if rss_bytes > self._max_rss_bytes:
                        self._max_rss_bytes = rss_bytes
                if self.telemetry:
                    self.telemetry.publish(
                        "psutil_sample",
                        {
                            "timestamp_ns": ts_now,
                            "suite": self.current_suite,
                            "cpu_percent": cpu_percent,
                            "rss_bytes": rss_bytes,
                            "num_threads": num_threads,
                        },
                    )
            except Exception:
                pass
            time.sleep(1.0)
            try:
                self.psutil_proc.cpu_percent(interval=None)  # type: ignore[arg-type]
            except Exception:
                pass

    def resource_summary(self) -> dict:
        with self._stats_lock:
            rss_mb = self._last_rss_bytes / (1024 * 1024)
            peak_rss_mb = self._max_rss_bytes / (1024 * 1024)
            return {
                "last_sample_ns": self._last_sample_ns,
                "last_cpu_percent": self._last_cpu_percent,
                "last_rss_bytes": self._last_rss_bytes,
                "last_rss_mb": rss_mb,
                "last_num_threads": self._last_num_threads,
                "peak_cpu_percent": self._max_cpu_percent,
                "peak_rss_bytes": self._max_rss_bytes,
                "peak_rss_mb": peak_rss_mb,
            }

    def _telemetry_loop(self) -> None:
        while not self.temp_stop.is_set():
            payload = {
                "ts_unix_ns": time.time_ns(),
                "temp_c": None,
                "freq_hz": None,
                "throttled_hex": "",
            }
            if self._vcgencmd_available:
                try:
                    out = subprocess.check_output(["vcgencmd", "measure_temp"]).decode(errors="ignore")
                    payload["temp_c"] = float(out.split("=")[1].split("'")[0])
                except Exception:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()

            freq_path = Path("/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq")
            if freq_path.exists():
                try:
                    payload["freq_hz"] = int(freq_path.read_text().strip()) * 1000
                except Exception:
                    pass
            elif self._vcgencmd_available:
                try:
                    out = subprocess.check_output(["vcgencmd", "measure_clock", "arm"]).decode(errors="ignore")
                    payload["freq_hz"] = int(out.split("=")[1].strip())
                except Exception:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()

            if self._vcgencmd_available:
                try:
                    out = subprocess.check_output(["vcgencmd", "get_throttled"]).decode(errors="ignore")
                    payload["throttled_hex"] = out.strip().split("=")[1]
                except Exception:
                    self._vcgencmd_available = False
                    _warn_vcgencmd_unavailable()
            try:
                assert self.temp_writer is not None
                self.temp_writer.writerow(payload)
                self.temp_csv_handle.flush()
                if self.telemetry:
                    payload = dict(payload)
                    payload["suite"] = self.current_suite
                    self.telemetry.publish("thermal_sample", payload)
            except Exception:
                pass
            time.sleep(1.0)

    def rotate(self, pid: int, outdir: Path, suite: str) -> None:
        if not self.enabled:
            write_marker(suite)
            return
        self.stop()
        self.start(pid, outdir, suite, session_dir=self.session_dir)
        self._record_artifacts(outdir / f"perf_samples_{suite}.csv", outdir / f"psutil_proc_{suite}.csv", outdir / f"sys_telemetry_{suite}.csv")
        write_marker(suite)

    def stop(self) -> None:
        if not self.enabled:
            return

        self.perf_stop.set()
        if self.perf_thread:
            self.perf_thread.join(timeout=1.0)
        if self.perf:
            killtree(self.perf)
            self.perf = None
        if self.perf_csv_handle:
            try:
                self.perf_csv_handle.close()
            except Exception:
                pass
            self.perf_csv_handle = None

        killtree(self.pidstat)
        self.pidstat = None
        if self.pidstat_out:
            try:
                self.pidstat_out.close()
            except Exception:
                pass
            self.pidstat_out = None

        self.psutil_stop.set()
        if self.psutil_thread:
            self.psutil_thread.join(timeout=1.0)
            self.psutil_thread = None
        if self.psutil_csv_handle:
            try:
                self.psutil_csv_handle.close()
            except Exception:
                pass
            self.psutil_csv_handle = None

        self.temp_stop.set()
        if self.temp_thread:
            self.temp_thread.join(timeout=1.0)
            self.temp_thread = None
        if self.temp_csv_handle:
            try:
                self.temp_csv_handle.close()
            except Exception:
                pass
            self.temp_csv_handle = None

        if self.telemetry:
            self.telemetry.publish(
                "monitors_stopped",
                {
                    "timestamp_ns": time.time_ns(),
                    "suite": self.current_suite,
                },
            )
        self._write_manifest()

    def register_artifacts(self, *paths: Path) -> None:
        self._record_artifacts(*paths)

    def _write_manifest(self) -> None:
        try:
            self.session_dir.mkdir(parents=True, exist_ok=True)
            payload = {
                "session_dir": str(self.session_dir),
                "artifacts": sorted(self._artifact_paths),
            }
            self.manifest_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        except Exception:
            pass

    def _record_artifacts(self, *paths: Path) -> None:
        updated = False
        with self._artifact_lock:
            for candidate in paths:
                if candidate is None:
                    continue
                try:
                    path_obj = Path(candidate)
                except TypeError:
                    continue
                path_str = str(path_obj)
                if not path_str:
                    continue
                if path_str not in self._artifact_paths:
                    self._artifact_paths.add(path_str)
                    updated = True
        if updated:
            self._write_manifest()


class ControlServer(threading.Thread):
    """Line-delimited JSON control server for the scheduler."""

    def __init__(self, host: str, port: int, state: dict):
        super().__init__(daemon=True)
        self.host = host
        self.port = port
        self.state = state
        try:
            addrinfo = socket.getaddrinfo(
                self.host,
                self.port,
                0,
                socket.SOCK_STREAM,
                proto=0,
                flags=socket.AI_PASSIVE if not self.host else 0,
            )
        except socket.gaierror as exc:
            raise OSError(f"control server bind failed for {self.host}:{self.port}: {exc}") from exc

        last_exc: Optional[Exception] = None
        bound_sock: Optional[socket.socket] = None
        for family, socktype, proto, _canon, sockaddr in addrinfo:
            try:
                candidate = socket.socket(family, socktype, proto)
                try:
                    candidate.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                    if family == socket.AF_INET6:
                        try:
                            candidate.setsockopt(socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0)
                        except OSError:
                            pass
                    candidate.bind(sockaddr)
                    candidate.listen(5)
                except Exception:
                    candidate.close()
                    raise
            except Exception as exc:
                last_exc = exc
                continue
            bound_sock = candidate
            break

        if bound_sock is None:
            message = last_exc or RuntimeError("no suitable address family")
            raise OSError(f"control server bind failed for {self.host}:{self.port}: {message}")

        self.sock = bound_sock

    def run(self) -> None:
        print(f"[follower] control listening on {self.host}:{self.port}", flush=True)
        while not self.state["stop_event"].is_set():
            try:
                self.sock.settimeout(0.5)
                conn, _addr = self.sock.accept()
            except socket.timeout:
                continue
            threading.Thread(target=self.handle, args=(conn,), daemon=True).start()
        self.sock.close()

    def handle(self, conn: socket.socket) -> None:
        try:
            line = conn.makefile().readline()
            request = json.loads(line.strip()) if line else {}
        except Exception:
            request = {}

        try:
            cmd = request.get("cmd")
            if cmd == "ping":
                self._send(conn, {"ok": True, "ts": ts()})
                return
            if cmd == "timesync":
                t1 = int(request.get("t1_ns", 0))
                t2 = time.time_ns()
                response = {"ok": True, "t1_ns": t1, "t2_ns": t2}
                t3 = time.time_ns()
                response["t3_ns"] = t3
                self._send(conn, response)
                return
            state_lock = self.state.get("lock")
            if state_lock is None:
                state_lock = threading.Lock()
                self.state["lock"] = state_lock
            if cmd == "capabilities":
                with state_lock:
                    snapshot = dict(self.state.get("capabilities") or {})
                    telemetry: Optional[TelemetryPublisher] = self.state.get("telemetry")
                self._send(conn, {"ok": True, "capabilities": snapshot})
                if telemetry and snapshot:
                    try:
                        telemetry.publish(
                            "capabilities_response",
                            {
                                "timestamp_ns": time.time_ns(),
                                "capabilities": snapshot,
                            },
                        )
                    except Exception:
                        pass
                return
            if cmd == "validate_suite":
                suite_raw = request.get("suite")
                suite = str(suite_raw or "").strip()
                if not suite:
                    self._send(conn, {"ok": False, "error": "missing_suite"})
                    return
                stage = str(request.get("stage") or "").strip() or "unspecified"
                with state_lock:
                    snapshot = dict(self.state.get("capabilities") or {})
                    telemetry: Optional[TelemetryPublisher] = self.state.get("telemetry")
                supported_set = set()
                supported_list = snapshot.get("supported_suites")
                if isinstance(supported_list, (list, tuple, set)):
                    supported_set = {str(item) for item in supported_list if isinstance(item, str)}
                unsupported_map: Dict[str, dict] = {}
                raw_unsupported = snapshot.get("unsupported_suites")
                if isinstance(raw_unsupported, list):
                    for entry in raw_unsupported:
                        if isinstance(entry, dict):
                            suite_name = entry.get("suite")
                            if isinstance(suite_name, str):
                                unsupported_map[suite_name] = entry
                response: Dict[str, object] = {
                    "ok": True,
                    "suite": suite,
                    "stage": stage,
                    "supported": True,
                }
                detail_entry = unsupported_map.get(suite)
                if supported_set and suite not in supported_set:
                    response["ok"] = False
                    response["error"] = "suite_unsupported"
                    response["supported"] = False
                    if detail_entry:
                        response["details"] = detail_entry
                elif detail_entry and not supported_set:
                    # When capabilities probing failed, fall back to advertised unsupported map.
                    response["ok"] = False
                    response["error"] = "suite_unsupported"
                    response["supported"] = False
                    response["details"] = detail_entry
                elif snapshot.get("timestamp_ns"):
                    response["capabilities_timestamp_ns"] = snapshot["timestamp_ns"]
                self._send(conn, response)
                if telemetry:
                    publish_payload = {
                        "timestamp_ns": time.time_ns(),
                        "event": "validate_suite",
                        "suite": suite,
                        "stage": stage,
                        "result": "ok" if response.get("ok") else "rejected",
                    }
                    if not response.get("ok") and detail_entry:
                        publish_payload["details"] = detail_entry
                    try:
                        telemetry.publish("validate_suite", publish_payload)
                    except Exception:
                        pass
                return
            if cmd == "status":
                with state_lock:
                    proxy = self.state["proxy"]
                    suite = self.state["suite"]
                    suite_epoch = int(self.state.get("suite_epoch") or 0)
                    pending_suite_epoch = self.state.get("pending_suite_epoch")
                    monitors_obj: Monitors = self.state["monitors"]
                    high_speed_monitor: HighSpeedMonitor = self.state.get("high_speed_monitor")
                    manager: Optional[PowerCaptureManager] = self.state.get("power_manager")
                    monitors_enabled = monitors_obj.enabled
                    running = bool(proxy and proxy.poll() is None)
                    proxy_pid = proxy.pid if proxy else None
                    telemetry: Optional[TelemetryPublisher] = self.state.get("telemetry")
                    pending_suite = self.state.get("pending_suite")
                    last_requested = self.state.get("last_requested_suite")
                    session_id = self.state.get("session_id")
                    session_dir = self.state.get("session_dir")
                    telemetry_status_path = self.state.get("telemetry_status_path")
                    monitor_manifest_path = getattr(monitors_obj, "manifest_path", None)
                    resource_summary = monitors_obj.resource_summary() if monitors_obj else {}
                    kinematics_summary = high_speed_monitor.kinematics_summary() if high_speed_monitor else {}
                    power_status = manager.status() if isinstance(manager, PowerCaptureManager) else {}
                    log_path = self.state.get("log_path")
                    status_payload = {
                        "suite": suite,
                        "active_suite": suite,
                        "suite_epoch": suite_epoch,
                        "pending_suite_epoch": pending_suite_epoch,
                        "pending_suite": pending_suite,
                        "last_requested_suite": last_requested,
                        "proxy_pid": proxy_pid,
                        "running": running,
                        "control_host": self.host,
                        "control_port": self.port,
                        "udp_recv_port": APP_RECV_PORT,
                        "udp_send_port": APP_SEND_PORT,
                        "session_id": session_id,
                        "session_dir": str(session_dir) if session_dir else "",
                        "monitors_enabled": monitors_enabled,
                        "monitor_manifest_path": str(monitor_manifest_path) if monitor_manifest_path else "",
                        "telemetry_status_path": str(telemetry_status_path) if telemetry_status_path else "",
                        "log_path": str(log_path) if log_path else "",
                    }
                    if resource_summary:
                        status_payload.update(
                            {
                                "resource_last_sample_ns": resource_summary.get("last_sample_ns", 0),
                                "resource_last_cpu_percent": resource_summary.get("last_cpu_percent", 0.0),
                                "resource_last_rss_mb": resource_summary.get("last_rss_mb", 0.0),
                                "resource_last_num_threads": resource_summary.get("last_num_threads", 0),
                                "resource_peak_cpu_percent": resource_summary.get("peak_cpu_percent", 0.0),
                                "resource_peak_rss_mb": resource_summary.get("peak_rss_mb", 0.0),
                            }
                        )
                    if kinematics_summary:
                        status_payload.update(
                            {
                                "pfc_last_sample_ns": kinematics_summary.get("last_sample_ns", 0),
                                "pfc_last_w": kinematics_summary.get("last_predicted_flight_constraint_w", 0.0),
                                "pfc_peak_w": kinematics_summary.get("peak_predicted_flight_constraint_w", 0.0),
                            }
                        )
                    if power_status:
                        status_payload.update(
                            {
                                "power_available": bool(power_status.get("available", False)),
                                "power_busy": bool(power_status.get("busy", False)),
                                "power_error": power_status.get("error") or "",
                                "power_pending_suite": power_status.get("pending_suite") or "",
                            }
                        )
                        summary = power_status.get("last_summary")
                        if isinstance(summary, dict):
                            def _coerce_float(value: object) -> float:
                                try:
                                    return float(value)
                                except (TypeError, ValueError):
                                    return 0.0

                            def _coerce_int(value: object) -> int:
                                try:
                                    return int(value)
                                except (TypeError, ValueError):
                                    return 0

                            status_payload.update(
                                {
                                    "power_last_suite": summary.get("suite", ""),
                                    "power_last_energy_j": _coerce_float(summary.get("energy_j")),
                                    "power_last_avg_w": _coerce_float(summary.get("avg_power_w")),
                                    "power_last_duration_s": _coerce_float(summary.get("duration_s")),
                                    "power_last_samples": _coerce_int(summary.get("samples")),
                                    "power_last_csv_path": summary.get("csv_path", ""),
                                    "power_last_summary_path": summary.get("summary_json_path", ""),
                                }
                            )
                self._send(conn, {"ok": True, **status_payload})
                if telemetry:
                    telemetry.publish(
                        "status_reply",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": status_payload["suite"],
                            "running": status_payload["running"],
                            "pending_suite": status_payload["pending_suite"],
                            "last_requested_suite": status_payload["last_requested_suite"],
                        },
                    )
                return
            if cmd == "session_info":
                with state_lock:
                    session_id = self.state.get("session_id")
                session_value = str(session_id) if session_id is not None else ""
                self._send(
                    conn,
                    {
                        "ok": True,
                        "session_id": session_value,
                    },
                )
                return
            if cmd == "log_tail":
                with state_lock:
                    log_path = self.state.get("log_path")
                override = request.get("path")
                if override:
                    try:
                        candidate = Path(str(override))
                        log_path = candidate
                    except Exception:
                        pass
                if not log_path:
                    self._send(conn, {"ok": False, "error": "log_path_unavailable"})
                    return
                lines_requested = request.get("lines")
                try:
                    line_count = int(lines_requested) if lines_requested is not None else 120
                except (TypeError, ValueError):
                    line_count = 120
                tail_lines = _tail_file_lines(Path(log_path), line_count)
                banner = f"[follower] LOG TAIL ({log_path}) last {len(tail_lines)} lines"
                print(banner, flush=True)
                for entry in tail_lines:
                    print(entry, flush=True)
                self._send(
                    conn,
                    {
                        "ok": True,
                        "path": str(log_path),
                        "lines": tail_lines,
                        "count": len(tail_lines),
                    },
                )
                return
            if cmd == "mark":
                suite = request.get("suite")
                kind = str(request.get("kind") or "rekey")
                telemetry: Optional[TelemetryPublisher] = None
                monitor: Optional[HighSpeedMonitor] = None
                monitors = None
                monitor_prev_suite: Optional[str] = None
                proxy = None
                rotate_args: Optional[Tuple[int, Path, str]] = None
                suite_epoch_value = 0
                pending_suite_epoch_value: Optional[int] = None
                with state_lock:
                    if not suite:
                        self._send(conn, {"ok": False, "error": "missing suite"})
                        return
                    supported = list((self.state.get("capabilities") or {}).get("supported_suites", []))
                    if supported and suite not in supported:
                        self._send(conn, {"ok": False, "error": "suite unsupported"})
                        return
                    proxy = self.state["proxy"]
                    proxy_running = bool(proxy and proxy.poll() is None)
                    if not proxy_running:
                        self._send(conn, {"ok": False, "error": "proxy not running"})
                        return
                    old_suite = self.state.get("suite")
                    suite_epoch_value = int(self.state.get("suite_epoch") or 0)
                    self.state["prev_suite"] = old_suite
                    self.state["pending_suite"] = suite
                    # Monotonic epoch used by the GCS scheduler as authoritative confirmation.
                    # Mark sets the expected epoch for the in-flight rekey to ensure idempotent
                    # rekey_complete handling (retries won't double-increment).
                    pending_suite_epoch_value = suite_epoch_value + 1
                    self.state["pending_suite_epoch"] = pending_suite_epoch_value
                    self.state["last_requested_suite"] = suite
                    suite_outdir = self.state["suite_outdir"]
                    outdir = suite_outdir(suite)
                    monitors = self.state["monitors"]
                    monitor = self.state.get("high_speed_monitor")
                    telemetry = self.state.get("telemetry")
                    monitor_prev_suite = old_suite
                    if proxy:
                        rotate_args = (proxy.pid, outdir, suite)
                if monitor and monitor_prev_suite != suite:
                    monitor.start_rekey(monitor_prev_suite or "unknown", suite)
                if monitors and rotate_args:
                    pid, outdir, new_suite = rotate_args
                    monitors.rotate(pid, outdir, new_suite)
                self._send(conn, {"ok": True, "marked": suite})
                if telemetry:
                    telemetry.publish(
                        "mark",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                            "prev_suite": monitor_prev_suite,
                            "requested_suite": suite,
                            "kind": kind,
                            "suite_epoch": suite_epoch_value,
                            "pending_suite_epoch": pending_suite_epoch_value,
                        },
                    )
                self._append_mark_entry([
                    "mark",
                    str(time.time_ns()),
                    kind,
                    suite or "",
                    monitor_prev_suite or "",
                ])
                return
            if cmd == "rekey_complete":
                status_value = str(request.get("status", "ok"))
                success = status_value.lower() == "ok"
                requested_suite = str(request.get("suite") or "")
                monitor: Optional[HighSpeedMonitor] = None
                telemetry: Optional[TelemetryPublisher] = None
                monitors = None
                proxy = None
                rotate_args: Optional[Tuple[int, Path, str]] = None
                monitor_update_suite: Optional[str] = None
                suite_epoch_after = 0
                with state_lock:
                    monitor = self.state.get("high_speed_monitor")
                    telemetry = self.state.get("telemetry")
                    monitors = self.state["monitors"]
                    proxy = self.state.get("proxy")
                    suite_outdir = self.state["suite_outdir"]
                    if requested_suite:
                        self.state["last_requested_suite"] = requested_suite
                    previous_suite = self.state.get("prev_suite")
                    pending_suite = self.state.get("pending_suite")
                    suite_epoch = int(self.state.get("suite_epoch") or 0)
                    pending_suite_epoch = self.state.get("pending_suite_epoch")
                    if success:
                        if requested_suite and pending_suite and requested_suite != pending_suite:
                            print(
                                f"[follower] pending suite {pending_suite} does not match requested {requested_suite}; updating to requested",
                                flush=True,
                            )
                            pending_suite = requested_suite
                        if pending_suite:
                            self.state["suite"] = pending_suite
                            monitor_update_suite = pending_suite
                        elif requested_suite:
                            self.state["suite"] = requested_suite
                            monitor_update_suite = requested_suite
                        # Only advance epoch when a mark established an expected epoch.
                        # This keeps the operation idempotent across scheduler retries.
                        if pending_suite_epoch is not None:
                            try:
                                pending_epoch_int = int(pending_suite_epoch)
                            except (TypeError, ValueError):
                                pending_epoch_int = suite_epoch + 1
                            if pending_epoch_int > suite_epoch:
                                self.state["suite_epoch"] = pending_epoch_int
                    else:
                        if previous_suite is not None:
                            self.state["suite"] = previous_suite
                            monitor_update_suite = previous_suite
                            if proxy and proxy.poll() is None:
                                outdir = suite_outdir(previous_suite)
                                rotate_args = (proxy.pid, outdir, previous_suite)
                        elif pending_suite:
                            monitor_update_suite = pending_suite
                    self.state.pop("pending_suite", None)
                    self.state.pop("prev_suite", None)
                    self.state.pop("pending_suite_epoch", None)
                    current_suite = self.state.get("suite")
                    if success and requested_suite and current_suite != requested_suite:
                        print(
                            f"[follower] active suite {current_suite} disagrees with requested {requested_suite}; forcing to requested",
                            flush=True,
                        )
                        self.state["suite"] = requested_suite
                        current_suite = requested_suite
                        monitor_update_suite = requested_suite
                    suite_epoch_after = int(self.state.get("suite_epoch") or 0)
                if rotate_args and monitors and proxy and proxy.poll() is None:
                    pid, outdir, suite_name = rotate_args
                    monitors.rotate(pid, outdir, suite_name)
                if monitor and monitor_update_suite:
                    monitor.current_suite = monitor_update_suite
                    monitor.end_rekey(success=success, new_suite=monitor_update_suite)
                elif monitor:
                    monitor.end_rekey(success=success, new_suite=current_suite)
                # Acknowledgement includes status and suite context to aid scheduler recovery logic.
                # IMPORTANT: send exactly one reply line (older code accidentally replied twice).
                ack_payload = {
                    "ok": True,
                    "status": status_value,
                    "suite": current_suite,
                    "active_suite": current_suite,
                    "suite_epoch": suite_epoch_after,
                    "requested_suite": requested_suite or current_suite,
                }
                self._send(conn, ack_payload)
                if telemetry:
                    telemetry.publish(
                        "rekey_complete",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": current_suite,
                            "suite_epoch": suite_epoch_after,
                            "requested_suite": requested_suite or current_suite,
                            "status": status_value,
                        },
                    )
                return
            if cmd == "rollback":
                # Restore previous suite if available and clear transitional state.
                monitor: Optional[HighSpeedMonitor] = None
                telemetry: Optional[TelemetryPublisher] = None
                restored_suite: Optional[str] = None
                suite_epoch_after = 0
                with state_lock:
                    monitor = self.state.get("high_speed_monitor")
                    telemetry = self.state.get("telemetry")
                    previous_suite = self.state.pop("prev_suite", None)
                    pending_suite = self.state.pop("pending_suite", None)
                    self.state.pop("pending_suite_epoch", None)
                    if previous_suite is not None:
                        self.state["suite"] = previous_suite
                        restored_suite = previous_suite
                    else:
                        # If no previous, just leave current suite intact but clear pending.
                        restored_suite = self.state.get("suite")
                    suite_epoch_after = int(self.state.get("suite_epoch") or 0)
                if monitor and restored_suite:
                    monitor.current_suite = restored_suite
                    monitor.end_rekey(success=False, new_suite=restored_suite)
                resp = {
                    "ok": True,
                    "rolled_back": bool(restored_suite),
                    "suite": restored_suite,
                }
                self._send(conn, resp)
                if telemetry and restored_suite:
                    telemetry.publish(
                        "rollback",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": restored_suite,
                            "had_prev": restored_suite is not None,
                            "suite_epoch": suite_epoch_after,
                        },
                    )
                return
            if cmd == "schedule_mark":
                suite = request.get("suite")
                kind = str(request.get("kind") or "window")
                t0_ns = int(request.get("t0_ns", 0))
                if not suite or not t0_ns:
                    self._send(conn, {"ok": False, "error": "missing suite or t0_ns"})
                    return
                with state_lock:
                    supported = list((self.state.get("capabilities") or {}).get("supported_suites", []))
                if supported and suite not in supported:
                    self._send(conn, {"ok": False, "error": "suite unsupported"})
                    return

                def _do_mark() -> None:
                    delay = max(0.0, (t0_ns - time.time_ns()) / 1e9)
                    if delay:
                        time.sleep(delay)
                    proxy = None
                    monitors = None
                    monitor: Optional[HighSpeedMonitor] = None
                    suite_outdir_fn = None
                    with state_lock:
                        proxy = self.state.get("proxy")
                        monitors = self.state.get("monitors")
                        suite_outdir_fn = self.state.get("suite_outdir")
                        monitor = self.state.get("high_speed_monitor")
                    proxy_running = bool(proxy and proxy.poll() is None)
                    if monitor and suite and monitor.current_suite != suite:
                        monitor.current_suite = suite
                    if proxy_running and monitors and suite_outdir_fn and proxy:
                        outdir = suite_outdir_fn(suite)
                        monitors.rotate(proxy.pid, outdir, suite)
                    else:
                        write_marker(suite)
                    self._append_mark_entry([
                        "mark",
                        str(time.time_ns()),
                        kind,
                        suite or "",
                        "",
                    ])

                threading.Thread(target=_do_mark, daemon=True).start()
                self._send(conn, {"ok": True, "scheduled": suite, "t0_ns": t0_ns})
                telemetry = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish(
                        "schedule_mark",
                        {
                            "timestamp_ns": time.time_ns(),
                            "suite": suite,
                            "t0_ns": t0_ns,
                            "kind": kind,
                            "requested_suite": suite,
                        },
                    )
                return
            if cmd == "power_capture":
                manager = self.state.get("power_manager")
                if not isinstance(manager, PowerCaptureManager):
                    self._send(conn, {"ok": False, "error": "power_monitor_unavailable"})
                    return
                duration_s = request.get("duration_s")
                suite = request.get("suite") or self.state.get("suite") or "unknown"
                try:
                    duration_val = float(duration_s)
                except (TypeError, ValueError):
                    self._send(conn, {"ok": False, "error": "invalid_duration"})
                    return
                start_ns = request.get("start_ns")
                try:
                    start_ns_val = int(start_ns) if start_ns is not None else None
                except (TypeError, ValueError):
                    start_ns_val = None
                ok, error = manager.start_capture(suite, duration_val, start_ns_val)
                if ok:
                    self._send(
                        conn,
                        {
                            "ok": True,
                            "scheduled": True,
                            "suite": suite,
                            "duration_s": duration_val,
                            "start_ns": start_ns_val,
                        },
                    )
                    telemetry = self.state.get("telemetry")
                    if telemetry:
                        telemetry.publish(
                            "power_capture_request",
                            {
                                "timestamp_ns": time.time_ns(),
                                "suite": suite,
                                "duration_s": duration_val,
                                "start_ns": start_ns_val,
                            },
                        )
                else:
                    self._send(conn, {"ok": False, "error": error or "power_capture_failed"})
                return
            if cmd == "power_status":
                manager = self.state.get("power_manager")
                if not isinstance(manager, PowerCaptureManager):
                    self._send(conn, {"ok": False, "error": "power_monitor_unavailable"})
                    return
                status = manager.status()
                self._send(conn, {"ok": True, **status})
                return
            if cmd == "artifact_status":
                session_dir = self.state.get("session_dir")
                telemetry_status_path = self.state.get("telemetry_status_path")
                monitors_obj = self.state.get("monitors")
                manager = self.state.get("power_manager")
                manifest_path: Optional[Path] = None
                monitor_artifacts: list[str] = []
                if isinstance(monitors_obj, Monitors):
                    manifest_path = getattr(monitors_obj, "manifest_path", None)
                    artifact_paths = getattr(monitors_obj, "_artifact_paths", set())
                    lock_obj = getattr(monitors_obj, "_artifact_lock", None)
                    lock_acquired = False
                    if isinstance(lock_obj, threading.Lock):
                        try:
                            lock_acquired = lock_obj.acquire(timeout=1.0)
                        except TypeError:
                            lock_obj.acquire()
                            lock_acquired = True
                    try:
                        if artifact_paths:
                            monitor_artifacts = sorted(str(path) for path in artifact_paths)
                    finally:
                        if isinstance(lock_obj, threading.Lock) and lock_acquired:
                            lock_obj.release()
                power_status = {}
                if isinstance(manager, PowerCaptureManager):
                    try:
                        power_status = manager.status()
                    except Exception:
                        power_status = {}
                response = {
                    "ok": True,
                    "session_dir": str(session_dir) if session_dir else "",
                    "monitor_manifest_path": str(manifest_path) if manifest_path else "",
                    "telemetry_status_path": str(telemetry_status_path) if telemetry_status_path else "",
                    "artifact_paths": monitor_artifacts,
                    "power_status": power_status,
                }
                self._send(conn, response)
                return
            if cmd == "stop":
                self.state["monitors"].stop()
                self.state["stop_event"].set()
                self._send(conn, {"ok": True, "stopping": True})
                telemetry = self.state.get("telemetry")
                if telemetry:
                    telemetry.publish(
                        "stop",
                        {"timestamp_ns": time.time_ns()},
                    )
                return
            self._send(conn, {"ok": False, "error": "unknown_cmd"})
        finally:
            try:
                conn.close()
            except Exception:
                pass

    @staticmethod
    def _send(conn: socket.socket, obj: dict) -> None:
        conn.sendall((json.dumps(obj) + "\n").encode())

    def _append_mark_entry(self, row: list[str]) -> None:
        monitor = self.state.get("high_speed_monitor")
        if monitor and hasattr(monitor, "_append_rekey_mark"):
            try:
                monitor._append_rekey_mark(row)
                return
            except Exception:
                pass
        session_dir = self.state.get("session_dir")
        session_id = self.state.get("session_id")
        if not session_dir or not session_id:
            return
        path = Path(session_dir) / f"rekey_marks_{session_id}.csv"
        lock = self.state.setdefault("_marks_lock", threading.Lock())
        try:
            lock_acquired = lock.acquire(timeout=1.5)
        except TypeError:
            lock.acquire()
            lock_acquired = True
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            new_file = not path.exists()
            with path.open("a", newline="", encoding="utf-8") as handle:
                writer = csv.writer(handle)
                if new_file:
                    writer.writerow(["kind", "timestamp_ns", "field1", "field2", "field3"])
                writer.writerow(row)
        except Exception as exc:
            print(f"[{ts()}] follower mark append failed: {exc}", flush=True)
        finally:
            if lock_acquired:
                lock.release()


def main(argv: Optional[list[str]] = None) -> None:
    args = _parse_args(argv)
    device_generation = "pi5" if args.pi5 else "pi4"
    os.environ.setdefault("DRONE_DEVICE_GENERATION", device_generation)

    log_runtime_environment("follower")
    if hasattr(os, "geteuid"):
        try:
            if os.geteuid() == 0:
                print(
                    f"[{ts()}] follower running as root; ensure venv packages are available",
                    flush=True,
                )
        except Exception:
            pass

    OUTDIR.mkdir(parents=True, exist_ok=True)
    MARK_DIR.mkdir(parents=True, exist_ok=True)

    default_suite = discover_initial_suite()
    auto = AUTO_DRONE_CONFIG

    session_prefix = str(auto.get("session_prefix") or "session")
    session_id = os.environ.get("DRONE_SESSION_ID") or f"{session_prefix}_{int(time.time())}"
    stop_event = threading.Event()

    monitor_base_cfg = auto.get("monitor_output_base")
    if monitor_base_cfg:
        monitor_base = Path(monitor_base_cfg).expanduser()
    else:
        monitor_base = DEFAULT_MONITOR_BASE.expanduser()
    monitor_base = monitor_base.resolve()
    session_dir = monitor_base / session_id
    session_dir.mkdir(parents=True, exist_ok=True)
    print(f"[follower] session_id={session_id}")
    print(f"[follower] monitor output -> {session_dir}")
    print(f"[follower] device generation={device_generation}")

    capabilities = _collect_capabilities_snapshot()
    supported_suites = list(capabilities.get("supported_suites", []))
    if not supported_suites:
        print(
            "[follower] ERROR: no cryptographic suites available; check oqs/AEAD dependencies",
            file=sys.stderr,
            flush=True,
        )
        sys.exit(3)

    print(f"[follower] supported suites={supported_suites}")
    unavailable_count = len(capabilities.get("unsupported_suites", []))
    if unavailable_count:
        print(
            f"[follower] note: {unavailable_count} suites filtered due to missing KEM/SIG/AEAD",
            flush=True,
        )
    missing_aeads = capabilities.get("missing_aead_reasons") or {}
    if isinstance(missing_aeads, dict) and missing_aeads:
        for token, reason in sorted(missing_aeads.items()):
            print(f"[follower] missing AEAD {token}: {reason}", flush=True)
    missing_kems = capabilities.get("missing_kems") or []
    if missing_kems:
        print(f"[follower] missing KEMs: {missing_kems}", flush=True)
    missing_sigs = capabilities.get("missing_sigs") or []
    if missing_sigs:
        print(f"[follower] missing signatures: {missing_sigs}", flush=True)

    for env_key, env_value in auto.get("power_env", {}).items():
        if env_value is None:
            continue
        os.environ.setdefault(env_key, str(env_value))

    telemetry: Optional[TelemetryPublisher] = None
    telemetry_status_path: Optional[Path] = None
    telemetry_enabled = bool(auto.get("telemetry_enabled", True))
    telemetry_host_cfg = auto.get("telemetry_host")
    telemetry_host = str(telemetry_host_cfg or CONTROL_HOST or "0.0.0.0").strip() or "0.0.0.0"
    telemetry_port_cfg = auto.get("telemetry_port")
    telemetry_port = TELEMETRY_DEFAULT_PORT if telemetry_port_cfg in (None, "") else int(telemetry_port_cfg)

    if telemetry_enabled:
        telemetry = TelemetryPublisher(telemetry_host, telemetry_port, session_id)
        telemetry.start()
        print(f"[follower] telemetry publisher started (session={session_id})")
        telemetry_status_path = session_dir / "telemetry_status.json"
        telemetry.configure_status_sink(telemetry_status_path)
        try:
            telemetry.publish(
                "capabilities_snapshot",
                {
                    "sent_timestamp_ns": time.time_ns(),
                    "capabilities": capabilities,
                },
            )
        except Exception:
            pass
    else:
        print("[follower] telemetry disabled via AUTO_DRONE configuration")

    if bool(auto.get("cpu_optimize", True)):
        target_khz = PI5_TARGET_KHZ if args.pi5 else PI4_TARGET_KHZ
        optimize_cpu_performance(target_khz=target_khz)
        print(
            f"[follower] cpu governor target ~{target_khz / 1000:.0f} MHz ({device_generation})",
            flush=True,
        )

    _record_hardware_context(session_dir, telemetry)

    power_dir = session_dir / "power"
    power_dir.mkdir(parents=True, exist_ok=True)
    power_manager = PowerCaptureManager(power_dir, session_id, telemetry)
    if telemetry_status_path is not None:
        power_manager.register_telemetry_status(telemetry_status_path)

    high_speed_monitor = HighSpeedMonitor(session_dir, session_id, telemetry)
    high_speed_monitor.start()

    candidate_initial = auto.get("initial_suite") or default_suite
    if candidate_initial not in supported_suites:
        fallback_suite = supported_suites[0]
        print(
            f"[follower] initial suite {candidate_initial} unsupported; falling back to {fallback_suite}",
            flush=True,
        )
        candidate_initial = fallback_suite
    initial_suite = candidate_initial
    proxy, proxy_log = start_drone_proxy(initial_suite)
    monitors_enabled = bool(auto.get("monitors_enabled", True))
    if not monitors_enabled:
        print("[follower] monitors disabled via AUTO_DRONE configuration")
    monitors = Monitors(enabled=monitors_enabled, telemetry=telemetry, session_dir=session_dir)
    power_manager.register_monitor_manifest(monitors.manifest_path)
    monitors.register_artifacts(session_dir / "hardware_context.json")
    if telemetry_status_path is not None:
        monitors.register_artifacts(telemetry_status_path)
    power_manager.register_artifact_sink(lambda paths: monitors.register_artifacts(*paths))
    time.sleep(1)
    if proxy.poll() is None:
        monitors.start(proxy.pid, suite_outdir(initial_suite), initial_suite, session_dir=session_dir)
        high_speed_monitor.attach_proxy(proxy.pid)
        high_speed_monitor.current_suite = initial_suite
        monitors.register_artifacts(high_speed_monitor.csv_path, high_speed_monitor.rekey_marks_path)

    echo = UdpEcho(
        APP_BIND_HOST,
        APP_RECV_PORT,
        APP_SEND_HOST,
        APP_SEND_PORT,
        stop_event,
        high_speed_monitor,
        session_dir,
        telemetry,
    )
    echo.start()
    monitors.register_artifacts(echo.packet_log_path)

    state = {
        "proxy": proxy,
        "suite": initial_suite,
        "suite_epoch": 0,
        "pending_suite_epoch": None,
        "suite_outdir": suite_outdir,
        "monitors": monitors,
        "stop_event": stop_event,
        "high_speed_monitor": high_speed_monitor,
        "telemetry": telemetry,
        "prev_suite": None,
        "pending_suite": None,
        "last_requested_suite": initial_suite,
        "power_manager": power_manager,
        "device_generation": device_generation,
        "lock": threading.Lock(),
        "session_id": session_id,
        "session_dir": session_dir,
        "telemetry_status_path": telemetry_status_path,
        "log_path": Path(getattr(proxy_log, "name", "")) if proxy_log else None,
        "capabilities": capabilities,
    }
    control = ControlServer(CONTROL_HOST, CONTROL_PORT, state)
    control.start()

    try:
        while not stop_event.is_set():
            if proxy.poll() is not None:
                print(f"[follower] proxy exited with {proxy.returncode}", flush=True)
                stop_event.set()
                break
            time.sleep(0.5)
    except KeyboardInterrupt:
        stop_event.set()
    finally:
        monitors.stop()
        high_speed_monitor.stop()
        if proxy:
            try:
                proxy.send_signal(signal.SIGTERM)
            except Exception:
                pass
            killtree(proxy)
        if proxy_log:
            try:
                proxy_log.close()
            except Exception:
                pass
        if telemetry:
            telemetry.stop()


if __name__ == "__main__":
    # Test plan:
    # 1. Start the follower before the scheduler and confirm telemetry connects after retries.
    # 2. Run the Windows scheduler to drive a full suite cycle without rekey failures.
    # 3. Remove the logs/auto/drone/<suite> directory and confirm it is recreated automatically.
    # 4. Stop the telemetry collector mid-run and verify the follower reconnects without crashing.
    main()

==================================================

auto\gcs_scheduler.py
==================================================
#!/usr/bin/env python3
"""GCS scheduler that drives rekeys and UDP traffic using central configuration."""

from __future__ import annotations

import argparse
import bisect
import csv
import errno
import io
import json
import math
import os
import shlex
import socket
import subprocess
import sys
import threading
import time
import shutil
import ctypes
from contextlib import contextmanager
from collections import deque, OrderedDict
from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, IO, Iterable, Iterator, List, Optional, Set, Tuple



try:
    from openpyxl import Workbook
    from openpyxl.chart import BarChart, LineChart, Reference
except ImportError:  # pragma: no cover
    Workbook = None
    BarChart = None
    LineChart = None
    Reference = None

def _ensure_core_importable() -> Path:
    root = Path(__file__).resolve().parents[1]
    root_str = str(root)
    if root_str not in sys.path:
        sys.path.insert(0, root_str)
    try:
        __import__("core")
    except ModuleNotFoundError as exc:
        raise RuntimeError(
            f"Unable to import 'core'; repo root {root} missing from sys.path."
        ) from exc
    return root


ROOT = _ensure_core_importable()

from core import suites as suites_mod
from core.config import CONFIG
from tools.blackout_metrics import compute_blackout
from tools.merge_power import extract_power_fields
from tools.power_utils import PowerSample, align_gcs_to_drone, integrate_energy_mj, load_power_trace


DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

CONTROL_PORT = int(CONFIG.get("DRONE_CONTROL_PORT", 48080))

APP_SEND_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))
APP_RECV_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))

OUTDIR = ROOT / "logs/auto/gcs"
SUITES_OUTDIR = OUTDIR / "suites"
SECRETS_DIR = ROOT / "secrets/matrix"

EXCEL_OUTPUT_DIR = ROOT / Path(
    CONFIG.get("GCS_EXCEL_OUTPUT")
    or os.getenv("GCS_EXCEL_OUTPUT", "output/gcs")
)

COMBINED_OUTPUT_DIR = ROOT / Path(
    CONFIG.get("GCS_COMBINED_OUTPUT_BASE")
    or os.getenv("GCS_COMBINED_OUTPUT_BASE", "output/gcs")
)

DRONE_MONITOR_BASE = ROOT / Path(
    CONFIG.get("DRONE_MONITOR_OUTPUT_BASE")
    or os.getenv("DRONE_MONITOR_OUTPUT_BASE", "output/drone")
)

TELEMETRY_BIND_HOST = CONFIG.get("GCS_TELEMETRY_BIND", "0.0.0.0")
TELEMETRY_PORT = int(
    CONFIG.get("GCS_TELEMETRY_PORT")
    or CONFIG.get("DRONE_TELEMETRY_PORT")
    or 52080
)

PROXY_STATUS_PATH = OUTDIR / "gcs_status.json"
PROXY_SUMMARY_PATH = OUTDIR / "gcs_summary.json"
SUMMARY_CSV = OUTDIR / "summary.csv"
EVENTS_FILENAME = "blaster_events.jsonl"
BLACKOUT_CSV = OUTDIR / "gcs_blackouts.csv"
STEP_RESULTS_PATH = OUTDIR / "step_results.jsonl"
RUN_SEQUENCE_LOG = OUTDIR / "run_sequence.jsonl"

def _log_event(record: Dict[str, Any]) -> None:
    """Append a structured JSON event to the consolidated run-sequence log.

    Record MUST NOT contain secrets (key material, shared secrets, nonces).
    Only metadata (suite id, timings, counters, error codes) is permitted.
    Failures should include: phase, error_code, message, remediation_hint.
    """
    try:
        RUN_SEQUENCE_LOG.parent.mkdir(parents=True, exist_ok=True)
        payload = dict(record)
        # Basic mandatory fields enrichment
        if "ts" not in payload:
            payload["ts"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        line = json.dumps(payload, separators=(",", ":"))
        with open(RUN_SEQUENCE_LOG, "a", encoding="utf-8") as fh:
            fh.write(line + "\n")
    except Exception:
        # Never raise from logging path; keep scheduler robust.
        pass

def _suite_log_path(suite: str) -> Path:
    """Return per-suite textual log path (created on demand)."""
    d = suite_outdir(suite)
    p = d / "suite.log"
    return p

def _append_suite_text(suite: str, message: str) -> None:
    """Append human-readable line to per-suite log."""
    try:
        path = _suite_log_path(suite)
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "a", encoding="utf-8") as fh:
            fh.write(f"{message}\n")
    except Exception:
        pass

SEQ_TS_OVERHEAD_BYTES = 12
UDP_HEADER_BYTES = 8
IPV4_HEADER_BYTES = 20
IPV6_HEADER_BYTES = 40
MIN_DELAY_SAMPLES = 30
HYSTERESIS_WINDOW = 3
MAX_BISECT_STEPS = 3
WARMUP_FRACTION = 0.1
MAX_WARMUP_SECONDS = 1.0
SATURATION_COARSE_RATES = [5, 25, 50, 75, 100, 125, 150, 175, 200]
SATURATION_LINEAR_RATES = [
    5,
    10,
    15,
    20,
    25,
    30,
    35,
    40,
    45,
    50,
    60,
    70,
    80,
    90,
    100,
    125,
    150,
    175,
    200,
]
SATURATION_SIGNALS = ("owd_p95_spike", "delivery_degraded", "loss_excess")
TELEMETRY_BUFFER_MAXLEN_DEFAULT = 100_000
REKEY_SETTLE_SECONDS = 2.0
REKEY_WAIT_TIMEOUT_SECONDS = 45.0
REKEY_SKIP_MULTIPLIER = 1.0
REKEY_SKIP_THRESHOLD_SECONDS = REKEY_WAIT_TIMEOUT_SECONDS * REKEY_SKIP_MULTIPLIER
FAILURE_LOG_TAIL_LINES = 120
class SuiteSkipped(RuntimeError):
    def __init__(self, suite: str, reason: str, *, elapsed_s: Optional[float] = None) -> None:
        message = reason
        if elapsed_s is not None:
            message = f"{reason} (elapsed {elapsed_s:.2f}s)"
        super().__init__(message)
        self.suite = suite
        self.elapsed_s = elapsed_s

CLOCK_OFFSET_THRESHOLD_NS = 50_000_000
CONSTANT_RATE_MBPS_DEFAULT = 8.0
WINDOWS_TIMER_RESOLUTION_MS = 1

try:
    _WINMM = ctypes.WinDLL("winmm") if os.name == "nt" else None
except Exception:  # pragma: no cover - Windows only path
    _WINMM = None

_WINDOWS_TIMER_LOCK = threading.Lock()
_WINDOWS_TIMER_USERS = 0


@contextmanager
def _windows_timer_resolution() -> Iterator[bool]:
    """Best-effort reduction of Windows timer quantum during high-rate sends."""

    if os.name != "nt" or _WINMM is None:
        yield False
        return

    acquired = False
    global _WINDOWS_TIMER_USERS

    with _WINDOWS_TIMER_LOCK:
        if _WINDOWS_TIMER_USERS == 0:
            try:
                result = _WINMM.timeBeginPeriod(WINDOWS_TIMER_RESOLUTION_MS)  # type: ignore[attr-defined]
            except Exception:
                yield False
                return
            if result != 0:
                yield False
                return
        _WINDOWS_TIMER_USERS += 1
        acquired = True

    try:
        yield True
    finally:
        if acquired:
            with _WINDOWS_TIMER_LOCK:
                _WINDOWS_TIMER_USERS -= 1
                if _WINDOWS_TIMER_USERS == 0:
                    try:
                        _WINMM.timeEndPeriod(WINDOWS_TIMER_RESOLUTION_MS)  # type: ignore[attr-defined]
                    except Exception:
                        pass


def _precise_sleep_until(target_perf_ns: int) -> None:
    """Sleep with sub-millisecond granularity using perf_counter."""

    while True:
        now = time.perf_counter_ns()
        remaining = target_perf_ns - now
        if remaining <= 0:
            return
        if remaining > 5_000_000:  # >5 ms
            time.sleep((remaining - 2_000_000) / 1_000_000_000)
        elif remaining > 200_000:  # >0.2 ms
            time.sleep(0)
        else:
            # Busy-wait for the final few hundred nanoseconds
            continue


def _extract_iperf3_udp_metrics(report: Dict[str, Any]) -> Dict[str, float]:
    end_section = report.get("end") or {}
    summary = end_section.get("sum") or {}
    streams = end_section.get("streams") or []

    if (isinstance(summary, dict) and "packets" in summary) or not streams:
        source = summary if isinstance(summary, dict) else {}
    else:
        source = {}
        for entry in streams:
            if not isinstance(entry, dict):
                continue
            for key in ("udp", "sender", "receiver"):
                candidate = entry.get(key)
                if isinstance(candidate, dict) and "packets" in candidate:
                    source = candidate
                    break
            if source:
                break

    receiver = {}
    if streams:
        first = streams[0]
        if isinstance(first, dict):
            receiver = first.get("receiver") or {}

    def _get(obj: Dict[str, Any], key: str, default: float = 0.0) -> float:
        value = obj.get(key, default) if isinstance(obj, dict) else default
        try:
            return float(value)
        except (TypeError, ValueError):
            return default

    metrics = {
        "bits_per_second": _get(source, "bits_per_second"),
        "packets": int(_get(source, "packets")),
        "bytes": int(_get(source, "bytes")),
        "lost_packets": int(_get(source, "lost_packets")),
        "lost_percent": _get(source, "lost_percent"),
        "jitter_ms": _get(source, "jitter_ms"),
        "receiver_bytes": int(_get(receiver, "bytes", _get(source, "bytes"))),
        "receiver_packets": int(_get(receiver, "packets", _get(source, "packets"))),
        "receiver_bps": _get(receiver, "bits_per_second", _get(source, "bits_per_second")),
    }
    return metrics


def _run_iperf3_client(
    suite: str,
    *,
    duration_s: float,
    bandwidth_mbps: float,
    payload_bytes: int,
    server_host: str,
    server_port: int,
    binary: str,
    extra_args: Iterable[object],
) -> Dict[str, Any]:
    if bandwidth_mbps <= 0:
        raise RuntimeError("iperf3 traffic requires positive bandwidth")

    duration_int = max(1, int(round(duration_s)))
    payload_len = max(8, int(payload_bytes))
    bitrate_arg = f"{bandwidth_mbps:.6f}M"

    cmd: List[str] = [
        str(binary),
        "-c",
        str(server_host),
        "-u",
        "-b",
        bitrate_arg,
        "-t",
        str(duration_int),
        "-p",
        str(int(server_port)),
        "-l",
        str(payload_len),
        "--json",
    ]

    for arg in extra_args or []:
        cmd.append(str(arg))

    printable = " ".join(shlex.quote(part) for part in cmd)
    print(f"[{ts()}] launching iperf3 for suite {suite}: {printable}")

    try:
        completed = subprocess.run(cmd, capture_output=True, text=True, check=False)
    except FileNotFoundError as exc:
        raise RuntimeError(f"iperf3 binary not found: {exc}") from exc

    if completed.returncode != 0:
        error_text = completed.stderr.strip() or completed.stdout.strip() or f"exit {completed.returncode}"
        raise RuntimeError(f"iperf3 failed: {error_text}")

    try:
        report = json.loads(completed.stdout)
    except json.JSONDecodeError as exc:
        raise RuntimeError(f"iperf3 returned invalid JSON: {exc}") from exc

    metrics = _extract_iperf3_udp_metrics(report)
    if not metrics.get("packets"):
        raise RuntimeError("iperf3 report missing packet counters")

    result = {
        "sent_packets": int(metrics["packets"]),
        "sent_bytes": int(metrics["bytes"]),
        "rcvd_packets": int(metrics["receiver_packets"]),
        "rcvd_bytes": int(metrics["receiver_bytes"]),
        "lost_packets": int(metrics["lost_packets"]),
        "lost_percent": float(metrics["lost_percent"]),
        "jitter_ms": float(metrics["jitter_ms"]),
        "throughput_bps": float(metrics["receiver_bps"] or metrics["bits_per_second"]),
        "raw_report": report,
    }
    return result


def _compute_sampling_params(duration_s: float, event_sample: int, min_delay_samples: int) -> Tuple[int, int]:
    if event_sample <= 0:
        return 0, 0
    effective_sample = event_sample
    effective_min = max(0, min_delay_samples)
    if duration_s < 20.0:
        effective_sample = max(1, min(event_sample, 20))
        scale = max(duration_s, 5.0) / 20.0
        effective_min = max(10, int(math.ceil(effective_min * scale))) if effective_min else 0
    return effective_sample, effective_min


def _close_socket(sock: Optional[socket.socket]) -> None:
    if sock is None:
        return
    try:
        sock.close()
    except Exception:
        pass


def _close_file(handle: Optional[IO[str]]) -> None:
    if handle is None:
        return
    try:
        handle.flush()
    except Exception:
        pass
    try:
        handle.close()
    except Exception:
        pass


class P2Quantile:
    def __init__(self, p: float) -> None:
        if not 0.0 < p < 1.0:
            raise ValueError("p must be between 0 and 1")
        self.p = p
        self._initial: List[float] = []
        self._q: List[float] = []
        self._n: List[int] = []
        self._np: List[float] = []
        self._dn = [0.0, p / 2.0, p, (1.0 + p) / 2.0, 1.0]
        self.count = 0

    def add(self, sample: float) -> None:
        x = float(sample)
        self.count += 1
        if self.count <= 5:
            bisect.insort(self._initial, x)
            if self.count == 5:
                self._q = list(self._initial)
                self._n = [1, 2, 3, 4, 5]
                self._np = [1.0, 1.0 + 2.0 * self.p, 1.0 + 4.0 * self.p, 3.0 + 2.0 * self.p, 5.0]
            return

        if not self._q:
            # Should not happen, but guard for consistency
            self._q = list(self._initial)
            self._n = [1, 2, 3, 4, 5]
            self._np = [1.0, 1.0 + 2.0 * self.p, 1.0 + 4.0 * self.p, 3.0 + 2.0 * self.p, 5.0]

        if x < self._q[0]:
            self._q[0] = x
            k = 0
        elif x >= self._q[4]:
            self._q[4] = x
            k = 3
        else:
            k = 0
            for idx in range(4):
                if self._q[idx] <= x < self._q[idx + 1]:
                    k = idx
                    break

        for idx in range(k + 1, 5):
            self._n[idx] += 1

        for idx in range(5):
            self._np[idx] += self._dn[idx]

        for idx in range(1, 4):
            d = self._np[idx] - self._n[idx]
            if (d >= 1 and self._n[idx + 1] - self._n[idx] > 1) or (d <= -1 and self._n[idx - 1] - self._n[idx] < -1):
                step = 1 if d > 0 else -1
                candidate = self._parabolic(idx, step)
                if self._q[idx - 1] < candidate < self._q[idx + 1]:
                    self._q[idx] = candidate
                else:
                    self._q[idx] = self._linear(idx, step)
                self._n[idx] += step

    def value(self) -> float:
        if self.count == 0:
            return 0.0
        if self.count <= 5 and self._initial:
            rank = (self.count - 1) * self.p
            idx = max(0, min(len(self._initial) - 1, int(round(rank))))
            return float(self._initial[idx])
        if not self._q:
            return 0.0
        return float(self._q[2])

    def _parabolic(self, idx: int, step: int) -> float:
        numerator_left = self._n[idx] - self._n[idx - 1] + step
        numerator_right = self._n[idx + 1] - self._n[idx] - step
        denominator = self._n[idx + 1] - self._n[idx - 1]
        if denominator == 0:
            return self._q[idx]
        return self._q[idx] + (step / denominator) * (
            numerator_left * (self._q[idx + 1] - self._q[idx]) / max(self._n[idx + 1] - self._n[idx], 1)
            + numerator_right * (self._q[idx] - self._q[idx - 1]) / max(self._n[idx] - self._n[idx - 1], 1)
        )

    def _linear(self, idx: int, step: int) -> float:
        target = idx + step
        denominator = self._n[target] - self._n[idx]
        if denominator == 0:
            return self._q[idx]
        return self._q[idx] + step * (self._q[target] - self._q[idx]) / denominator


def wilson_interval(successes: int, n: int, z: float = 1.96) -> Tuple[float, float]:
    if n <= 0:
        return (0.0, 1.0)
    proportion = successes / n
    z2 = z * z
    denom = 1.0 + z2 / n
    center = (proportion + z2 / (2.0 * n)) / denom
    margin = (z * math.sqrt((proportion * (1.0 - proportion) / n) + (z2 / (4.0 * n * n)))) / denom
    return (max(0.0, center - margin), min(1.0, center + margin))


def ip_header_bytes_for_host(host: str) -> int:
    return IPV6_HEADER_BYTES if ":" in host else IPV4_HEADER_BYTES


APP_IP_HEADER_BYTES = ip_header_bytes_for_host(APP_SEND_HOST)


def ts() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def log_runtime_environment(component: str) -> None:
    preview = ";".join(sys.path[:5])
    print(f"[{ts()}] {component} python_exe={sys.executable}")
    print(f"[{ts()}] {component} cwd={Path.cwd()}")
    print(f"[{ts()}] {component} sys.path_prefix={preview}")


def _parse_cli_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Drive GCS automation runs and post-run tasks.")
    parser.add_argument(
        "--post-fetch-only",
        metavar="SESSION_ID",
        help="Re-run post-run steps for SESSION_ID and exit (artifacts must already be synced locally).",
    )
    parser.add_argument(
        "--generate-report",
        action="store_true",
        help="After --post-fetch-only completes, regenerate post-run reports.",
    )
    parser.add_argument(
        "--nist-level",
        metavar="LEVEL",
        help="Restrict runs to a single NIST security level (e.g., L1, L3, L5).",
    )
    parser.add_argument(
        "--nist-levels",
        metavar="LEVELS",
        help="Comma-separated list of NIST levels to include (e.g., L1,L3). Overrides --nist-level.",
    )
    parser.add_argument(
        "--verify",
        action="store_true",
        help="Run in fast verification mode (10s duration, no power capture, no benchmarking).",
    )
    parser.add_argument(
        "--duration-s",
        type=float,
        metavar="SECONDS",
        help="Override per-suite traffic duration (seconds). Takes precedence over --verify if provided.",
    )
    parsed = parser.parse_args(list(argv) if argv is not None else None)
    if parsed.generate_report and not parsed.post_fetch_only:
        parser.error("--generate-report requires --post-fetch-only")
    return parsed


def _merge_defaults(defaults: dict, override: Optional[dict]) -> dict:
    result = deepcopy(defaults)
    if isinstance(override, dict):
        for key, value in override.items():
            if isinstance(value, dict) and isinstance(result.get(key), dict):
                merged = result[key].copy()
                merged.update(value)
                result[key] = merged
            else:
                result[key] = value
    return result


AUTO_GCS_DEFAULTS = {
    "session_prefix": "session",  # string prefix for generated session IDs
    "traffic": "constant",  # modes: constant|blast|mavproxy|saturation
    "traffic_engine": "iperf3",  # traffic generator: native|iperf3
    "mode": "benchmark",  # benchmark|test_only
    "duration_s": 45.0,  # positive float seconds per traffic window
    "pre_gap_s": 1.0,  # non-negative float seconds before traffic starts
    "inter_gap_s": 15.0,  # non-negative float seconds between suites
    "payload_bytes": 256,  # UDP payload size in bytes (>0)
    "event_sample": 100,  # sample every N packets (0 disables sampling)
    "passes": 1,  # positive integer pass count over suite list
    "rate_pps": 0,  # target packets/sec (0 lets bandwidth_mbps drive)
    "bandwidth_mbps": 0.0,  # Mbps target (0 means derive from rate_pps)
    "max_rate_mbps": 200.0,  # saturation search maximum Mbps (>0)
    "sat_search": "auto",  # saturation search: auto|linear|bisect
    "sat_delivery_threshold": 0.85,  # accepted delivery ratio in saturation
    "sat_loss_threshold_pct": 5.0,  # max loss percent during saturation
    "sat_rtt_spike_factor": 1.6,  # RTT spike multiplier for saturation skip
    "suites": None,  # None for all suites or explicit iterable override
    "launch_proxy": True,  # run local proxy (False assumes external proxy)
    "monitors_enabled": True,  # enable local monitor collection
    "telemetry_enabled": True,  # publish telemetry back to scheduler
    "telemetry_target_host": DRONE_HOST,  # override telemetry target host
    "telemetry_port": TELEMETRY_PORT,  # override telemetry port
    "export_combined_excel": True,  # write combined Excel workbook
    "power_capture": True,  # request power capture from follower
    "artifact_fetch_strategy": "auto",  # artifact fetch strategy: auto|sftp|scp|rsync|command|http|smb
    "iperf3": {
        "server_host": None,  # override iperf3 server host or None for default
        "server_port": 5201,  # iperf3 UDP port (1-65535)
        "binary": "iperf3",  # iperf3 executable path/name
        "extra_args": [],  # additional CLI args list for iperf3
        "force_cli": False,  # force CLI even if JSON parsing fails
    },
    "aead_exclude_tokens": [],  # list of AEAD tokens to skip (e.g., ["ascon128"])
}

AUTO_GCS_CONFIG = _merge_defaults(AUTO_GCS_DEFAULTS, CONFIG.get("AUTO_GCS"))

AUTO_GCS_MODE = str(AUTO_GCS_CONFIG.get("mode") or os.getenv("AUTO_GCS_MODE") or "benchmark").strip().lower()
TEST_ONLY_MODE = AUTO_GCS_MODE == "test_only"

SATURATION_SEARCH_MODE = str(AUTO_GCS_CONFIG.get("sat_search") or "auto").lower()
SATURATION_RTT_SPIKE = float(AUTO_GCS_CONFIG.get("sat_rtt_spike_factor") or 1.6)
SATURATION_DELIVERY_THRESHOLD = float(AUTO_GCS_CONFIG.get("sat_delivery_threshold") or 0.85)
SATURATION_LOSS_THRESHOLD = float(AUTO_GCS_CONFIG.get("sat_loss_threshold_pct") or 5.0)


def _coerce_bool(value: object, default: bool) -> bool:
    if value is None:
        return default
    if isinstance(value, bool):
        return value
    if isinstance(value, (int, float)):
        return bool(value)
    text = str(value).strip().lower()
    if text in {"1", "true", "yes", "on"}:
        return True
    if text in {"0", "false", "no", "off"}:
        return False
    return default


POWER_FETCH_ENABLED = _coerce_bool(AUTO_GCS_CONFIG.get("power_fetch_enabled"), False)
POWER_FETCH_ENABLED = _coerce_bool(os.getenv("DRONE_POWER_FETCH_ENABLED"), POWER_FETCH_ENABLED)



def _candidate_local_artifact_paths(remote_path: str) -> List[Path]:
    variants: List[Path] = []
    raw = str(remote_path).strip()
    if not raw:
        return variants

    normalized = raw.replace("\\", "/")
    lower_norm = normalized.lower()
    marker = "/research/"
    idx = lower_norm.find(marker)
    if idx != -1:
        suffix = normalized[idx + len(marker) :]
        variants.append(ROOT / suffix)
    if lower_norm.endswith("/research"):
        variants.append(ROOT)
    if normalized.startswith("~/"):
        variants.append(ROOT / normalized[2:])

    expanded = Path(raw).expanduser()
    variants.append(expanded)
    if not expanded.is_absolute():
        variants.append(ROOT / expanded)

    deduped: List[Path] = []
    seen: Set[str] = set()
    for candidate in variants:
        key = str(candidate)
        if key in seen:
            continue
        seen.add(key)
        deduped.append(candidate)
    return deduped

FETCH_DISABLED_REASONS = {"power_fetch_disabled", "remote_fetch_removed"}


def _ensure_local_artifact(suite: str, remote_path: str, category: str) -> Tuple[Optional[Path], Optional[str]]:
    if not remote_path:
        return (None, None)

    remote_str = str(remote_path)
    for candidate in _candidate_local_artifact_paths(remote_str):
        try:
            if candidate.exists():
                try:
                    return (candidate.resolve(), None)
                except Exception:
                    return (candidate, None)
        except Exception:
            continue

    if POWER_FETCH_ENABLED:
        return (None, "remote_fetch_removed")
    return (None, "power_fetch_disabled")


def _ensure_local_power_artifact(suite: str, remote_path: str) -> Tuple[Optional[Path], Optional[str]]:
    return _ensure_local_artifact(suite, remote_path, "power")


def _errors_indicate_fetch_disabled(message: Optional[str]) -> bool:
    if not message:
        return False
    for chunk in message.split(";"):
        detail = chunk.strip()
        if not detail:
            continue
        reason = detail.split(":", 1)[-1].strip()
        if reason not in FETCH_DISABLED_REASONS:
            return False
    return True


def _fetch_power_artifacts(suite: str, payload: Dict[str, object]) -> Tuple[Dict[str, Path], Optional[str]]:
    fetched: Dict[str, Path] = {}
    errors: List[str] = []
    for key in ("csv_path", "summary_json_path"):
        value = payload.get(key)
        if not value:
            continue
        local_path, err = _ensure_local_power_artifact(suite, str(value))
        if local_path is not None:
            fetched[key] = local_path
        elif err:
            errors.append(f"{key}:{err}")
    error_msg = "; ".join(errors) if errors else None
    return fetched, error_msg


def _resolve_manifest_entry(entry: object, session_dir: Optional[str]) -> Tuple[Optional[Path], Optional[str]]:
    if entry in (None, ""):
        return None, "empty"
    try:
        candidate = Path(str(entry))
    except Exception as exc:
        return None, f"invalid:{exc}"
    if session_dir:
        try:
            session_path = Path(session_dir)
        except Exception as exc:
            return None, f"session_dir_invalid:{exc}"
        if not candidate.is_absolute():
            candidate = session_path / candidate
    return candidate, None


def _fetch_monitor_artifacts(suite: str, payload: Dict[str, object]) -> Dict[str, object]:
    result: Dict[str, object] = {
        "manifest_path": None,
        "telemetry_status_path": None,
        "artifact_paths": [],
        "categorized_paths": {},
        "remote_map": {},
        "status": "",
        "error": "",
    }

    if not payload:
        result["status"] = "missing"
        return result

    session_dir_val = payload.get("session_dir") or ""
    session_dir_str = str(session_dir_val) if session_dir_val else ""

    manifest_remote = payload.get("monitor_manifest_path") or ""
    if not manifest_remote and session_dir_str:
        manifest_remote = str(Path(session_dir_str) / "monitor_manifest.json")

    errors: List[str] = []
    disabled_due_to_fetch = False
    manifest_local: Optional[Path] = None
    manifest_err: Optional[str] = None
    categorized_paths: Dict[str, List[Path]] = {}
    remote_map: Dict[str, str] = {}
    aggregate_paths: List[Path] = []

    def _record_artifact(category: str, local: Path, remote: str) -> None:
        local_key = str(local)
        if local_key in remote_map:
            return
        bucket = categorized_paths.setdefault(category, [])
        bucket.append(local)
        aggregate_paths.append(local)
        remote_map[local_key] = remote

    if manifest_remote:
        manifest_local, manifest_err = _ensure_local_artifact(suite, manifest_remote, "monitor")

    if manifest_local is None:
        if manifest_err in FETCH_DISABLED_REASONS:
            disabled_due_to_fetch = True
        elif manifest_err:
            errors.append(f"manifest:{manifest_err}")
        elif manifest_remote:
            errors.append("manifest:not_found")
    else:
        result["manifest_path"] = manifest_local
        remote_map[str(manifest_local)] = str(manifest_remote)
        try:
            manifest_data = json.loads(manifest_local.read_text(encoding="utf-8"))
            artifacts = manifest_data.get("artifacts") or []
        except Exception as exc:
            errors.append(f"manifest_parse:{exc}")
            artifacts = []

        seen: Set[str] = set()
        for entry in artifacts if isinstance(artifacts, list) else []:
            resolved_path, resolve_err = _resolve_manifest_entry(entry, session_dir_str)
            if resolved_path is None:
                if resolve_err and resolve_err != "empty":
                    errors.append(f"manifest_entry:{resolve_err}")
                continue
            resolved_str = str(resolved_path)
            if resolved_str in seen:
                continue
            seen.add(resolved_str)
            path_obj = Path(resolved_str)
            parts_lower = [part.lower() for part in path_obj.parts]
            name_lower = path_obj.name.lower()
            if "power" in parts_lower or "power" in name_lower:
                category = "power"
            else:
                category = "monitor"
            local_path, fetch_err = _ensure_local_artifact(suite, resolved_str, category)
            if local_path is not None:
                _record_artifact(category, local_path, resolved_str)
            elif fetch_err in FETCH_DISABLED_REASONS:
                disabled_due_to_fetch = True
            elif fetch_err:
                errors.append(f"{resolved_str}:{fetch_err}")

    telemetry_remote = payload.get("telemetry_status_path") or ""
    if not telemetry_remote and session_dir_str:
        telemetry_remote = str(Path(session_dir_str) / "telemetry_status.json")
    if telemetry_remote:
        telemetry_local, telemetry_err = _ensure_local_artifact(suite, telemetry_remote, "telemetry")
        if telemetry_local is not None:
            result["telemetry_status_path"] = telemetry_local
            _record_artifact("telemetry", telemetry_local, str(telemetry_remote))
        elif telemetry_err in FETCH_DISABLED_REASONS:
            disabled_due_to_fetch = True
        elif telemetry_err:
            errors.append(f"telemetry:{telemetry_err}")

    if aggregate_paths:
        result["artifact_paths"] = aggregate_paths
    if categorized_paths:
        result["categorized_paths"] = categorized_paths
    if remote_map:
        result["remote_map"] = remote_map

    if errors:
        result["error"] = "; ".join(sorted(set(errors)))
    if result["manifest_path"] or result["artifact_paths"]:
        result["status"] = "ok" if not errors else "partial"
    elif disabled_due_to_fetch and not errors:
        result["status"] = "disabled"
    elif errors:
        result["status"] = "error"
    else:
        result["status"] = "missing"

    return result


def mkdirp(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def _atomic_write_bytes(path: Path, data: bytes, *, tmp_suffix: str = ".tmp", retries: int = 6, backoff: float = 0.05) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp_path = path.with_name(path.name + tmp_suffix)
    fd = os.open(str(tmp_path), os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o644)
    try:
        with os.fdopen(fd, "wb", closefd=True) as handle:
            handle.write(data)
            try:
                handle.flush()
                os.fsync(handle.fileno())
            except Exception:
                pass
    except Exception:
        try:
            os.remove(tmp_path)
        except Exception:
            pass
        raise

    delay = backoff
    last_exc: Optional[Exception] = None
    for attempt in range(retries):
        try:
            os.replace(tmp_path, path)
            return
        except PermissionError as exc:  # pragma: no cover - platform specific
            last_exc = exc
            if attempt == retries - 1:
                try:
                    os.remove(path)
                except FileNotFoundError:
                    pass
                except Exception:
                    pass
                try:
                    os.replace(tmp_path, path)
                    return
                except Exception as final_exc:
                    last_exc = final_exc
                    break
        except OSError as exc:  # pragma: no cover - platform specific
            if exc.errno not in (errno.EACCES, errno.EPERM):
                raise
            last_exc = exc
            if attempt == retries - 1:
                try:
                    os.remove(path)
                except FileNotFoundError:
                    pass
                except Exception:
                    pass
                try:
                    os.replace(tmp_path, path)
                    return
                except Exception as final_exc:
                    last_exc = final_exc
                    break
        time.sleep(delay)
        delay = min(delay * 2, 0.5)

    try:
        os.remove(tmp_path)
    except Exception:
        pass
    if last_exc is not None:
        raise last_exc


def _robust_copy(src: Path, dst: Path, attempts: int = 3, delay: float = 0.05) -> bool:
    for attempt in range(1, attempts + 1):
        try:
            data = src.read_bytes()
        except FileNotFoundError:
            return False
        except OSError as exc:
            print(f"[WARN] failed to read {src}: {exc}", file=sys.stderr)
            if attempt == attempts:
                return False
            time.sleep(delay)
            continue
        try:
            _atomic_write_bytes(dst, data)
            return True
        except Exception as exc:  # pragma: no cover - platform specific
            print(f"[WARN] failed to update {dst}: {exc}", file=sys.stderr)
            if attempt == attempts:
                return False
            time.sleep(delay)
    return False


def suite_outdir(suite: str) -> Path:
    target = SUITES_OUTDIR / suite
    mkdirp(target)
    return target


def _as_float(value: object) -> Optional[float]:
    if value in (None, ""):
        return None
    try:
        result = float(value)
    except (TypeError, ValueError):
        return None
    if math.isnan(result):
        return None
    return result


def _rounded(value: object, digits: int) -> object:
    num = _as_float(value)
    if num is None:
        return ""
    return round(num, digits)


def _ns_to_ms(value: object) -> float:
    try:
        num = float(value)
    except (TypeError, ValueError):
        return 0.0
    return round(num / 1_000_000.0, 3)


def _ns_to_us(value: object) -> float:
    try:
        num = float(value)
    except (TypeError, ValueError):
        return 0.0
    return round(num / 1_000.0, 3)


def _flatten_handshake_metrics(metrics: Dict[str, object]) -> Dict[str, object]:
    base = {
        "handshake_role": "",
        "handshake_total_ms": 0.0,
        "handshake_wall_start_ns": 0,
        "handshake_wall_end_ns": 0,
        "handshake_kem_keygen_us": 0.0,
        "handshake_kem_encap_us": 0.0,
        "handshake_kem_decap_us": 0.0,
        "handshake_sig_sign_us": 0.0,
        "handshake_sig_verify_us": 0.0,
        "handshake_kdf_server_us": 0.0,
        "handshake_kdf_client_us": 0.0,
        "handshake_kem_pub_bytes": 0,
        "handshake_kem_ct_bytes": 0,
        "handshake_sig_bytes": 0,
        "handshake_auth_tag_bytes": 0,
        "handshake_shared_secret_bytes": 0,
        "handshake_server_hello_bytes": 0,
        "handshake_challenge_bytes": 0,
    }
    if not isinstance(metrics, dict) or not metrics:
        return base.copy()

    result = base.copy()

    def _as_int(value: object) -> int:
        try:
            return int(value)
        except (TypeError, ValueError):
            return 0

    result["handshake_role"] = str(metrics.get("role") or "")
    result["handshake_total_ms"] = _ns_to_ms(metrics.get("handshake_total_ns"))
    result["handshake_wall_start_ns"] = _as_int(metrics.get("handshake_wall_start_ns"))
    result["handshake_wall_end_ns"] = _as_int(metrics.get("handshake_wall_end_ns"))

    primitives = metrics.get("primitives") or {}
    if isinstance(primitives, dict):
        kem_metrics = primitives.get("kem") or {}
        if isinstance(kem_metrics, dict):
            result["handshake_kem_keygen_us"] = _ns_to_us(kem_metrics.get("keygen_ns"))
            result["handshake_kem_encap_us"] = _ns_to_us(kem_metrics.get("encap_ns"))
            result["handshake_kem_decap_us"] = _ns_to_us(kem_metrics.get("decap_ns"))
            result["handshake_kem_pub_bytes"] = _as_int(kem_metrics.get("public_key_bytes"))
            result["handshake_kem_ct_bytes"] = _as_int(kem_metrics.get("ciphertext_bytes"))
            result["handshake_shared_secret_bytes"] = _as_int(kem_metrics.get("shared_secret_bytes"))
        sig_metrics = primitives.get("signature") or {}
        if isinstance(sig_metrics, dict):
            result["handshake_sig_sign_us"] = _ns_to_us(sig_metrics.get("sign_ns"))
            result["handshake_sig_verify_us"] = _ns_to_us(sig_metrics.get("verify_ns"))
            if not result["handshake_sig_bytes"]:
                result["handshake_sig_bytes"] = _as_int(sig_metrics.get("signature_bytes"))

    result["handshake_kdf_server_us"] = _ns_to_us(metrics.get("kdf_server_ns"))
    result["handshake_kdf_client_us"] = _ns_to_us(metrics.get("kdf_client_ns"))

    artifacts = metrics.get("artifacts") or {}
    if isinstance(artifacts, dict):
        if not result["handshake_sig_bytes"]:
            result["handshake_sig_bytes"] = _as_int(artifacts.get("signature_bytes"))
        result["handshake_auth_tag_bytes"] = _as_int(artifacts.get("auth_tag_bytes"))
        result["handshake_server_hello_bytes"] = _as_int(artifacts.get("server_hello_bytes"))
        result["handshake_challenge_bytes"] = _as_int(artifacts.get("challenge_bytes"))

    return result


def resolve_suites(requested: Optional[Iterable[str]]) -> List[str]:
    suite_listing = suites_mod.list_suites()
    if isinstance(suite_listing, dict):
        available = list(suite_listing.keys())
    else:
        available = list(suite_listing)
    if not available:
        raise RuntimeError("No suites registered in core.suites; cannot proceed")

    if not requested:
        return available

    resolved: List[str] = []
    seen: Set[str] = set()
    for name in requested:
        info = suites_mod.get_suite(name)
        suite_id = info["suite_id"]
        if suite_id not in available:
            raise RuntimeError(f"Suite {name} not present in core registry")
        if suite_id not in seen:
            resolved.append(suite_id)
            seen.add(suite_id)
    return resolved


def _apply_nist_level_filter(all_suite_ids: List[str], parsed_args: argparse.Namespace) -> List[str]:
    """Filter suite IDs by requested NIST level(s) from CLI arguments."""
    levels_arg = getattr(parsed_args, "nist_levels", None)
    single_level = getattr(parsed_args, "nist_level", None)
    if not levels_arg and not single_level:
        return all_suite_ids
    if levels_arg:
        raw_levels = [chunk.strip() for chunk in str(levels_arg).split(",") if chunk.strip()]
    else:
        raw_levels = [str(single_level).strip()]
    if not raw_levels:
        return all_suite_ids
    try:
        valid_levels = set(suites_mod.valid_nist_levels())
    except Exception:
        valid_levels = {"L1", "L3", "L5"}
    requested = [lvl for lvl in raw_levels if lvl in valid_levels]
    if not requested:
        print(f"[WARN] No valid NIST levels in request {raw_levels}; skipping filter", file=sys.stderr)
        return all_suite_ids
    try:
        filtered_tuple = suites_mod.filter_suites_by_levels(requested)
    except Exception as exc:
        print(f"[WARN] NIST level filtering failed ({requested}): {exc}; continuing without filter", file=sys.stderr)
        return all_suite_ids
    filtered_set = set(filtered_tuple)
    return [sid for sid in all_suite_ids if sid in filtered_set]


def preflight_filter_suites(candidates: List[str]) -> Tuple[List[str], List[Dict[str, object]]]:
    """Filter out suites whose primitives are not available in the current runtime."""

    try:
        enabled_kems = {name for name in suites_mod.enabled_kems()}
    except Exception as exc:
        print(f"[WARN] suite capability probe failed (KEM list): {exc}", file=sys.stderr)
        return list(candidates), []

    try:
        enabled_sigs = {name for name in suites_mod.enabled_sigs()}
    except Exception as exc:
        print(f"[WARN] suite capability probe failed (signature list): {exc}", file=sys.stderr)
        return list(candidates), []

    available_aeads = set(suites_mod.available_aead_tokens())
    missing_aead_reasons = suites_mod.unavailable_aead_reasons()

    filtered: List[str] = []
    skipped: List[Dict[str, object]] = []

    for suite_id in candidates:
        try:
            suite_info = suites_mod.get_suite(suite_id)
        except NotImplementedError as exc:
            skipped.append(
                {
                    "suite": suite_id,
                    "reason": "unknown_suite",
                    "details": str(exc),
                    "stage": "preflight",
                }
            )
            continue

        missing_reasons: List[str] = []
        kem_name = suite_info.get("kem_name")
        sig_name = suite_info.get("sig_name")
        aead_token = suite_info.get("aead_token")

        if enabled_kems and kem_name not in enabled_kems:
            missing_reasons.append("kem_unavailable")
        if enabled_sigs and sig_name not in enabled_sigs:
            missing_reasons.append("sig_unavailable")
        if available_aeads and aead_token not in available_aeads:
            missing_reasons.append("aead_unavailable")

        if missing_reasons:
            detail_payload: Dict[str, object] = {
                "kem_name": kem_name,
                "sig_name": sig_name,
            }
            if aead_token:
                detail_payload["aead_token"] = aead_token
                hint = missing_aead_reasons.get(str(aead_token))
                if hint:
                    detail_payload["aead_hint"] = hint
            skipped.append(
                {
                    "suite": suite_info.get("suite_id", suite_id),
                    "reason": "+".join(missing_reasons),
                    "details": detail_payload,
                    "stage": "preflight",
                }
            )
            continue

        filtered.append(suite_info["suite_id"])

    return filtered, skipped


def filter_suites_for_follower(
    candidates: List[str], capabilities: Dict[str, object]
) -> Tuple[List[str], List[Dict[str, object]]]:
    """Intersect scheduler suite plan with follower-reported capabilities."""

    if not capabilities:
        return list(candidates), []

    supported = set()
    supported_list = capabilities.get("supported_suites")
    if isinstance(supported_list, (list, tuple, set)):
        supported = {str(item) for item in supported_list if isinstance(item, str)}

    unsupported_entries: Dict[str, dict] = {}
    raw_unsupported = capabilities.get("unsupported_suites")
    if isinstance(raw_unsupported, list):
        for entry in raw_unsupported:
            if isinstance(entry, dict):
                suite_name = entry.get("suite")
                if isinstance(suite_name, str):
                    unsupported_entries[suite_name] = entry

    filtered: List[str] = []
    skipped: List[Dict[str, object]] = []

    if not supported:
        for suite_id in candidates:
            skipped.append(
                {
                    "suite": suite_id,
                    "reason": "drone_no_supported_suites",
                    "details": {},
                    "stage": "follower",
                }
            )
        return [], skipped

    for suite_id in candidates:
        if suite_id in supported:
            filtered.append(suite_id)
            continue

        detail_entry = unsupported_entries.get(suite_id)
        reason_tokens: List[str] = []
        details: Dict[str, object] = {}
        if detail_entry:
            raw_reasons = detail_entry.get("reasons")
            if isinstance(raw_reasons, (list, tuple, set)):
                reason_tokens = [str(item) for item in raw_reasons if item]
            elif raw_reasons:
                reason_tokens = [str(raw_reasons)]
            detail_details = detail_entry.get("details")
            if isinstance(detail_details, dict):
                details = detail_details

        skipped.append(
            {
                "suite": suite_id,
                "reason": "+".join(reason_tokens) if reason_tokens else "suite_not_supported",
                "details": details,
                "stage": "follower",
            }
        )

    return filtered, skipped


def preferred_initial_suite(candidates: List[str]) -> Optional[str]:
    configured = CONFIG.get("SIMPLE_INITIAL_SUITE")
    if not configured:
        return None
    try:
        suite_id = suites_mod.get_suite(configured)["suite_id"]
    except NotImplementedError:
        return None
    return suite_id if suite_id in candidates else None


def ctl_send(obj: dict, timeout: float = 2.0, retries: int = 4, backoff: float = 0.5) -> dict:
    """Send a single JSON control command and read a one-line JSON reply.

    Previous implementation used sock.makefile().readline() which could block
    past the intended timeout if the follower never responded (e.g. encryption
    failure upstream). This version enforces an absolute deadline and performs
    a manual recv loop looking for a '\n'. It retries with backoff on any
    connection / send / parse error.
    """
    last_exc: Optional[Exception] = None
    for attempt in range(1, retries + 1):
        deadline = time.time() + timeout
        try:
            with socket.create_connection((DRONE_HOST, CONTROL_PORT), timeout=timeout) as sock:
                # Ensure subsequent operations also respect timeout.
                sock.settimeout(max(0.1, timeout))
                payload = (json.dumps(obj) + "\n").encode()
                sock.sendall(payload)
                sock.shutdown(socket.SHUT_WR)
                buf = bytearray()
                # Manual recv loop until newline or deadline.
                while True:
                    remaining = deadline - time.time()
                    if remaining <= 0:
                        raise TimeoutError(f"control recv timeout waiting for reply to cmd={obj.get('cmd')}")
                    try:
                        chunk = sock.recv(4096)
                    except socket.timeout:
                        continue  # Allow loop to re-check deadline.
                    if not chunk:
                        # Peer closed without newline; treat as empty response.
                        break
                    buf.extend(chunk)
                    nl = buf.find(b"\n")
                    if nl != -1:
                        line = buf[:nl].decode(errors="replace").strip()
                        if not line:
                            return {}
                        try:
                            return json.loads(line)
                        except Exception as parse_exc:
                            raise RuntimeError(f"invalid JSON reply for cmd={obj.get('cmd')}: {parse_exc}")
                # No newline before close; attempt to parse entire buffer if non-empty.
                if buf:
                    try:
                        return json.loads(buf.decode(errors="replace").strip())
                    except Exception:
                        return {}
                return {}
        except Exception as exc:
            last_exc = exc
            if attempt < retries:
                time.sleep(backoff * attempt)
                continue
            raise
    if last_exc:
        raise last_exc
    return {}


def _ensure_suite_supported_remote(suite: str, stage: str) -> None:
    """Best-effort validation that the follower can service a suite."""

    payload = {
        "cmd": "validate_suite",
        "suite": suite,
        "stage": stage,
    }
    try:
        response = ctl_send(payload, timeout=1.2, retries=2, backoff=0.4)
    except Exception as exc:
        print(
            f"[WARN] validate_suite failed for {suite} during {stage}: {exc}",
            file=sys.stderr,
        )
        return

    if response.get("ok"):
        return

    error = str(response.get("error") or "unknown_error")
    if error == "unknown_cmd":
        # Older followers may not implement validation; continue best-effort.
        return

    detail_text = ""
    details = response.get("details")
    if isinstance(details, dict):
        reasons = details.get("reasons")
        if isinstance(reasons, (list, tuple, set)):
            detail_text = "+".join(str(item) for item in reasons if item)
        elif isinstance(reasons, str):
            detail_text = reasons
        hint = details.get("aead_hint") or details.get("hint")
        if hint:
            detail_text = f"{detail_text}:{hint}" if detail_text else str(hint)
    if detail_text:
        error = f"{error}:{detail_text}"

    if "unsupported" in error:
        raise SuiteSkipped(suite, f"follower rejects suite during {stage}: {error}")

    raise RuntimeError(f"validate_suite failed during {stage} for suite {suite}: {error}")


def request_power_capture(suite: str, duration_s: float, start_ns: Optional[int]) -> dict:
    payload = {
        "cmd": "power_capture",
        "suite": suite,
        "duration_s": duration_s,
    }
    if start_ns is not None:
        payload["start_ns"] = int(start_ns)
    try:
        resp = ctl_send(payload, timeout=1.5, retries=2, backoff=0.4)
    except Exception as exc:
        print(f"[WARN] power_capture request failed: {exc}", file=sys.stderr)
        return {"ok": False, "error": str(exc)}
    return resp


def poll_power_status(max_wait_s: float = 12.0, poll_s: float = 0.6) -> dict:
    deadline = time.time() + max_wait_s
    last: dict = {}
    while time.time() < deadline:
        try:
            resp = ctl_send({"cmd": "power_status"}, timeout=1.5, retries=1, backoff=0.3)
        except Exception as exc:
            last = {"ok": False, "error": str(exc)}
            time.sleep(poll_s)
            continue
        last = resp
        if not resp.get("ok"):
            break
        if not resp.get("available", True):
            break
        if not resp.get("busy", False):
            break
        time.sleep(poll_s)
    return last


class Blaster:
    """High-rate UDP blaster with RTT sampling and throughput accounting."""

    def __init__(
        self,
        send_host: str,
        send_port: int,
        recv_host: str,
        recv_port: int,
        events_path: Optional[Path],
        payload_bytes: int,
        sample_every: int,
        offset_ns: int,
    ) -> None:
        self.payload_bytes = max(12, int(payload_bytes))
        self.sample_every = max(0, int(sample_every))
        self.offset_ns = offset_ns

        send_info = socket.getaddrinfo(send_host, send_port, 0, socket.SOCK_DGRAM)
        if not send_info:
            raise OSError(f"Unable to resolve send address {send_host}:{send_port}")
        send_family, _stype, _proto, _canon, send_sockaddr = send_info[0]

        recv_info = socket.getaddrinfo(recv_host, recv_port, send_family, socket.SOCK_DGRAM)
        if not recv_info:
            recv_info = socket.getaddrinfo(recv_host, recv_port, 0, socket.SOCK_DGRAM)
        if not recv_info:
            raise OSError(f"Unable to resolve recv address {recv_host}:{recv_port}")
        recv_family, _rstype, _rproto, _rcanon, recv_sockaddr = recv_info[0]

        self.tx = socket.socket(send_family, socket.SOCK_DGRAM)
        self.rx = socket.socket(recv_family, socket.SOCK_DGRAM)
        self.send_addr = send_sockaddr
        self.recv_addr = recv_sockaddr
        self.rx.bind(self.recv_addr)
        self.rx.settimeout(0.001)
        self.rx_burst = max(1, int(os.getenv("GCS_RX_BURST", "32")))
        self._lock = threading.Lock()
        self._run_active = threading.Event()
        self._rx_thread: Optional[threading.Thread] = None
        self._stop_event: Optional[threading.Event] = None
        self._closed = False
        try:
            # Allow overriding socket buffer sizes via environment variables
            # Use GCS_SOCK_SNDBUF and GCS_SOCK_RCVBUF if present, otherwise default to 1 MiB
            sndbuf = int(os.getenv("GCS_SOCK_SNDBUF", str(1 << 20)))
            rcvbuf = int(os.getenv("GCS_SOCK_RCVBUF", str(1 << 20)))
            self.tx.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, sndbuf)
            self.rx.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, rcvbuf)
            actual_snd = self.tx.getsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF)
            actual_rcv = self.rx.getsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF)
            print(
                f"[{ts()}] blaster UDP socket buffers: snd={actual_snd} rcv={actual_rcv}",
                flush=True,
            )
        except Exception:
            # best-effort; continue even if setting buffers fails
            pass

        family = self.tx.family if self.tx.family in (socket.AF_INET, socket.AF_INET6) else self.rx.family
        ip_bytes = IPV6_HEADER_BYTES if family == socket.AF_INET6 else IPV4_HEADER_BYTES
        self.wire_header_bytes = UDP_HEADER_BYTES + ip_bytes

        self.events_path = events_path
        self.events: Optional[IO[str]] = None
        if events_path is not None:
            mkdirp(events_path.parent)
            self.events = open(events_path, "w", encoding="utf-8")

        self.truncated = 0
        self.sent = 0
        self.rcvd = 0
        self.sent_bytes = 0
        self.rcvd_bytes = 0
        self.rtt_sum_ns = 0
        self.rtt_samples = 0
        self.rtt_max_ns = 0
        self.rtt_min_ns: Optional[int] = None
        self.pending: Dict[int, int] = {}
        self.rtt_p50 = P2Quantile(0.5)
        self.rtt_p95 = P2Quantile(0.95)
        self.owd_p50 = P2Quantile(0.5)
        self.owd_p95 = P2Quantile(0.95)
        self.owd_samples = 0
        self.owd_p50_ns = 0.0
        self.owd_p95_ns = 0.0
        self.rtt_p50_ns = 0.0
        self.rtt_p95_ns = 0.0

    def _log_event(self, payload: dict) -> None:
        # Buffered write; caller flushes at end of run()
        if self.events is None:
            return
        self.events.write(json.dumps(payload) + "\n")

    def _now(self) -> int:
        return time.time_ns() + self.offset_ns

    def _maybe_log(self, kind: str, seq: int, t_ns: int) -> None:
        if self.sample_every == 0:
            return
        if kind == "send":
            if seq % self.sample_every:
                return
        else:
            with self._lock:
                rcvd_count = self.rcvd
            if rcvd_count % self.sample_every:
                return
        self._log_event({"event": kind, "seq": seq, "t_ns": t_ns})

    def run(self, duration_s: float, rate_pps: int, max_packets: Optional[int] = None) -> None:
        if self._closed:
            raise RuntimeError("Blaster is closed")
        if self._run_active.is_set():
            raise RuntimeError("Blaster.run is already in progress")

        stop_at = self._now() + int(max(0.0, duration_s) * 1e9)
        payload_pad = b"\x00" * (self.payload_bytes - 12)
        interval_ns = 0 if rate_pps <= 0 else max(1, int(round(1_000_000_000 / max(1, rate_pps))))

        stop_event = threading.Event()
        self._stop_event = stop_event
        self._run_active.set()
        rx_thread = threading.Thread(target=self._rx_loop, args=(stop_event,), daemon=True)
        self._rx_thread = rx_thread
        rx_thread.start()

        with self._lock:
            self.pending.clear()

        seq = 0
        burst = 32 if interval_ns == 0 else 1
        next_send_target = time.perf_counter_ns()

        try:
            with _windows_timer_resolution():
                while self._now() < stop_at:
                    if max_packets is not None:
                        with self._lock:
                            if self.sent >= max_packets:
                                break
                    loop_progress = False
                    sends_this_loop = burst
                    while sends_this_loop > 0:
                        if interval_ns > 0:
                            _precise_sleep_until(next_send_target)
                        now_ns = self._now()
                        if now_ns >= stop_at:
                            break
                        packet = seq.to_bytes(4, "big") + int(now_ns).to_bytes(8, "big") + payload_pad
                        try:
                            self.tx.sendto(packet, self.send_addr)
                        except Exception as exc:  # pragma: no cover - hard to surface in tests
                            self._log_event({"event": "send_error", "err": str(exc), "seq": seq, "ts": ts()})
                            break
                        t_send_int = int(now_ns)
                        with self._lock:
                            if self.sample_every and (seq % self.sample_every == 0):
                                self.pending[seq] = t_send_int
                            self.sent += 1
                            self.sent_bytes += len(packet)
                        loop_progress = True
                        self._maybe_log("send", seq, t_send_int)
                        seq += 1
                        sends_this_loop -= 1
                        if interval_ns > 0:
                            next_send_target += interval_ns
                            current_perf = time.perf_counter_ns()
                            if next_send_target < current_perf - interval_ns:
                                next_send_target = current_perf
                        if max_packets is not None:
                            with self._lock:
                                if self.sent >= max_packets:
                                    break
                    if interval_ns == 0 and (seq & 0x3FFF) == 0:
                        time.sleep(0)
                    if not loop_progress:
                        time.sleep(0.0005)

                tail_deadline = self._now() + int(0.25 * 1e9)
                while self._now() < tail_deadline:
                    time.sleep(0.0005)
        finally:
            stop_event.set()
            rx_thread.join(timeout=0.5)
            self._run_active.clear()
            self._rx_thread = None
            self._stop_event = None
            self.owd_p50_ns = self.owd_p50.value()
            self.owd_p95_ns = self.owd_p95.value()
            self.rtt_p50_ns = self.rtt_p50.value()
            self.rtt_p95_ns = self.rtt_p95.value()
            self._cleanup()
        _close_socket(self.tx)
        _close_socket(self.rx)

    def _rx_loop(self, stop_event: threading.Event) -> None:
        while not stop_event.is_set():
            if not self._run_active.is_set():
                break
            progressed = False
            for _ in range(self.rx_burst):
                if self._rx_once():
                    progressed = True
                else:
                    break
            if not progressed:
                time.sleep(0.0005)

    def _rx_once(self) -> bool:
        try:
            data, _ = self.rx.recvfrom(65535)
        except socket.timeout:
            return False
        except (socket.error, OSError) as exc:
            # Only log unexpected socket failures
            if not isinstance(exc, (ConnectionResetError, ConnectionRefusedError)):
                self._log_event({"event": "rx_error", "err": str(exc), "ts": ts()})
            return False

        t_recv = self._now()
        data_len = len(data)
        if data_len < 4:
            with self._lock:
                self.rcvd += 1
                self.rcvd_bytes += data_len
                self.truncated += 1
            return True

        seq = int.from_bytes(data[:4], "big")
        header_t_send = int.from_bytes(data[4:12], "big") if data_len >= 12 else None
        drone_recv_ns = int.from_bytes(data[-8:], "big") if data_len >= 20 else None

        log_recv = False
        with self._lock:
            self.rcvd += 1
            self.rcvd_bytes += data_len
            t_send = self.pending.pop(seq, None)
            if t_send is None:
                t_send = header_t_send

            if t_send is not None:
                rtt = t_recv - t_send
                if rtt >= 0:
                    self.rtt_sum_ns += rtt
                    self.rtt_samples += 1
                    if rtt > self.rtt_max_ns:
                        self.rtt_max_ns = rtt
                    if self.rtt_min_ns is None or rtt < self.rtt_min_ns:
                        self.rtt_min_ns = rtt
                    self.rtt_p50.add(rtt)
                    self.rtt_p95.add(rtt)
                    log_recv = True

            if t_send is not None and drone_recv_ns is not None:
                owd_up_ns = drone_recv_ns - t_send
                if 0 <= owd_up_ns <= 5_000_000_000:
                    self.owd_samples += 1
                    self.owd_p50.add(owd_up_ns)
                    self.owd_p95.add(owd_up_ns)
            if data_len < 20:
                self.truncated += 1

        if log_recv:
            self._maybe_log("recv", seq, int(t_recv))
        return True

    def _cleanup(self) -> None:
        if self.events:
            try:
                self.events.flush()
                self.events.close()
            except Exception:
                pass
            self.events = None


def wait_handshake(timeout: float = 20.0) -> bool:
    deadline = time.time() + timeout
    while time.time() < deadline:
        if PROXY_STATUS_PATH.exists():
            try:
                with open(PROXY_STATUS_PATH, encoding="utf-8") as handle:
                    js = json.load(handle)
            except Exception:
                js = {}
            state = js.get("state") or js.get("status")
            if state in {"running", "completed", "ready", "handshake_ok"}:
                return True
        time.sleep(0.3)
    return False


def wait_active_suite(target: str, timeout: float = 10.0) -> bool:
    return wait_rekey_transition(target, timeout=timeout)


def run_suite_connectivity_only(
    suite: str,
    *,
    duration_s: float = 2.0,
    pre_gap_s: float = 0.5,
) -> Dict[str, object]:
    """Minimal end-to-end connectivity check for a single suite.

    This helper reuses the existing proxy status path and handshake wait logic
    but skips high-rate benchmarking, power capture, blackout metrics, and
    artifact fetching. It is intended for lightweight correctness validation
    when AUTO_GCS.mode == "test_only".
    """

    start_ts = ts()
    handshake_ok = wait_handshake(timeout=REKEY_WAIT_TIMEOUT_SECONDS)
    connectivity_ok = False
    reason_parts: list[str] = []

    if not handshake_ok:
        reason_parts.append("handshake_fail")
    else:
        try:
            # Best-effort one-shot Blaster run with very low rate and sampling
            blaster = Blaster(
                APP_SEND_HOST,
                APP_SEND_PORT,
                APP_RECV_HOST,
                APP_RECV_PORT,
                events_path=None,
                payload_bytes=int(AUTO_GCS_CONFIG.get("payload_bytes", 64) or 64),
                sample_every=0,
                offset_ns=0,
            )
            try:
                blaster.run(duration_s=max(0.1, float(duration_s)), rate_pps=10, max_packets=50)
                connectivity_ok = blaster.rcvd > 0
            finally:
                # Blaster.run() already closes its sockets, but call cleanup
                # to ensure events file handles (if any) are closed.
                blaster._cleanup()
        except Exception as exc:  # pragma: no cover - defensive path
            reason_parts.append(f"blaster_error:{exc}")

    if connectivity_ok:
        outcome = "ok"
    elif not handshake_ok:
        outcome = "handshake_fail"
    else:
        outcome = "connectivity_fail"

    return {
        "timestamp_utc": start_ts,
        "suite": suite,
        "mode": AUTO_GCS_MODE,
        "handshake_ok": handshake_ok,
        "connectivity_ok": connectivity_ok,
        "outcome": outcome,
        "reason": ";".join(reason_parts) if reason_parts else "",
    }


def wait_pending_suite(target: str, timeout: float = 18.0, stable_checks: int = 2) -> bool:
    deadline = time.time() + timeout
    stable = 0
    while time.time() < deadline:
        try:
            status = ctl_send({"cmd": "status"}, timeout=0.6, retries=1)
        except Exception:
            status = {}
        pending = status.get("pending_suite")
        suite = status.get("suite")
        if pending == target:
            stable += 1
            if stable >= stable_checks:
                return True
        elif suite == target and pending in (None, "", target):
            # Rekey may have already completed; treat as success.
            return True
        else:
            stable = 0
        time.sleep(0.2)
    return False


def wait_rekey_transition(target: str, timeout: float = 20.0, stable_checks: int = 3) -> bool:
    deadline = time.time() + timeout
    last_status: dict = {}
    stable = 0
    while time.time() < deadline:
        try:
            status = ctl_send({"cmd": "status"}, timeout=0.6, retries=1)
        except Exception:
            status = {}
        last_status = status
        suite = status.get("suite")
        pending = status.get("pending_suite")
        last_requested = status.get("last_requested_suite")
        if suite == target and (pending in (None, "", target)):
            stable += 1
            if stable >= stable_checks:
                if last_requested and last_requested not in (suite, target):
                    print(
                        f"[{ts()}] follower reports suite={suite} but last_requested={last_requested}; continuing anyway",
                        file=sys.stderr,
                    )
                return True
        else:
            stable = 0
        time.sleep(0.2)
    if last_status:
        print(
            f"[{ts()}] follower status before timeout: suite={last_status.get('suite')} pending={last_status.get('pending_suite')}",
            file=sys.stderr,
        )
    return False


def timesync() -> dict:
    t1 = time.time_ns()
    resp = ctl_send({"cmd": "timesync", "t1_ns": t1})
    t4 = time.time_ns()
    t2 = int(resp.get("t2_ns", t1))
    t3 = int(resp.get("t3_ns", t4))
    delay_ns = (t4 - t1) - (t3 - t2)
    offset_ns = ((t2 - t1) + (t3 - t4)) // 2
    return {"offset_ns": offset_ns, "rtt_ns": delay_ns}


def snapshot_proxy_artifacts(suite: str) -> None:
    target_dir = suite_outdir(suite)
    if PROXY_STATUS_PATH.exists():
        _robust_copy(PROXY_STATUS_PATH, target_dir / "gcs_status.json")
    if PROXY_SUMMARY_PATH.exists():
        _robust_copy(PROXY_SUMMARY_PATH, target_dir / "gcs_summary.json")


def start_gcs_proxy(initial_suite: str) -> tuple[subprocess.Popen, IO[str], Path]:
    key_path = SECRETS_DIR / initial_suite / "gcs_signing.key"
    if not key_path.exists():
        raise FileNotFoundError(f"Missing GCS signing key for suite {initial_suite}: {key_path}")

    mkdirp(OUTDIR)
    log_path = OUTDIR / f"gcs_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_handle: IO[str] = open(log_path, "w", encoding="utf-8", errors="replace")

    env = os.environ.copy()
    env["DRONE_HOST"] = DRONE_HOST
    env["GCS_HOST"] = GCS_HOST
    env["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    env["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"

    root_str = str(ROOT)
    existing_py_path = env.get("PYTHONPATH")
    if existing_py_path:
        if root_str not in existing_py_path.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing_py_path
    else:
        env["PYTHONPATH"] = root_str

    proc = subprocess.Popen(
        [
            sys.executable,
            "-m",
            "core.run_proxy",
            "gcs",
            "--suite",
            initial_suite,
            "--gcs-secret-file",
            str(key_path),
            "--control-manual",
            "--status-file",
            str(PROXY_STATUS_PATH),
            "--json-out",
            str(PROXY_SUMMARY_PATH),
        ],
        stdin=subprocess.PIPE,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        text=True,
        bufsize=1,
        env=env,
        cwd=str(ROOT),
    )
    return proc, log_handle, log_path


def read_proxy_stats_live() -> dict:
    try:
        with open(PROXY_STATUS_PATH, encoding="utf-8") as handle:
            js = json.load(handle)
    except Exception:
        return {}
    if isinstance(js, dict):
        counters = js.get("counters")
        if isinstance(counters, dict):
            return counters
        if any(k in js for k in ("enc_out", "enc_in")):
            return js
    return {}


def read_proxy_summary() -> dict:
    if not PROXY_SUMMARY_PATH.exists():
        return {}
    try:
        with open(PROXY_SUMMARY_PATH, encoding="utf-8") as handle:
            return json.load(handle)
    except Exception:
        return {}



def _read_proxy_counters() -> dict:

    counters = read_proxy_stats_live()

    if isinstance(counters, dict) and counters:

        return counters

    summary = read_proxy_summary()

    if isinstance(summary, dict):

        summary_counters = summary.get("counters")

        if isinstance(summary_counters, dict):

            return summary_counters

        if any(key in summary for key in ("enc_out", "enc_in", "rekeys_ok", "rekeys_fail", "last_rekey_suite")):

            return summary

    return {}


def _tail_file_lines(path: Path, limit: int = FAILURE_LOG_TAIL_LINES) -> List[str]:
    limit = max(1, min(int(limit), 500))
    try:
        with open(path, encoding="utf-8", errors="replace") as handle:
            lines = list(deque(handle, maxlen=limit))
    except FileNotFoundError:
        return []
    except OSError:
        return []
    return [line.rstrip("\n") for line in lines]


def dump_failure_diagnostics(
    suite: str,
    reason: str,
    *,
    gcs_log_handle: Optional[IO[str]] = None,
    gcs_log_path: Optional[Path] = None,
    tail_lines: int = FAILURE_LOG_TAIL_LINES,
) -> None:
    banner = f"[{ts()}] diagnostics for suite {suite}: {reason}"
    print(banner, flush=True)
    if gcs_log_handle:
        try:
            gcs_log_handle.flush()
        except Exception:
            pass
    if gcs_log_path:
        gcs_tail = _tail_file_lines(gcs_log_path, tail_lines)
        if gcs_tail:
            print(f"[{ts()}] --- GCS proxy log tail ({gcs_log_path}) ---", flush=True)
            for line in gcs_tail:
                print(line, flush=True)
        else:
            print(f"[WARN] gcs log tail unavailable at {gcs_log_path}", file=sys.stderr)
    else:
        print("[WARN] gcs log path unavailable for diagnostics", file=sys.stderr)
    try:
        resp = ctl_send({"cmd": "log_tail", "lines": tail_lines}, timeout=1.5, retries=1)
    except Exception as exc:
        print(f"[WARN] follower log tail request failed: {exc}", file=sys.stderr)
        return
    follower_lines = resp.get("lines")
    follower_path = resp.get("path") or "remote"
    if isinstance(follower_lines, list) and follower_lines:
        print(f"[{ts()}] --- follower log tail ({follower_path}) ---", flush=True)
        for line in follower_lines:
            print(str(line), flush=True)
    else:
        print(f"[WARN] follower log tail empty ({follower_path})", file=sys.stderr)





def wait_proxy_rekey(
    target_suite: str,
    baseline: Dict[str, object],
    *,
    timeout: float = 20.0,
    poll_interval: float = 0.4,
    proc: subprocess.Popen,
) -> str:
    start = time.time()

    baseline_ok = int(baseline.get("rekeys_ok", 0) or 0)
    baseline_fail = int(baseline.get("rekeys_fail", 0) or 0)

    while time.time() - start < timeout:
        if proc.poll() is not None:
            raise RuntimeError("GCS proxy exited during rekey")

        counters = _read_proxy_counters()

        if counters:
            rekeys_ok = int(counters.get("rekeys_ok", 0) or 0)
            rekeys_fail = int(counters.get("rekeys_fail", 0) or 0)
            last_suite = counters.get("last_rekey_suite") or counters.get("suite") or ""

            if rekeys_fail > baseline_fail:
                return "fail"

            if rekeys_ok > baseline_ok and (not last_suite or last_suite == target_suite):
                return "ok"

        time.sleep(poll_interval)

    return "timeout"


def _extract_companion_metrics(
    samples: List[dict],
    *,
    suite: str,
    start_ns: int,
    end_ns: int,
) -> Dict[str, object]:
    cpu_max = 0.0
    rss_max_bytes = 0
    pfc_sum = 0.0
    vh_sum = 0.0
    vv_sum = 0.0
    kin_count = 0

    for sample in samples:
        try:
            ts_ns = int(sample.get("timestamp_ns"))
        except (TypeError, ValueError):
            continue
        if ts_ns < start_ns or ts_ns > end_ns:
            continue
        sample_suite = str(sample.get("suite") or "").strip()
        if sample_suite and sample_suite != suite:
            continue

        kind = str(sample.get("kind") or "").lower()
        if kind == "system_sample":
            cpu_val = _as_float(sample.get("cpu_percent"))
            if cpu_val is not None:
                cpu_max = max(cpu_max, cpu_val)
            mem_mb = _as_float(sample.get("mem_used_mb"))
            if mem_mb is not None:
                rss_candidate = int(mem_mb * 1024 * 1024)
                rss_max_bytes = max(rss_max_bytes, rss_candidate)
        elif kind == "psutil_sample":
            cpu_val = _as_float(sample.get("cpu_percent"))
            if cpu_val is not None:
                cpu_max = max(cpu_max, cpu_val)
            rss_val = _as_float(sample.get("rss_bytes"))
            if rss_val is not None:
                rss_candidate = int(rss_val)
                rss_max_bytes = max(rss_max_bytes, rss_candidate)
        elif kind == "kinematics":
            pfc_val = _as_float(sample.get("predicted_flight_constraint_w"))
            vh_val = _as_float(sample.get("velocity_horizontal_mps"))
            vv_val = _as_float(sample.get("velocity_vertical_mps"))
            if pfc_val is not None:
                pfc_sum += pfc_val
            if vh_val is not None:
                vh_sum += vh_val
            if vv_val is not None:
                vv_sum += vv_val
            kin_count += 1

    avg_vh = vh_sum / kin_count if kin_count else 0.0
    avg_vv = vv_sum / kin_count if kin_count else 0.0
    avg_pfc = pfc_sum / kin_count if kin_count else 0.0

    return {
        "cpu_max_percent": round(cpu_max, 3),
        "max_rss_bytes": int(max(0, rss_max_bytes)),
        "pfc_watts": round(avg_pfc, 3),
        "kinematics_vh": round(avg_vh, 3),
        "kinematics_vv": round(avg_vv, 3),
    }


def activate_suite(
    gcs: subprocess.Popen,
    suite: str,
    is_first: bool,
    *,
    gcs_log_handle: Optional[IO[str]] = None,
    gcs_log_path: Optional[Path] = None,
    failure_tail_lines: int = FAILURE_LOG_TAIL_LINES,
) -> Tuple[float, Optional[int], Optional[int]]:

    if gcs.poll() is not None:

        raise RuntimeError("GCS proxy is not running; cannot continue")

    start_ns = time.time_ns()
    mark_ns: Optional[int] = None
    rekey_complete_ns: Optional[int] = None
    rekey_status = "ok" if is_first else "pending"

    if is_first:
        mark_ns = None
        rekey_complete_ns = None

        if not wait_rekey_transition(suite, timeout=12.0):
            raise RuntimeError(f"Follower did not confirm initial suite {suite}")

    else:
        _ensure_suite_supported_remote(suite, stage="pre_rekey")

        assert gcs.stdin is not None

        try:
            status_snapshot = ctl_send({"cmd": "status"}, timeout=0.6, retries=1)
        except Exception:
            status_snapshot = {}
        previous_suite = status_snapshot.get("suite")

        print(f"[{ts()}] rekey -> {suite}")

        gcs.stdin.write(suite + "\n")
        gcs.stdin.flush()

        baseline = _read_proxy_counters()

        mark_ns = time.time_ns()
        pending_ack = False
        pending_ack_error: Optional[str] = None
        try:
            mark_resp = ctl_send({"cmd": "mark", "suite": suite, "kind": "rekey"})
            if not mark_resp.get("ok"):
                mark_error = str(mark_resp.get("error") or "mark_failed")
                pending_ack_error = mark_error
                print(f"[WARN] follower mark rejected for {suite}: {mark_error}", file=sys.stderr)
        except Exception as exc:
            pending_ack_error = str(exc)
            print(f"[WARN] control mark failed for {suite}: {exc}", file=sys.stderr)
        try:
            pending_ack = wait_pending_suite(suite, timeout=12.0)
        except Exception as exc:
            pending_ack_error = str(exc)

        rekey_status = "timeout"
        diagnostics_emitted = False

        try:

            result = wait_proxy_rekey(suite, baseline, timeout=REKEY_WAIT_TIMEOUT_SECONDS, proc=gcs)

            rekey_status = result

            if result == "timeout":

                print(f"[WARN] timed out waiting for proxy to activate suite {suite}", file=sys.stderr)

            elif result == "fail":

                print(f"[WARN] proxy reported failed rekey for suite {suite}", file=sys.stderr)

        except RuntimeError as exc:
            rekey_status = "error"
            dump_failure_diagnostics(
                suite,
                f"proxy exited during rekey: {exc}",
                gcs_log_handle=gcs_log_handle,
                gcs_log_path=gcs_log_path,
                tail_lines=failure_tail_lines,
            )
            diagnostics_emitted = True
            raise
        except Exception as exc:
            rekey_status = "error"
            print(f"[WARN] error while waiting for proxy rekey {suite}: {exc}", file=sys.stderr)
            dump_failure_diagnostics(
                suite,
                f"exception while waiting for proxy rekey: {exc}",
                gcs_log_handle=gcs_log_handle,
                gcs_log_path=gcs_log_path,
                tail_lines=failure_tail_lines,
            )
            diagnostics_emitted = True
        finally:
            try:
                rekey_complete_ns = time.time_ns()
                ctl_send({"cmd": "rekey_complete", "suite": suite, "status": rekey_status})
            except Exception as exc:
                print(f"[WARN] rekey_complete failed for {suite}: {exc}", file=sys.stderr)

        if rekey_status != "ok":
            if not pending_ack and pending_ack_error:
                print(
                    f"[WARN] follower pending status check failed for suite {suite}: {pending_ack_error}",
                    file=sys.stderr,
                )
            elif not pending_ack:
                print(
                    f"[WARN] follower did not acknowledge pending suite {suite} before proxy reported {rekey_status}",
                    file=sys.stderr,
                )
            if not previous_suite:
                raise RuntimeError(f"Proxy rekey to {suite} reported {rekey_status}; previous suite unknown")
            expected_suite = previous_suite
            # Attempt an explicit rollback command to ensure follower clears any dangling pending state.
            try:
                rb = ctl_send({"cmd": "rollback"}, timeout=0.8, retries=1)
                if not rb.get("ok"):
                    print(f"[WARN] rollback command returned error for suite {suite}: {rb}", file=sys.stderr)
            except Exception as exc:
                print(f"[WARN] rollback command failed for suite {suite}: {exc}", file=sys.stderr)
        else:
            expected_suite = suite

        transition_ok = wait_rekey_transition(expected_suite, timeout=REKEY_WAIT_TIMEOUT_SECONDS)

        elapsed_ms = (time.time_ns() - start_ns) / 1_000_000
        elapsed_s = elapsed_ms / 1000.0

        if not transition_ok:
            if not diagnostics_emitted:
                reason = (
                    f"timeout waiting for follower to report suite {expected_suite} after status {rekey_status}"
                )
                dump_failure_diagnostics(
                    suite,
                    reason,
                    gcs_log_handle=gcs_log_handle,
                    gcs_log_path=gcs_log_path,
                    tail_lines=failure_tail_lines,
                )
                diagnostics_emitted = True
            if rekey_status == "timeout" and elapsed_s >= REKEY_SKIP_THRESHOLD_SECONDS:
                raise SuiteSkipped(
                    suite,
                    f"rekey confirmation exceeded {REKEY_SKIP_THRESHOLD_SECONDS:.2f}s limit",
                    elapsed_s=elapsed_s,
                )
            raise RuntimeError(
                f"Follower did not confirm suite {expected_suite} after rekey status {rekey_status}"
            )

        if rekey_status != "ok":
            if not diagnostics_emitted:
                reason = f"proxy reported rekey status {rekey_status}"
                dump_failure_diagnostics(
                    suite,
                    reason,
                    gcs_log_handle=gcs_log_handle,
                    gcs_log_path=gcs_log_path,
                    tail_lines=failure_tail_lines,
                )
                diagnostics_emitted = True
            if rekey_status == "timeout" and elapsed_s >= REKEY_SKIP_THRESHOLD_SECONDS:
                raise SuiteSkipped(
                    suite,
                    f"rekey exceeded {REKEY_SKIP_THRESHOLD_SECONDS:.2f}s limit",
                    elapsed_s=elapsed_s,
                )
            raise RuntimeError(f"Proxy reported rekey status {rekey_status} for suite {suite}")
    elapsed_ms = (time.time_ns() - start_ns) / 1_000_000

    if REKEY_SETTLE_SECONDS > 0:
        time.sleep(REKEY_SETTLE_SECONDS)

    return elapsed_ms, mark_ns, rekey_complete_ns




def run_suite(
    gcs: subprocess.Popen,
    suite: str,
    is_first: bool,
    duration_s: float,
    payload_bytes: int,
    event_sample: int,
    offset_ns: int,
    pass_index: int,
    traffic_mode: str,
    traffic_engine: str,
    iperf3_config: Dict[str, Any],
    pre_gap: float,
    inter_gap_s: float,
    rate_pps: int,
    target_bandwidth_mbps: float,
    power_capture_enabled: bool,
    clock_offset_warmup_s: float,
    min_delay_samples: int,
    telemetry_collector: Optional["TelemetryCollector"] = None,
    gcs_log_handle: Optional[IO[str]] = None,
    gcs_log_path: Optional[Path] = None,
) -> dict:
    # Preflight identity check: skip suites missing GCS signing secret to avoid noisy rekey failures
    ident_dir = Path("secrets") / "matrix" / suite
    gcs_secret_key = ident_dir / "gcs_signing.key"
    gcs_secret_sec = ident_dir / "gcs_signing.sec"
    if not (gcs_secret_key.exists() or gcs_secret_sec.exists()):
        _log_event({
            "suite": suite,
            "phase": "skipped",
            "skip_reason": "missing_gcs_signing_key",
            "pass_index": pass_index,
        })
        _append_suite_text(suite, f"[{ts()}] SKIP suite={suite} reason=missing_gcs_signing_key paths={gcs_secret_key},{gcs_secret_sec}")
        return {"suite": suite, "skipped": True, "skip_reason": "missing_gcs_signing_key"}
    # Suite start event
    _log_event({
        "suite": suite,
        "phase": "start",
        "pass_index": pass_index,
        "is_first": bool(is_first),
        "traffic_mode": traffic_mode,
        "payload_bytes": payload_bytes,
        "duration_s": duration_s,
    })
    _append_suite_text(suite, f"[{ts()}] START suite={suite} pass={pass_index} mode={traffic_mode} duration={duration_s:.2f}s")
    try:
        rekey_duration_ms, rekey_mark_ns, rekey_complete_ns = activate_suite(
            gcs,
            suite,
            is_first,
            gcs_log_handle=gcs_log_handle,
            gcs_log_path=gcs_log_path,
        )
    except Exception as exc:
        _log_event({
            "suite": suite,
            "phase": "activate_failure",
            "error": str(exc),
            "pass_index": pass_index,
        })
        _append_suite_text(suite, f"[{ts()}] ACTIVATE_FAILURE suite={suite} error={exc}")
        return {"suite": suite, "failed": True, "error_phase": "activate", "error": str(exc)}
    _log_event({
        "suite": suite,
        "phase": "rekey_complete",
        "rekey_ms": rekey_duration_ms,
        "rekey_mark_ns": rekey_mark_ns,
        "rekey_ok_ns": rekey_complete_ns,
    })
    _append_suite_text(suite, f"[{ts()}] REKEY_COMPLETE suite={suite} rekey_ms={rekey_duration_ms:.2f}")

    effective_sample_every, effective_min_delay = _compute_sampling_params(
        duration_s,
        event_sample,
        min_delay_samples,
    )

    suite_dir = suite_outdir(suite)
    engine_kind = str(traffic_engine or "native").lower()
    use_iperf3 = traffic_mode in {"blast", "constant"} and engine_kind == "iperf3"
    iperf3_cfg = iperf3_config if isinstance(iperf3_config, dict) else {}
    iperf3_server_host = str(iperf3_cfg.get("server_host") or APP_SEND_HOST)
    iperf3_server_port = int(iperf3_cfg.get("server_port") or APP_SEND_PORT)
    iperf3_binary = str(iperf3_cfg.get("binary") or "iperf3")
    extra_args_cfg = iperf3_cfg.get("extra_args")
    if isinstance(extra_args_cfg, (list, tuple)):
        iperf3_extra_args = [str(arg) for arg in extra_args_cfg]
    elif extra_args_cfg is None:
        iperf3_extra_args = []
    else:
        iperf3_extra_args = [str(extra_args_cfg)]

    derived_bandwidth = target_bandwidth_mbps if target_bandwidth_mbps > 0 else 0.0
    if derived_bandwidth <= 0 and rate_pps > 0:
        derived_bandwidth = (rate_pps * payload_bytes * 8) / 1_000_000

    if use_iperf3 and derived_bandwidth <= 0:
        print(
            f"[WARN] iperf3 engine requires bandwidth target; falling back to native blaster for suite {suite}",
            file=sys.stderr,
        )
        use_iperf3 = False

    traffic_engine_resolved = "iperf3" if use_iperf3 else "native"
    if use_iperf3:
        effective_sample_every = 0
        effective_min_delay = 0

    events_path = None if use_iperf3 else suite_dir / EVENTS_FILENAME
    start_mark_ns = time.time_ns() + offset_ns + int(0.150 * 1e9) + int(max(pre_gap, 0.0) * 1e9)
    try:
        ctl_send(
            {
                "cmd": "schedule_mark",
                "suite": suite,
                "t0_ns": start_mark_ns,
                "kind": "window",
            }
        )
    except Exception as exc:
        print(f"[WARN] schedule_mark failed for {suite}: {exc}", file=sys.stderr)

    power_request_ok = False
    power_request_error: Optional[str] = None
    power_status: Dict[str, Any] = {}
    power_note = ""
    if power_capture_enabled:
        power_start_ns = time.time_ns() + offset_ns + int(max(pre_gap, 0.0) * 1e9)
        power_resp = request_power_capture(suite, duration_s, power_start_ns)
        power_request_ok = bool(power_resp.get("ok"))
        power_request_error = power_resp.get("error") if not power_request_ok else None
        if not power_request_ok and power_request_error:
            print(f"[WARN] power capture not scheduled: {power_request_error}", file=sys.stderr)
        banner = f"[{ts()}] ===== POWER: START in {pre_gap:.1f}s | suite={suite} | duration={duration_s:.1f}s mode={traffic_mode} ====="
    else:
        banner = (
            f"[{ts()}] ===== TRAFFIC: START in {pre_gap:.1f}s | suite={suite} | duration={duration_s:.1f}s "
            f"mode={traffic_mode} (power capture disabled) ====="
        )

    print(banner)
    if pre_gap > 0:
        time.sleep(pre_gap)

    warmup_s = max(clock_offset_warmup_s, min(MAX_WARMUP_SECONDS, duration_s * WARMUP_FRACTION))
    start_wall_ns = time.time_ns()
    start_perf_ns = time.perf_counter_ns()
    sent_packets = 0
    rcvd_packets = 0
    rcvd_bytes = 0
    avg_rtt_ns = 0
    max_rtt_ns = 0
    rtt_samples = 0
    blaster_sent_bytes = 0
    blaster: Optional[Blaster] = None
    iperf3_result: Dict[str, Any] = {}
    iperf3_jitter_ms: Optional[float] = None
    iperf3_lost_pct: Optional[float] = None
    iperf3_lost_packets: Optional[int] = None
    iperf3_report_path: Optional[str] = None

    wire_header_bytes = UDP_HEADER_BYTES + APP_IP_HEADER_BYTES

    if traffic_mode in {"blast", "constant"}:
        if use_iperf3:
            start_wall_ns = time.time_ns()
            start_perf_ns = time.perf_counter_ns()
            iperf3_result = _run_iperf3_client(
                suite,
                duration_s=duration_s,
                bandwidth_mbps=derived_bandwidth,
                payload_bytes=payload_bytes,
                server_host=iperf3_server_host,
                server_port=iperf3_server_port,
                binary=iperf3_binary,
                extra_args=iperf3_extra_args,
            )
            sent_packets = iperf3_result.get("sent_packets", 0)
            rcvd_packets = iperf3_result.get("rcvd_packets", 0)
            rcvd_bytes = iperf3_result.get("rcvd_bytes", 0)
            blaster_sent_bytes = iperf3_result.get("sent_bytes", 0)
            iperf3_jitter_ms = iperf3_result.get("jitter_ms")
            iperf3_lost_pct = iperf3_result.get("lost_percent")
            iperf3_lost_packets = iperf3_result.get("lost_packets")
            if iperf3_lost_packets is not None:
                try:
                    iperf3_lost_packets = int(iperf3_lost_packets)
                except (TypeError, ValueError):
                    iperf3_lost_packets = None
            raw_report = iperf3_result.get("raw_report")
            if isinstance(raw_report, dict):
                report_bytes = json.dumps(raw_report, indent=2).encode("utf-8")
                report_path = suite_dir / "iperf3_report.json"
                try:
                    _atomic_write_bytes(report_path, report_bytes)
                    iperf3_report_path = str(report_path)
                except Exception as exc:
                    print(f"[WARN] failed to persist iperf3 report for {suite}: {exc}", file=sys.stderr)
        else:
            if warmup_s > 0:
                warmup_blaster = Blaster(
                    APP_SEND_HOST,
                    APP_SEND_PORT,
                    APP_RECV_HOST,
                    APP_RECV_PORT,
                    events_path=None,
                    payload_bytes=payload_bytes,
                    sample_every=0,
                    offset_ns=offset_ns,
                )
                warmup_blaster.run(duration_s=warmup_s, rate_pps=rate_pps)
            start_wall_ns = time.time_ns()
            start_perf_ns = time.perf_counter_ns()
            blaster = Blaster(
                APP_SEND_HOST,
                APP_SEND_PORT,
                APP_RECV_HOST,
                APP_RECV_PORT,
                events_path,
                payload_bytes=payload_bytes,
                sample_every=effective_sample_every if effective_sample_every > 0 else 0,
                offset_ns=offset_ns,
            )
            blaster.run(duration_s=duration_s, rate_pps=rate_pps)
            sent_packets = blaster.sent
            rcvd_packets = blaster.rcvd
            rcvd_bytes = blaster.rcvd_bytes
            blaster_sent_bytes = blaster.sent_bytes
            wire_header_bytes = getattr(blaster, "wire_header_bytes", wire_header_bytes)
            sample_count = max(1, blaster.rtt_samples)
            avg_rtt_ns = blaster.rtt_sum_ns // sample_count
            max_rtt_ns = blaster.rtt_max_ns
            rtt_samples = blaster.rtt_samples
        if use_iperf3 and rcvd_bytes == 0 and iperf3_result.get("throughput_bps"):
            throughput_bps = float(iperf3_result["throughput_bps"])
            rcvd_bytes = int(throughput_bps * duration_s / 8)
        if use_iperf3 and blaster_sent_bytes == 0 and sent_packets > 0:
            blaster_sent_bytes = sent_packets * payload_bytes
    else:
        time.sleep(duration_s)

    end_wall_ns = time.time_ns()
    end_perf_ns = time.perf_counter_ns()
    if power_capture_enabled:
        print(f"[{ts()}] ===== POWER: STOP | suite={suite} =====")
    else:
        print(f"[{ts()}] ===== TRAFFIC: STOP | suite={suite} =====")

    snapshot_proxy_artifacts(suite)
    proxy_stats = read_proxy_stats_live() or read_proxy_summary()
    if not isinstance(proxy_stats, dict):
        proxy_stats = {}
    handshake_metrics_payload: Dict[str, object] = {}
    if isinstance(proxy_stats, dict):
        handshake_metrics_payload = proxy_stats.get("handshake_metrics") or {}
        if not isinstance(handshake_metrics_payload, dict):
            handshake_metrics_payload = {}
    handshake_fields = _flatten_handshake_metrics(handshake_metrics_payload)

    if power_capture_enabled and power_request_ok:
        power_status = poll_power_status(max_wait_s=max(6.0, duration_s * 0.25))
        if power_status.get("error"):
            print(f"[WARN] power status error: {power_status['error']}", file=sys.stderr)
        if power_status.get("busy"):
            power_status.setdefault("error", "capture_incomplete")

    power_summary = power_status.get("last_summary") if isinstance(power_status, dict) else None
    status_for_extract: Dict[str, Any] = {}
    if isinstance(power_status, dict) and power_status:
        status_for_extract = power_status
    elif power_summary:
        status_for_extract = {"last_summary": power_summary}
    power_fields = extract_power_fields(status_for_extract) if status_for_extract else {}
    power_capture_complete = bool(power_summary)
    power_error = None
    if not power_capture_complete:
        if isinstance(power_status, dict):
            power_error = power_status.get("error")
            if not power_error and power_status.get("busy"):
                power_error = "capture_incomplete"
        if power_error is None:
            power_error = power_request_error

    monitor_payload: Dict[str, object] = {}
    if isinstance(power_status, dict) and power_status:
        monitor_payload = dict(power_status)
    elif isinstance(power_summary, dict):
        monitor_payload = {
            "monitor_manifest_path": power_summary.get("monitor_manifest_path"),
            "telemetry_status_path": power_summary.get("telemetry_status_path"),
            "session_dir": power_summary.get("session_dir"),
        }

    monitor_fetch_info = _fetch_monitor_artifacts(suite, monitor_payload) if not use_iperf3 else {
        "status": "external",
        "error": "traffic_engine=iperf3",
    }
    monitor_manifest_local = monitor_fetch_info.get("manifest_path")
    telemetry_status_local = monitor_fetch_info.get("telemetry_status_path")
    monitor_artifact_paths: List[Path] = list(monitor_fetch_info.get("artifact_paths") or [])
    raw_categorized = monitor_fetch_info.get("categorized_paths")
    monitor_categorized_paths: Dict[str, List[Path]] = {}
    if isinstance(raw_categorized, dict):
        for key, values in raw_categorized.items():
            category = str(key)
            bucket: List[Path] = []
            if isinstance(values, Iterable):
                for item in values:
                    try:
                        bucket.append(Path(item))
                    except Exception:
                        continue
            if bucket:
                monitor_categorized_paths[category] = bucket
    raw_remote_map = monitor_fetch_info.get("remote_map")
    monitor_remote_map: Dict[str, str] = {}
    if isinstance(raw_remote_map, dict):
        for local_key, remote_val in raw_remote_map.items():
            try:
                local_str = str(Path(local_key))
            except Exception:
                local_str = str(local_key)
            monitor_remote_map[local_str] = str(remote_val)
    monitor_fetch_status = str(monitor_fetch_info.get("status") or "")
    monitor_fetch_error = str(monitor_fetch_info.get("error") or "")

    fetched_paths: Dict[str, Path] = {}
    fetch_error_msg: Optional[str] = None
    power_fetch_status = ""
    power_fetch_error = ""
    combined_paths: Dict[str, object] = {}
    if isinstance(power_summary, dict):
        for key in ("csv_path", "summary_json_path"):
            value = power_summary.get(key)
            if value:
                combined_paths[key] = value
    if isinstance(power_fields, dict):
        summary_candidate = power_fields.get("summary_json_path")
        if summary_candidate and "summary_json_path" not in combined_paths:
            combined_paths["summary_json_path"] = summary_candidate
    if combined_paths:
        fetched_paths, fetch_error_msg = _fetch_power_artifacts(suite, combined_paths)
        if fetched_paths and fetch_error_msg:
            power_fetch_status = "partial"
            power_fetch_error = fetch_error_msg
        elif fetched_paths:
            power_fetch_status = "ok"
        elif fetch_error_msg:
            if _errors_indicate_fetch_disabled(fetch_error_msg):
                power_fetch_status = "disabled"
            else:
                power_fetch_status = "error"
                power_fetch_error = fetch_error_msg
        else:
            power_fetch_status = "missing"
    else:
        power_fetch_status = "no_paths"

    if fetched_paths.get("csv_path") is not None:
        local_csv = fetched_paths["csv_path"]
        if isinstance(power_summary, dict):
            power_summary["csv_path"] = str(local_csv)
        if isinstance(power_fields, dict):
            power_fields["csv_path"] = str(local_csv)
    if fetched_paths.get("summary_json_path") is not None:
        local_summary = fetched_paths["summary_json_path"]
        if isinstance(power_summary, dict):
            power_summary["summary_json_path"] = str(local_summary)
        if isinstance(power_fields, dict):
            power_fields["summary_json_path"] = str(local_summary)

    if isinstance(power_fields, dict):
        if not power_fields.get("csv_path"):
            for candidate in monitor_artifact_paths:
                parts_lower = [part.lower() for part in candidate.parts]
                name_lower = candidate.name.lower()
                if candidate.suffix.lower() == ".csv" and ("power" in parts_lower or "power" in name_lower):
                    power_fields["csv_path"] = str(candidate)
                    if isinstance(power_summary, dict):
                        power_summary.setdefault("csv_path", str(candidate))
                    break
        if not power_fields.get("summary_json_path"):
            for candidate in monitor_artifact_paths:
                parts_lower = [part.lower() for part in candidate.parts]
                name_lower = candidate.name.lower()
                if candidate.suffix.lower() == ".json" and ("power" in parts_lower or "power" in name_lower):
                    power_fields["summary_json_path"] = str(candidate)
                    if isinstance(power_summary, dict):
                        power_summary.setdefault("summary_json_path", str(candidate))
                    break

    if power_fetch_status in {"error", "partial"} and power_fetch_error:
        print(
            f"[WARN] power artifact fetch failed for suite {suite}: {power_fetch_error}",
            file=sys.stderr,
        )

    if monitor_fetch_status in {"error", "partial"} and monitor_fetch_error:
        print(
            f"[WARN] monitor artifact fetch issues for suite {suite}: {monitor_fetch_error}",
            file=sys.stderr,
        )

    def _to_int_or_none(value: object) -> Optional[int]:
        try:
            return int(value)
        except (TypeError, ValueError):
            return None

    capture_start_remote = (
        _to_int_or_none(power_summary.get("start_ns")) if isinstance(power_summary, dict) else None
    )
    capture_end_remote = (
        _to_int_or_none(power_summary.get("end_ns")) if isinstance(power_summary, dict) else None
    )

    if not power_capture_enabled:
        power_note = "disabled"
    elif not power_request_ok:
        power_note = f"request_error:{power_error}" if power_error else "request_error"
    elif power_capture_complete:
        power_note = "ok"
    else:
        if isinstance(power_status, dict) and power_status.get("busy"):
            power_note = "capture_incomplete:busy"
        else:
            power_note = f"capture_incomplete:{power_error}" if power_error else "capture_incomplete"

    elapsed_s = max(1e-9, (end_perf_ns - start_perf_ns) / 1e9)
    pps = sent_packets / elapsed_s if elapsed_s > 0 else 0.0
    throughput_mbps = (rcvd_bytes * 8) / (elapsed_s * 1_000_000) if elapsed_s > 0 else 0.0
    sent_mbps = (blaster_sent_bytes * 8) / (elapsed_s * 1_000_000) if blaster_sent_bytes else 0.0
    delivered_ratio = throughput_mbps / sent_mbps if sent_mbps > 0 else 0.0
    avg_rtt_ms = avg_rtt_ns / 1_000_000
    max_rtt_ms = max_rtt_ns / 1_000_000

    timer_resolution_warning = False
    if (
        os.name == "nt"
        and traffic_engine_resolved == "native"
        and rate_pps > 0
        and pps < rate_pps * 0.8
    ):
        timer_resolution_warning = True
        print(
            f"[WARN] achieved rate {pps:.0f} pps < target {rate_pps} pps; Windows timer granularity may limit throughput. "
            "Consider setting AUTO_GCS.traffic_engine='iperf3' for higher rates.",
            file=sys.stderr,
        )

    app_packet_bytes = payload_bytes + SEQ_TS_OVERHEAD_BYTES
    wire_packet_bytes_est = app_packet_bytes + wire_header_bytes
    goodput_mbps = (rcvd_packets * payload_bytes * 8) / (elapsed_s * 1_000_000) if elapsed_s > 0 else 0.0
    wire_throughput_mbps_est = (
        (rcvd_packets * wire_packet_bytes_est * 8) / (elapsed_s * 1_000_000)
        if elapsed_s > 0
        else 0.0
    )
    if sent_mbps > 0:
        goodput_ratio = goodput_mbps / sent_mbps
        goodput_ratio = max(0.0, min(1.0, goodput_ratio))
    else:
        goodput_ratio = 0.0

    owd_p50_ms = 0.0
    owd_p95_ms = 0.0
    rtt_p50_ms = 0.0
    rtt_p95_ms = 0.0
    sample_quality = "disabled" if effective_sample_every == 0 else "low"
    owd_samples = 0

    if traffic_mode in {"blast", "constant"} and blaster is not None:
        owd_p50_ms = blaster.owd_p50_ns / 1_000_000
        owd_p95_ms = blaster.owd_p95_ns / 1_000_000
        rtt_p50_ms = blaster.rtt_p50_ns / 1_000_000
        rtt_p95_ms = blaster.rtt_p95_ns / 1_000_000
        owd_samples = blaster.owd_samples
        if effective_sample_every > 0:
            if (
                effective_min_delay == 0
                or (blaster.rtt_samples >= effective_min_delay and blaster.owd_samples >= effective_min_delay)
            ):
                sample_quality = "ok"
    elif use_iperf3:
        sample_quality = "external"

    loss_pct = 0.0
    if sent_packets:
        loss_pct = max(0.0, (sent_packets - rcvd_packets) * 100.0 / sent_packets)
    if use_iperf3:
        loss_low = loss_high = (iperf3_lost_pct or loss_pct) / 100.0
        loss_successes = max(0, iperf3_lost_packets or sent_packets - rcvd_packets)
    else:
        loss_successes = max(0, sent_packets - rcvd_packets)
        loss_low, loss_high = wilson_interval(loss_successes, sent_packets)

    power_avg_w_val = power_fields.get("avg_power_w") if power_fields else None
    if power_avg_w_val is None and power_summary:
        power_avg_w_val = power_summary.get("avg_power_w")
    if power_avg_w_val is not None:
        try:
            power_avg_w_val = float(power_avg_w_val)
        except (TypeError, ValueError):
            power_avg_w_val = None
    power_energy_val = power_fields.get("energy_j") if power_fields else None
    if power_energy_val is None and power_summary:
        power_energy_val = power_summary.get("energy_j")
    if power_energy_val is not None:
        try:
            power_energy_val = float(power_energy_val)
        except (TypeError, ValueError):
            power_energy_val = None
    power_duration_val = power_fields.get("duration_s") if power_fields else None
    if power_duration_val is None and power_summary:
        power_duration_val = power_summary.get("duration_s")
    if power_duration_val is not None:
        try:
            power_duration_val = float(power_duration_val)
        except (TypeError, ValueError):
            power_duration_val = None
    power_summary_path_val = ""
    if power_fields and power_fields.get("summary_json_path"):
        power_summary_path_val = str(power_fields.get("summary_json_path") or "")
    elif power_summary:
        power_summary_path_val = str(power_summary.get("summary_json_path") or power_summary.get("csv_path") or "")
    power_csv_path_val = power_summary.get("csv_path") if power_summary else ""
    if isinstance(power_summary_path_val, Path):
        power_summary_path_val = str(power_summary_path_val)
    if isinstance(power_csv_path_val, Path):
        power_csv_path_val = str(power_csv_path_val)
    power_samples_val = power_summary.get("samples") if power_summary else 0
    power_avg_current_val = (
        round(power_summary.get("avg_current_a", 0.0), 6) if power_summary else 0.0
    )
    power_avg_voltage_val = (
        round(power_summary.get("avg_voltage_v", 0.0), 6) if power_summary else 0.0
    )
    power_sample_rate_val = (
        round(power_summary.get("sample_rate_hz", 0.0), 3) if power_summary else 0.0
    )

    power_trace: List[PowerSample]
    power_trace_error: Optional[str] = None
    if isinstance(power_csv_path_val, str) and power_csv_path_val:
        try:
            power_trace = load_power_trace(power_csv_path_val)
        except FileNotFoundError as exc:
            power_trace = []
            power_trace_error = str(exc)
        except Exception as exc:  # pragma: no cover - defensive parsing
            power_trace = []
            power_trace_error = str(exc)
    else:
        power_trace = []

    monitor_manifest_path_val = (
        str(monitor_manifest_local)
        if isinstance(monitor_manifest_local, Path)
        else (monitor_manifest_local or "")
    )
    telemetry_status_path_val = (
        str(telemetry_status_local)
        if isinstance(telemetry_status_local, Path)
        else (telemetry_status_local or "")
    )
    monitor_artifact_count = len(monitor_artifact_paths)
    monitor_artifact_paths_serialized = [str(path) for path in monitor_artifact_paths]
    monitor_categorized_serialized: Dict[str, List[str]] = {
        category: [str(path) for path in paths]
        for category, paths in monitor_categorized_paths.items()
    }

    companion_metrics = {
        "cpu_max_percent": 0.0,
        "max_rss_bytes": 0,
        "pfc_watts": 0.0,
        "kinematics_vh": 0.0,
        "kinematics_vv": 0.0,
    }
    if telemetry_collector and telemetry_collector.enabled:
        try:
            companion_metrics = _extract_companion_metrics(
                telemetry_collector.snapshot(),
                suite=suite,
                start_ns=start_wall_ns,
                end_ns=end_wall_ns,
            )
        except Exception as exc:
            print(f"[WARN] telemetry aggregation failed for suite {suite}: {exc}", file=sys.stderr)

    part_b_metrics = proxy_stats.get("part_b_metrics") if isinstance(proxy_stats.get("part_b_metrics"), dict) else None
    if not isinstance(part_b_metrics, dict):
        part_b_metrics = {
            key: proxy_stats.get(key)
            for key in (
                "kem_keygen_ms",
                "kem_encaps_ms",
                "kem_decap_ms",
                "sig_sign_ms",
                "sig_verify_ms",
                "primitive_total_ms",
                "pub_key_size_bytes",
                "ciphertext_size_bytes",
                "sig_size_bytes",
                "shared_secret_size_bytes",
            )
        }

    def _metric_ms(name: str) -> float:
        value = part_b_metrics.get(name)
        return _as_float(value) if value is not None else 0.0

    def _metric_int(name: str) -> int:
        value = part_b_metrics.get(name)
        try:
            return int(value)
        except (TypeError, ValueError):
            return 0

    row = {
        "pass": pass_index,
        "suite": suite,
        "traffic_mode": traffic_mode,
        "traffic_engine": traffic_engine_resolved,
        "pre_gap_s": round(pre_gap, 3),
        "inter_gap_s": round(inter_gap_s, 3),
        "duration_s": round(elapsed_s, 3),
        "sent": sent_packets,
        "rcvd": rcvd_packets,
        "pps": round(pps, 1),
        "target_rate_pps": rate_pps,
        "target_bandwidth_mbps": round(target_bandwidth_mbps, 3) if target_bandwidth_mbps else 0.0,
        "throughput_mbps": round(throughput_mbps, 3),
        "sent_mbps": round(sent_mbps, 3),
        "delivered_ratio": round(delivered_ratio, 3) if sent_mbps > 0 else 0.0,
        "goodput_mbps": round(goodput_mbps, 3),
        "wire_throughput_mbps_est": round(wire_throughput_mbps_est, 3),
        "app_packet_bytes": app_packet_bytes,
        "wire_packet_bytes_est": wire_packet_bytes_est,
        "cpu_max_percent": companion_metrics["cpu_max_percent"],
        "max_rss_bytes": companion_metrics["max_rss_bytes"],
        "pfc_watts": companion_metrics["pfc_watts"],
        "kinematics_vh": companion_metrics["kinematics_vh"],
        "kinematics_vv": companion_metrics["kinematics_vv"],
        "goodput_ratio": round(goodput_ratio, 3),
        "rtt_avg_ms": round(avg_rtt_ms, 3),
        "rtt_max_ms": round(max_rtt_ms, 3),
        "rtt_p50_ms": round(rtt_p50_ms, 3),
        "rtt_p95_ms": round(rtt_p95_ms, 3),
        "owd_p50_ms": round(owd_p50_ms, 3),
        "owd_p95_ms": round(owd_p95_ms, 3),
        "rtt_samples": rtt_samples,
        "owd_samples": owd_samples,
        "sample_every": effective_sample_every,
        "min_delay_samples": effective_min_delay,
        "sample_quality": sample_quality,
        "loss_pct": round(loss_pct, 3),
        "loss_pct_wilson_low": round(loss_low * 100.0, 3),
        "loss_pct_wilson_high": round(loss_high * 100.0, 3),
        "enc_out": proxy_stats.get("enc_out", 0),
        "enc_in": proxy_stats.get("enc_in", 0),
        "drops": proxy_stats.get("drops", 0),
        "rekeys_ok": proxy_stats.get("rekeys_ok", 0),
        "rekeys_fail": proxy_stats.get("rekeys_fail", 0),
        "start_ns": start_wall_ns,
        "end_ns": end_wall_ns,
        "scheduled_mark_ns": start_mark_ns,
        "rekey_mark_ns": rekey_mark_ns,
        "rekey_ok_ns": rekey_complete_ns,
        "rekey_ms": round(rekey_duration_ms, 3),
        "rekey_energy_mJ": 0.0,
        "rekey_energy_error": "",
        "handshake_energy_start_ns": 0,
        "handshake_energy_end_ns": 0,
        "rekey_energy_start_ns": 0,
        "rekey_energy_end_ns": 0,
        "handshake_energy_segments": 0,
        "rekey_energy_segments": 0,
        "clock_offset_ns": offset_ns,
        "power_request_ok": power_request_ok,
        "power_capture_ok": power_capture_complete,
        "power_note": power_note,
        "power_error": power_error,
        "power_avg_w": round(power_avg_w_val, 6) if power_avg_w_val is not None else 0.0,
        "power_energy_j": round(power_energy_val, 6) if power_energy_val is not None else 0.0,
        "power_samples": power_samples_val,
        "power_avg_current_a": power_avg_current_val,
        "power_avg_voltage_v": power_avg_voltage_val,
        "power_sample_rate_hz": power_sample_rate_val,
        "power_duration_s": round(power_duration_val, 3) if power_duration_val is not None else 0.0,
        "power_csv_path": power_csv_path_val or "",
        "power_summary_path": power_summary_path_val or "",
        "power_fetch_status": power_fetch_status,
        "power_fetch_error": power_fetch_error,
        "power_trace_samples": len(power_trace),
        "power_trace_error": power_trace_error or "",
        "iperf3_jitter_ms": round(iperf3_jitter_ms, 3) if iperf3_jitter_ms is not None else None,
        "iperf3_lost_pct": round(iperf3_lost_pct, 3) if iperf3_lost_pct is not None else None,
        "iperf3_lost_packets": iperf3_lost_packets,
        "iperf3_report_path": iperf3_report_path or "",
        "monitor_manifest_path": monitor_manifest_path_val,
        "telemetry_status_path": telemetry_status_path_val,
        "monitor_artifacts_fetched": monitor_artifact_count,
        "monitor_artifact_paths": monitor_artifact_paths_serialized,
        "monitor_artifact_categories": monitor_categorized_serialized,
        "monitor_remote_map": monitor_remote_map,
        "monitor_fetch_status": monitor_fetch_status,
        "monitor_fetch_error": monitor_fetch_error,
        "timer_resolution_warning": timer_resolution_warning,
        "blackout_ms": None,
        "gap_max_ms": None,
        "gap_p99_ms": None,
        "steady_gap_ms": None,
        "recv_rate_kpps_before": None,
        "recv_rate_kpps_after": None,
        "proc_ns_p95": None,
        "pair_start_ns": None,
        "pair_end_ns": None,
        "blackout_error": None,
        "timing_guard_ms": None,
        "timing_guard_violation": False,
        "kem_keygen_ms": round(_metric_ms("kem_keygen_ms"), 6),
        "kem_encaps_ms": round(_metric_ms("kem_encaps_ms"), 6),
        "kem_decap_ms": round(_metric_ms("kem_decap_ms"), 6),
        "sig_sign_ms": round(_metric_ms("sig_sign_ms"), 6),
        "sig_verify_ms": round(_metric_ms("sig_verify_ms"), 6),
        "primitive_total_ms": round(_metric_ms("primitive_total_ms"), 6),
        "pub_key_size_bytes": _metric_int("pub_key_size_bytes"),
        "ciphertext_size_bytes": _metric_int("ciphertext_size_bytes"),
        "sig_size_bytes": _metric_int("sig_size_bytes"),
        "shared_secret_size_bytes": _metric_int("shared_secret_size_bytes"),
    "kem_keygen_mJ": 0.0,
    "kem_encaps_mJ": 0.0,
    "kem_decap_mJ": 0.0,
    "sig_sign_mJ": 0.0,
    "sig_verify_mJ": 0.0,
    # Add handshake-prefixed per-primitive energy fields so downstream consumers
    # always see these columns even when the scheduler distributes handshake energy
    # across primitive timings.
    "handshake_kem_keygen_mJ": 0.0,
    "handshake_kem_encap_mJ": 0.0,
    "handshake_kem_decap_mJ": 0.0,
    "handshake_sig_sign_mJ": 0.0,
    "handshake_sig_verify_mJ": 0.0,
    }

    row.update(handshake_fields)

    def _remote_timestamp(value: object) -> Optional[int]:
        try:
            ts = int(value)
        except (TypeError, ValueError):
            return None
        if ts == 0:
            return None
        return align_gcs_to_drone(ts, offset_ns)

    def _clamp_to_capture(window_start: Optional[int], window_end: Optional[int]) -> Tuple[Optional[int], Optional[int]]:
        if window_start is None or window_end is None:
            return window_start, window_end
        adjusted_start = window_start
        adjusted_end = window_end
        if capture_start_remote is not None and adjusted_start < capture_start_remote:
            adjusted_start = capture_start_remote
        if capture_end_remote is not None and adjusted_end > capture_end_remote:
            adjusted_end = capture_end_remote
        if adjusted_end <= adjusted_start:
            return None, None
        return adjusted_start, adjusted_end

    handshake_start_remote = _remote_timestamp(handshake_fields.get("handshake_wall_start_ns"))
    handshake_end_remote = _remote_timestamp(handshake_fields.get("handshake_wall_end_ns"))
    handshake_start_remote, handshake_end_remote = _clamp_to_capture(handshake_start_remote, handshake_end_remote)
    row["handshake_energy_start_ns"] = handshake_start_remote or 0
    row["handshake_energy_end_ns"] = handshake_end_remote or 0

    row["handshake_energy_mJ"] = 0.0
    row["handshake_energy_error"] = power_trace_error or ""
    if (
        not power_trace_error
        and power_trace
        and handshake_start_remote is not None
        and handshake_end_remote is not None
        and handshake_end_remote > handshake_start_remote
    ):
        try:
            energy_mj, segments = integrate_energy_mj(
                power_trace,
                handshake_start_remote,
                handshake_end_remote,
            )
            row["handshake_energy_mJ"] = round(energy_mj, 3)
            row["handshake_energy_segments"] = segments
            row["handshake_energy_error"] = ""
        except Exception as exc:
            row["handshake_energy_error"] = str(exc)
    elif not row["handshake_energy_error"] and handshake_start_remote and handshake_end_remote:
        row["handshake_energy_error"] = "power_trace_empty"

    primitive_duration_map = {
        "kem_keygen_ms": row["kem_keygen_ms"],
        "kem_encaps_ms": row["kem_encaps_ms"],
        "kem_decap_ms": row["kem_decap_ms"],
        "sig_sign_ms": row["sig_sign_ms"],
        "sig_verify_ms": row["sig_verify_ms"],
    }
    duration_total_ms = sum(max(0.0, value) for value in primitive_duration_map.values())
    if duration_total_ms > 0 and row["handshake_energy_mJ"] > 0:
        for name, duration_ms in primitive_duration_map.items():
            if duration_ms <= 0:
                continue
            energy_key = name.replace("_ms", "_mJ")
            portion = duration_ms / duration_total_ms
            row[energy_key] = round(row["handshake_energy_mJ"] * portion, 3)

    rekey_energy_error: Optional[str] = power_trace_error
    rekey_start_remote = _remote_timestamp(rekey_mark_ns)
    rekey_end_remote = _remote_timestamp(rekey_complete_ns)
    rekey_start_remote, rekey_end_remote = _clamp_to_capture(rekey_start_remote, rekey_end_remote)
    row["rekey_energy_start_ns"] = rekey_start_remote or 0
    row["rekey_energy_end_ns"] = rekey_end_remote or 0

    row["rekey_energy_segments"] = 0
    if (
        not rekey_energy_error
        and power_trace
        and rekey_start_remote is not None
        and rekey_end_remote is not None
        and rekey_end_remote > rekey_start_remote
    ):
        try:
            energy_mj, segments = integrate_energy_mj(
                power_trace,
                rekey_start_remote,
                rekey_end_remote,
            )
            row["rekey_energy_mJ"] = round(energy_mj, 3)
            row["rekey_energy_segments"] = segments
            rekey_energy_error = None
        except Exception as exc:
            rekey_energy_error = str(exc)
    elif not rekey_energy_error and rekey_start_remote and rekey_end_remote:
        rekey_energy_error = "power_trace_empty"

    if rekey_energy_error:
        row["rekey_energy_error"] = rekey_energy_error

    if power_summary:
        print(
            f"[{ts()}] power summary suite={suite} avg={power_summary.get('avg_power_w', 0.0):.3f} W "
            f"energy={power_summary.get('energy_j', 0.0):.3f} J samples={power_summary.get('samples', 0)}"
        )
    elif power_capture_enabled and power_request_ok and power_error:
        print(f"[{ts()}] power summary unavailable for suite={suite}: {power_error}")

    target_desc = f" target={target_bandwidth_mbps:.2f} Mb/s" if target_bandwidth_mbps > 0 else ""
    print(
        f"[{ts()}] <<< FINISH suite={suite} mode={traffic_mode} engine={traffic_engine_resolved} "
        f"sent={sent_packets} rcvd={rcvd_packets} "
        f"pps~{pps:.0f} thr~{throughput_mbps:.2f} Mb/s sent~{sent_mbps:.2f} Mb/s loss={loss_pct:.2f}% "
        f"rtt_avg={avg_rtt_ms:.3f}ms rtt_max={max_rtt_ms:.3f}ms rekey={rekey_duration_ms:.2f}ms "
        f"enc_out={row['enc_out']} enc_in={row['enc_in']}{target_desc} >>>"
    )

    return row


def write_summary(rows: List[dict]) -> None:
    if not rows:
        return
    mkdirp(OUTDIR)
    headers = list(rows[0].keys())
    for attempt in range(3):
        try:
            buffer = io.StringIO()
            writer = csv.DictWriter(buffer, fieldnames=headers)
            writer.writeheader()
            writer.writerows(rows)
            _atomic_write_bytes(SUMMARY_CSV, buffer.getvalue().encode("utf-8"))
            print(f"[{ts()}] wrote {SUMMARY_CSV}")
            return
        except Exception as exc:
            if attempt == 2:
                print(f"[WARN] failed to write {SUMMARY_CSV}: {exc}", file=sys.stderr)
            time.sleep(0.1)


def _append_blackout_records(records: List[Dict[str, Any]]) -> None:
    if not records:
        return
    try:
        BLACKOUT_CSV.parent.mkdir(parents=True, exist_ok=True)
        fieldnames = [
            "timestamp_utc",
            "session_id",
            "index",
            "pass",
            "suite",
            "traffic_mode",
            "rekey_mark_ns",
            "rekey_ok_ns",
            "scheduled_mark_ns",
            "blackout_ms",
            "gap_max_ms",
            "gap_p99_ms",
            "steady_gap_ms",
            "recv_rate_kpps_before",
            "recv_rate_kpps_after",
            "proc_ns_p95",
            "pair_start_ns",
            "pair_end_ns",
            "blackout_error",
        ]
        new_file = not BLACKOUT_CSV.exists()
        with BLACKOUT_CSV.open("a", newline="", encoding="utf-8") as handle:
            writer = csv.DictWriter(handle, fieldnames=fieldnames)
            if new_file:
                writer.writeheader()
            for record in records:
                writer.writerow(record)
        print(f"[{ts()}] updated {BLACKOUT_CSV} ({len(records)} rows)")
    except Exception as exc:
        print(f"[WARN] blackout log append failed: {exc}", file=sys.stderr)


def _append_step_results(payloads: List[Dict[str, Any]]) -> None:
    if not payloads:
        return
    try:
        STEP_RESULTS_PATH.parent.mkdir(parents=True, exist_ok=True)
        with STEP_RESULTS_PATH.open("a", encoding="utf-8") as handle:
            for payload in payloads:
                handle.write(json.dumps(payload) + "\n")
        print(f"[{ts()}] appended {len(payloads)} step records -> {STEP_RESULTS_PATH}")
    except Exception as exc:
        print(f"[WARN] step_results append failed: {exc}", file=sys.stderr)


def _enrich_summary_rows(
    rows: List[dict],
    *,
    session_id: str,
    drone_session_dir: Optional[Path],
    traffic_mode: str,
    pre_gap_s: float,
    duration_s: float,
    inter_gap_s: float,
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    blackout_records: List[Dict[str, Any]] = []
    step_payloads: List[Dict[str, Any]] = []
    session_dir_exists = bool(drone_session_dir and drone_session_dir.exists())
    session_dir_str = str(drone_session_dir) if drone_session_dir else ""
    for index, row in enumerate(rows):
        mark_ns = row.get("rekey_mark_ns")
        ok_ns = row.get("rekey_ok_ns")
        metrics: Dict[str, Any] = {}
        blackout_error: Optional[str] = None
        if session_dir_exists and mark_ns and ok_ns and ok_ns >= mark_ns:
            try:
                metrics = compute_blackout(drone_session_dir, int(mark_ns), int(ok_ns))
            except Exception as exc:
                blackout_error = str(exc)
                metrics = {}
        else:
            if not session_dir_exists:
                blackout_error = "session_dir_unavailable"
            elif not mark_ns or not ok_ns:
                blackout_error = "missing_mark_or_ok"
            elif ok_ns is not None and mark_ns is not None and ok_ns < mark_ns:
                blackout_error = "invalid_timestamp_order"

        row["blackout_ms"] = metrics.get("blackout_ms")
        row["gap_max_ms"] = metrics.get("gap_max_ms")
        row["gap_p99_ms"] = metrics.get("gap_p99_ms")
        row["steady_gap_ms"] = metrics.get("steady_gap_ms")
        row["recv_rate_kpps_before"] = metrics.get("recv_rate_kpps_before")
        row["recv_rate_kpps_after"] = metrics.get("recv_rate_kpps_after")
        row["proc_ns_p95"] = metrics.get("proc_ns_p95")
        row["pair_start_ns"] = metrics.get("pair_start_ns")
        row["pair_end_ns"] = metrics.get("pair_end_ns")
        if blackout_error is None:
            blackout_error = metrics.get("error")
        row["blackout_error"] = blackout_error

        guard_ms = int(
            max(row.get("pre_gap_s", pre_gap_s) or 0.0, 0.0) * 1000.0
            + max(row.get("duration_s", duration_s) or 0.0, 0.0) * 1000.0
            + 10_000
        )
        row["timing_guard_ms"] = guard_ms
        rekey_ms = row.get("rekey_ms") or 0.0
        try:
            rekey_ms_val = float(rekey_ms)
        except (TypeError, ValueError):
            rekey_ms_val = 0.0
        timing_violation = bool(rekey_ms_val and rekey_ms_val > guard_ms)
        row["timing_guard_violation"] = timing_violation
        if timing_violation:
            print(
                f"[WARN] rekey duration {rekey_ms_val:.2f} ms exceeds guard {guard_ms} ms (suite={row.get('suite')} pass={row.get('pass')})",
                file=sys.stderr,
            )

        row.setdefault("traffic_mode", traffic_mode)
        row.setdefault("pre_gap_s", pre_gap_s)
        row.setdefault("inter_gap_s", inter_gap_s)

        blackout_records.append(
            {
                "timestamp_utc": ts(),
                "session_id": session_id,
                "index": index,
                "pass": row.get("pass"),
                "suite": row.get("suite"),
                "traffic_mode": row.get("traffic_mode"),
                "rekey_mark_ns": mark_ns or "",
                "rekey_ok_ns": ok_ns or "",
                "scheduled_mark_ns": row.get("scheduled_mark_ns") or "",
                "blackout_ms": row.get("blackout_ms"),
                "gap_max_ms": row.get("gap_max_ms"),
                "gap_p99_ms": row.get("gap_p99_ms"),
                "steady_gap_ms": row.get("steady_gap_ms"),
                "recv_rate_kpps_before": row.get("recv_rate_kpps_before"),
                "recv_rate_kpps_after": row.get("recv_rate_kpps_after"),
                "proc_ns_p95": row.get("proc_ns_p95"),
                "pair_start_ns": row.get("pair_start_ns"),
                "pair_end_ns": row.get("pair_end_ns"),
                "blackout_error": blackout_error or "",
            }
        )

        payload = dict(row)
        payload["ts_utc"] = ts()
        payload["session_id"] = session_id
        payload["session_dir"] = session_dir_str
        payload["index"] = index
        payload["blackout_error"] = blackout_error
        payload["timing_guard_ms"] = guard_ms
        payload["timing_guard_violation"] = timing_violation
        step_payloads.append(payload)

    return blackout_records, step_payloads


class SaturationTester:
    def __init__(
        self,
        suite: str,
        payload_bytes: int,
        duration_s: float,
        event_sample: int,
        offset_ns: int,
        output_dir: Path,
        max_rate_mbps: int,
        search_mode: str,
        delivery_threshold: float,
        loss_threshold: float,
        spike_factor: float,
        min_delay_samples: int,
    ) -> None:
        self.suite = suite
        self.payload_bytes = payload_bytes
        self.duration_s = duration_s
        self.event_sample = max(0, int(event_sample))
        self.offset_ns = offset_ns
        self.output_dir = output_dir
        self.max_rate_mbps = max_rate_mbps
        self.search_mode = search_mode
        self.delivery_threshold = delivery_threshold
        self.loss_threshold = loss_threshold
        self.spike_factor = spike_factor
        self.min_delay_samples = max(0, int(min_delay_samples))
        self.records: List[Dict[str, float]] = []
        self._rate_cache: Dict[int, Tuple[Dict[str, float], bool, Optional[str]]] = {}
        self._baseline: Optional[Dict[str, float]] = None
        self._signal_history = {key: deque(maxlen=HYSTERESIS_WINDOW) for key in SATURATION_SIGNALS}
        self._last_ok_rate: Optional[int] = None
        self._first_bad_rate: Optional[int] = None
        self._stop_cause: Optional[str] = None
        self._stop_samples = 0

    def run(self) -> Dict[str, Optional[float]]:
        self.records = []
        self._rate_cache.clear()
        self._baseline = None
        self._signal_history = {key: deque(maxlen=HYSTERESIS_WINDOW) for key in SATURATION_SIGNALS}
        self._last_ok_rate = None
        self._first_bad_rate = None
        self._stop_cause = None
        self._stop_samples = 0

        used_mode = self.search_mode
        if self.search_mode == "linear":
            self._linear_search()
        else:
            self._coarse_search()
            if self._first_bad_rate is not None and self._last_ok_rate is not None:
                self._bisect_search()
            elif self.search_mode == "bisect" and self._first_bad_rate is None:
                self._linear_search()
                used_mode = "linear"

        resolution = None
        if self._first_bad_rate is not None and self._last_ok_rate is not None:
            resolution = max(0, self._first_bad_rate - self._last_ok_rate)
        saturation_point = self._last_ok_rate if self._last_ok_rate is not None else self._first_bad_rate
        confidence = min(1.0, self._stop_samples / 200.0) if self._stop_samples > 0 else 0.0

        baseline = self._baseline or {}
        return {
            "suite": self.suite,
            "baseline_owd_p50_ms": baseline.get("owd_p50_ms"),
            "baseline_owd_p95_ms": baseline.get("owd_p95_ms"),
            "baseline_rtt_p50_ms": baseline.get("rtt_p50_ms"),
            "baseline_rtt_p95_ms": baseline.get("rtt_p95_ms"),
            "saturation_point_mbps": saturation_point,
            "stop_cause": self._stop_cause,
            "confidence": round(confidence, 3),
            "search_mode": used_mode,
            "resolution_mbps": resolution,
        }

    def _linear_search(self) -> None:
        for rate in SATURATION_LINEAR_RATES:
            if rate > self.max_rate_mbps:
                break
            _, is_bad, _ = self._evaluate_rate(rate)
            if is_bad:
                break

    def _coarse_search(self) -> None:
        for rate in SATURATION_COARSE_RATES:
            if rate > self.max_rate_mbps:
                break
            _, is_bad, _ = self._evaluate_rate(rate)
            if is_bad:
                break

    def _bisect_search(self) -> None:
        if self._first_bad_rate is None:
            return
        lo = self._last_ok_rate if self._last_ok_rate is not None else 0
        hi = self._first_bad_rate
        steps = 0
        while hi - lo > 5 and steps < MAX_BISECT_STEPS:
            mid = max(1, int(round((hi + lo) / 2)))
            if mid == hi or mid == lo:
                break
            _, is_bad, _ = self._evaluate_rate(mid)
            steps += 1
            metrics = self._rate_cache[mid][0]
            sample_ok = metrics.get("sample_quality") == "ok"
            if not sample_ok:
                is_bad = True
            if is_bad:
                if mid < hi:
                    hi = mid
                if self._first_bad_rate is None or mid < self._first_bad_rate:
                    self._first_bad_rate = mid
            else:
                if mid > lo:
                    lo = mid
                if self._last_ok_rate is None or mid > self._last_ok_rate:
                    self._last_ok_rate = mid

    def _evaluate_rate(self, rate: int) -> Tuple[Dict[str, float], bool, Optional[str]]:
        cached = self._rate_cache.get(rate)
        if cached:
            return cached

        metrics = self._run_rate(rate)
        metrics["suite"] = self.suite
        self.records.append(metrics)

        if self._baseline is None and metrics.get("sample_quality") == "ok":
            self._baseline = {
                "owd_p50_ms": metrics.get("owd_p50_ms"),
                "owd_p95_ms": metrics.get("owd_p95_ms"),
                "rtt_p50_ms": metrics.get("rtt_p50_ms"),
                "rtt_p95_ms": metrics.get("rtt_p95_ms"),
            }

        signals = self._classify_signals(metrics)
        is_bad = any(signals.values())
        cause = self._update_history(signals, rate, metrics)
        if is_bad:
            if self._first_bad_rate is None or rate < self._first_bad_rate:
                self._first_bad_rate = rate
        else:
            if metrics.get("sample_quality") == "ok":
                if self._last_ok_rate is None or rate > self._last_ok_rate:
                    self._last_ok_rate = rate

        result = (metrics, is_bad, cause)
        self._rate_cache[rate] = result
        return result

    def _classify_signals(self, metrics: Dict[str, float]) -> Dict[str, bool]:
        signals = {key: False for key in SATURATION_SIGNALS}
        baseline = self._baseline
        owd_spike = False
        if baseline:
            baseline_p95 = baseline.get("owd_p95_ms") or 0.0
            if baseline_p95 > 0:
                owd_p95 = metrics.get("owd_p95_ms", 0.0)
                owd_spike = owd_p95 >= baseline_p95 * self.spike_factor
        signals["owd_p95_spike"] = owd_spike

        goodput_ratio = metrics.get("goodput_ratio", 0.0)
        ratio_drop = goodput_ratio < self.delivery_threshold
        delivery_degraded = ratio_drop and owd_spike
        signals["delivery_degraded"] = delivery_degraded

        loss_flag = metrics.get("loss_pct", 0.0) > self.loss_threshold
        if metrics.get("sample_quality") != "ok" and loss_flag and not (delivery_degraded or owd_spike):
            loss_flag = False
        signals["loss_excess"] = loss_flag
        return signals

    def _update_history(
        self,
        signals: Dict[str, bool],
        rate: int,
        metrics: Dict[str, float],
    ) -> Optional[str]:
        cause = None
        for key in SATURATION_SIGNALS:
            history = self._signal_history[key]
            history.append(bool(signals.get(key)))
            if self._stop_cause is None and sum(history) >= 2:
                self._stop_cause = key
                self._stop_samples = max(metrics.get("rtt_samples", 0), metrics.get("owd_samples", 0))
                cause = key
        return cause

    def _run_rate(self, rate_mbps: int) -> Dict[str, float]:
        denominator = max(self.payload_bytes * 8, 1)
        rate_pps = int((rate_mbps * 1_000_000) / denominator)
        if rate_pps <= 0:
            rate_pps = 1
        events_path = self.output_dir / f"saturation_{rate_mbps}Mbps.jsonl"
        warmup_s = min(MAX_WARMUP_SECONDS, self.duration_s * WARMUP_FRACTION)
        effective_sample_every, effective_min_delay = _compute_sampling_params(
            self.duration_s,
            self.event_sample,
            self.min_delay_samples,
        )
        if warmup_s > 0:
            warmup_blaster = Blaster(
                APP_SEND_HOST,
                APP_SEND_PORT,
                APP_RECV_HOST,
                APP_RECV_PORT,
                events_path=None,
                payload_bytes=self.payload_bytes,
                sample_every=0,
                offset_ns=self.offset_ns,
            )
            warmup_blaster.run(duration_s=warmup_s, rate_pps=rate_pps)
        blaster = Blaster(
            APP_SEND_HOST,
            APP_SEND_PORT,
            APP_RECV_HOST,
            APP_RECV_PORT,
            events_path,
            payload_bytes=self.payload_bytes,
            sample_every=effective_sample_every if effective_sample_every > 0 else 0,
            offset_ns=self.offset_ns,
        )
        start = time.perf_counter()
        blaster.run(duration_s=self.duration_s, rate_pps=rate_pps)
        elapsed = max(1e-9, time.perf_counter() - start)

        sent_packets = blaster.sent
        rcvd_packets = blaster.rcvd
        sent_bytes = blaster.sent_bytes
        rcvd_bytes = blaster.rcvd_bytes

        pps_actual = sent_packets / elapsed if elapsed > 0 else 0.0
        throughput_mbps = (rcvd_bytes * 8) / (elapsed * 1_000_000) if elapsed > 0 else 0.0
        sent_mbps = (sent_bytes * 8) / (elapsed * 1_000_000) if sent_bytes else 0.0
        delivered_ratio = throughput_mbps / sent_mbps if sent_mbps > 0 else 0.0

        avg_rtt_ms = (blaster.rtt_sum_ns / max(1, blaster.rtt_samples)) / 1_000_000 if blaster.rtt_samples else 0.0
        min_rtt_ms = (blaster.rtt_min_ns or 0) / 1_000_000
        max_rtt_ms = blaster.rtt_max_ns / 1_000_000

        app_packet_bytes = self.payload_bytes + SEQ_TS_OVERHEAD_BYTES
        wire_header_bytes = getattr(blaster, "wire_header_bytes", UDP_HEADER_BYTES + APP_IP_HEADER_BYTES)
        wire_packet_bytes_est = app_packet_bytes + wire_header_bytes
        goodput_mbps = (
            (rcvd_packets * self.payload_bytes * 8) / (elapsed * 1_000_000)
            if elapsed > 0
            else 0.0
        )
        wire_throughput_mbps_est = (
            (rcvd_packets * wire_packet_bytes_est * 8) / (elapsed * 1_000_000)
            if elapsed > 0
            else 0.0
        )
        if sent_mbps > 0:
            goodput_ratio = goodput_mbps / sent_mbps
            goodput_ratio = max(0.0, min(1.0, goodput_ratio))
        else:
            goodput_ratio = 0.0

        loss_pct = 0.0
        if sent_packets:
            loss_pct = max(0.0, (sent_packets - rcvd_packets) * 100.0 / sent_packets)
        loss_low, loss_high = wilson_interval(max(0, sent_packets - rcvd_packets), sent_packets)

        sample_quality = "disabled" if effective_sample_every == 0 else "low"
        if effective_sample_every > 0:
            if (
                effective_min_delay == 0
                or (blaster.rtt_samples >= effective_min_delay and blaster.owd_samples >= effective_min_delay)
            ):
                sample_quality = "ok"
            if getattr(blaster, "truncated", 0) > 0:
                sample_quality = "low"

        return {
            "rate_mbps": float(rate_mbps),
            "pps": float(rate_pps),
            "pps_actual": round(pps_actual, 1),
            "sent_mbps": round(sent_mbps, 3),
            "throughput_mbps": round(throughput_mbps, 3),
            "goodput_mbps": round(goodput_mbps, 3),
            "wire_throughput_mbps_est": round(wire_throughput_mbps_est, 3),
            "goodput_ratio": round(goodput_ratio, 3),
            "loss_pct": round(loss_pct, 3),
            "loss_pct_wilson_low": round(loss_low * 100.0, 3),
            "loss_pct_wilson_high": round(loss_high * 100.0, 3),
            "delivered_ratio": round(delivered_ratio, 3) if sent_mbps > 0 else 0.0,
            "avg_rtt_ms": round(avg_rtt_ms, 3),
            "min_rtt_ms": round(min_rtt_ms, 3),
            "max_rtt_ms": round(max_rtt_ms, 3),
            "rtt_p50_ms": round(blaster.rtt_p50_ns / 1_000_000, 3),
            "rtt_p95_ms": round(blaster.rtt_p95_ns / 1_000_000, 3),
            "owd_p50_ms": round(blaster.owd_p50_ns / 1_000_000, 3),
            "owd_p95_ms": round(blaster.owd_p95_ns / 1_000_000, 3),
            "rtt_samples": blaster.rtt_samples,
            "owd_samples": blaster.owd_samples,
            "sample_every": effective_sample_every,
            "min_delay_samples": effective_min_delay,
            "sample_quality": sample_quality,
            "app_packet_bytes": app_packet_bytes,
            "wire_packet_bytes_est": wire_packet_bytes_est,
        }

    def export_excel(self, session_id: str, output_base: Path) -> Optional[Path]:
        if Workbook is None:
            print("[WARN] openpyxl not available; skipping Excel export")
            return None
        output_base.mkdir(parents=True, exist_ok=True)
        path = output_base / f"saturation_{self.suite}_{session_id}.xlsx"
        wb = Workbook()
        ws = wb.active
        ws.title = "Saturation"
        ws.append([
            "rate_mbps",
            "pps",
            "pps_actual",
            "sent_mbps",
            "throughput_mbps",
            "goodput_mbps",
            "wire_throughput_mbps_est",
            "goodput_ratio",
            "loss_pct",
            "loss_pct_wilson_low",
            "loss_pct_wilson_high",
            "delivered_ratio",
            "avg_rtt_ms",
            "min_rtt_ms",
            "max_rtt_ms",
            "rtt_p50_ms",
            "rtt_p95_ms",
            "owd_p50_ms",
            "owd_p95_ms",
            "rtt_samples",
            "owd_samples",
            "sample_quality",
            "app_packet_bytes",
            "wire_packet_bytes_est",
        ])
        for record in self.records:
            ws.append([
                record.get("rate_mbps", 0.0),
                record.get("pps", 0.0),
                record.get("pps_actual", 0.0),
                record.get("sent_mbps", 0.0),
                record.get("throughput_mbps", 0.0),
                record.get("goodput_mbps", 0.0),
                record.get("wire_throughput_mbps_est", 0.0),
                record.get("goodput_ratio", 0.0),
                record.get("loss_pct", 0.0),
                record.get("loss_pct_wilson_low", 0.0),
                record.get("loss_pct_wilson_high", 0.0),
                record.get("delivered_ratio", 0.0),
                record.get("avg_rtt_ms", 0.0),
                record.get("min_rtt_ms", 0.0),
                record.get("max_rtt_ms", 0.0),
                record.get("rtt_p50_ms", 0.0),
                record.get("rtt_p95_ms", 0.0),
                record.get("owd_p50_ms", 0.0),
                record.get("owd_p95_ms", 0.0),
                record.get("rtt_samples", 0),
                record.get("owd_samples", 0),
                record.get("sample_quality", "low"),
                record.get("app_packet_bytes", 0),
                record.get("wire_packet_bytes_est", 0),
            ])
        for attempt in range(3):
            try:
                buffer = io.BytesIO()
                wb.save(buffer)
                _atomic_write_bytes(path, buffer.getvalue())
                return path
            except OSError as exc:  # pragma: no cover - platform specific
                if attempt == 2:
                    print(f"[WARN] failed to save {path}: {exc}", file=sys.stderr)
            except Exception as exc:  # pragma: no cover - platform specific
                if attempt == 2:
                    print(f"[WARN] failed to write saturation workbook {path}: {exc}", file=sys.stderr)
            time.sleep(0.1)
        return None


class TelemetryCollector:
    def __init__(self, host: str, port: int) -> None:
        self.host = host
        self.port = port
        self.stop_event = threading.Event()
        # Bug #9 fix: Use deque with maxlen to prevent unbounded memory growth
        env_maxlen = os.getenv("GCS_TELEM_MAXLEN")
        maxlen = TELEMETRY_BUFFER_MAXLEN_DEFAULT
        if env_maxlen:
            try:
                candidate = int(env_maxlen)
                if candidate <= 0:
                    raise ValueError
                if candidate < 1000:
                    candidate = 1000
                if candidate > 1_000_000:
                    print(
                        f"[WARN] GCS_TELEM_MAXLEN={candidate} capped at 1000000", file=sys.stderr
                    )
                maxlen = min(candidate, 1_000_000)
            except ValueError:
                print(
                    f"[WARN] invalid GCS_TELEM_MAXLEN={env_maxlen!r}; using default {TELEMETRY_BUFFER_MAXLEN_DEFAULT}",
                    file=sys.stderr,
                )
                maxlen = TELEMETRY_BUFFER_MAXLEN_DEFAULT
        self.samples: deque = deque(maxlen=maxlen)
        self.lock = threading.Lock()
        self.enabled = True
        self.thread: Optional[threading.Thread] = None
        self._last_error: Optional[str] = None

    def start(self) -> None:
        if not self.enabled:
            return
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()

    def _run(self) -> None:
        backoff = 1.0
        while not self.stop_event.is_set():
            try:
                with socket.create_connection((self.host, self.port), timeout=5.0) as sock:
                    sock.settimeout(1.0)
                    print(f"[{ts()}] telemetry connected to {self.host}:{self.port}")
                    self._read_stream(sock)
                    print(f"[{ts()}] telemetry disconnected from {self.host}:{self.port}")
                    backoff = 1.0
            except Exception as exc:
                self._last_error = str(exc)
                if not self.stop_event.is_set():
                    print(f"[WARN] telemetry connection error: {exc}", file=sys.stderr)
            if self.stop_event.is_set():
                break
            time.sleep(min(backoff, 5.0))
            backoff = min(backoff * 1.5, 5.0)

    def _read_stream(self, sock: socket.socket) -> None:
        try:
            with sock.makefile("r", encoding="utf-8") as reader:
                for line in reader:
                    if self.stop_event.is_set():
                        break
                    data = line.strip()
                    if not data:
                        continue
                    try:
                        payload = json.loads(data)
                    except json.JSONDecodeError:
                        continue
                    payload.setdefault("collector_ts_ns", time.time_ns())
                    payload.setdefault("source", "drone")
                    payload.setdefault("peer", f"{self.host}:{self.port}")
                    with self.lock:
                        self.samples.append(payload)
        except Exception:
            if not self.stop_event.is_set():
                raise

    def snapshot(self) -> List[dict]:
        with self.lock:
            return list(self.samples)

    def stop(self) -> None:
        self.stop_event.set()
        if self.thread and self.thread.is_alive():
            self.thread.join(timeout=1.5)

def resolve_under_root(path: Path) -> Path:
    expanded = path.expanduser()
    return expanded if expanded.is_absolute() else ROOT / expanded


REMOTE_FETCH_REMOVED_MSG = (
    "Remote artifact fetch has been retired. Sync drone logs/output via Git or manual copy"
    " before running scheduler reports."
)


def _post_run_fetch_artifacts(session_id: str) -> None:
    fetch_cfg = AUTO_GCS_CONFIG.get("post_fetch") or {}
    enabled_default = _coerce_bool(fetch_cfg.get("enabled"), False)
    enabled = _coerce_bool(os.getenv("DRONE_FETCH_ENABLED"), enabled_default)
    if not enabled:
        return

    print(f"[{ts()}] post_fetch requested for session {session_id}, but remote fetch was removed.")
    print(f"[{ts()}] {REMOTE_FETCH_REMOVED_MSG}")
    logs_hint = ROOT / "logs" / "auto" / f"drone_{session_id}"
    output_hint = ROOT / "output" / "drone" / session_id
    print(f"[{ts()}] Ensure {logs_hint} and {output_hint} exist after syncing commits.")


def _post_run_collect_local(session_id: str, *, gcs_log_path: Optional[Path], combined_workbook: Optional[Path]) -> Path:
    session_dir = OUTDIR / session_id
    session_dir.mkdir(parents=True, exist_ok=True)
    if gcs_log_path and gcs_log_path.exists():
        target = session_dir / gcs_log_path.name
        try:
            shutil.copy2(gcs_log_path, target)
        except Exception as exc:
            print(f"[WARN] failed to copy GCS log: {exc}", file=sys.stderr)
    if SUMMARY_CSV.exists():
        try:
            shutil.copy2(SUMMARY_CSV, session_dir / SUMMARY_CSV.name)
        except Exception as exc:
            print(f"[WARN] failed to copy summary CSV: {exc}", file=sys.stderr)
    if combined_workbook and combined_workbook.exists():
        try:
            shutil.copy2(combined_workbook, session_dir / combined_workbook.name)
        except Exception as exc:
            print(f"[WARN] failed to copy combined workbook: {exc}", file=sys.stderr)
    return session_dir


def _post_run_generate_reports(session_id: str, *, session_dir: Path) -> None:
    report_cfg = AUTO_GCS_CONFIG.get("post_report") or {}
    enabled_default = _coerce_bool(report_cfg.get("enabled"), True)
    enabled = _coerce_bool(os.getenv("DRONE_REPORT_ENABLED"), enabled_default)
    if not enabled:
        return
    script_rel = os.getenv("DRONE_REPORT_SCRIPT") or report_cfg.get("script") or "tools/report_constant_run.py"
    script_path = resolve_under_root(Path(script_rel))
    if not script_path.exists():
        print(f"[WARN] report script missing: {script_path}")
        return
    if not SUMMARY_CSV.exists():
        print(f"[WARN] report generation skipped: {SUMMARY_CSV} missing")
        return
    output_rel = os.getenv("DRONE_REPORT_OUTPUT" ) or report_cfg.get("output_dir")
    if output_rel:
        output_dir = resolve_under_root(Path(output_rel)) / session_id
    else:
        output_dir = session_dir if session_dir else resolve_under_root(Path("output/gcs")) / session_id
    output_dir.mkdir(parents=True, exist_ok=True)

    table_name = os.getenv("DRONE_REPORT_TABLE") or report_cfg.get("table_name")
    text_name = os.getenv("DRONE_REPORT_TEXT") or report_cfg.get("text_name")

    cmd = [
        sys.executable,
        str(script_path),
        "--summary-csv",
        str(SUMMARY_CSV),
        "--run-id",
        session_id,
        "--output-dir",
        str(output_dir),
    ]
    if table_name:
        cmd += ["--table-name", str(table_name)]
    if text_name:
        cmd += ["--text-name", str(text_name)]

    env = os.environ.copy()
    root_str = str(ROOT)
    existing = env.get("PYTHONPATH")
    if existing:
        if root_str not in existing.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing
    else:
        env["PYTHONPATH"] = root_str

    print(f"[{ts()}] post_run report -> {output_dir}")
    result = subprocess.run(cmd, cwd=str(ROOT), text=True, capture_output=True, env=env)
    if result.returncode != 0:
        print(f"[WARN] report generation failed (exit {result.returncode}): {result.stderr.strip()}")
    elif result.stderr.strip():
        print(result.stderr.strip())
    if result.stdout.strip():
        print(result.stdout.strip())


def safe_sheet_name(name: str) -> str:
    sanitized = "".join("_" if ch in '[]:*?/\\' else ch for ch in name).strip()
    if not sanitized:
        sanitized = "Sheet"
    return sanitized[:31]


def unique_sheet_name(workbook, base_name: str) -> str:
    base = safe_sheet_name(base_name)
    if base not in workbook.sheetnames:
        return base
    index = 1
    while True:
        suffix = f"_{index}"
        name = base[: 31 - len(suffix)] + suffix
        if name not in workbook.sheetnames:
            return name
        index += 1


def append_dict_sheet(workbook, title: str, rows: List[dict]) -> None:
    if not rows:
        return
    sheet_name = unique_sheet_name(workbook, title)
    ws = workbook.create_sheet(sheet_name)
    headers: List[str] = []
    for row in rows:
        for key in row.keys():
            if key not in headers:
                headers.append(key)
    ws.append(headers)
    def _coerce(value: object) -> object:
        if value is None:
            return ""
        if isinstance(value, (str, int, float, bool)):
            return value
        if isinstance(value, Path):
            return str(value)
        try:
            return json.dumps(value, ensure_ascii=True, sort_keys=True)
        except TypeError:
            return str(value)

    for row in rows:
        ws.append([_coerce(row.get(header)) for header in headers])


def append_csv_sheet(workbook, path: Path, title: str) -> None:
    if not path.exists():
        return
    rows = None
    for attempt in range(3):
        try:
            with open(path, newline="", encoding="utf-8") as handle:
                reader = csv.reader(handle)
                rows = list(reader)
            break
        except OSError as exc:
            if attempt == 2:
                print(f"[WARN] failed to read CSV {path}: {exc}", file=sys.stderr)
            time.sleep(0.1)
        except Exception as exc:
            print(f"[WARN] failed to parse CSV {path}: {exc}", file=sys.stderr)
            return
    if not rows:
        return
    sheet_name = unique_sheet_name(workbook, title)
    ws = workbook.create_sheet(sheet_name)
    for row in rows:
        ws.append(row)


def locate_drone_session_dir(session_id: str) -> Optional[Path]:
    candidates = []
    try:
        candidates.append(resolve_under_root(DRONE_MONITOR_BASE) / session_id)
    except Exception:
        pass
    fallback = Path("/home/dev/research/output/drone") / session_id
    candidates.append(fallback)
    repo_default = ROOT / "output" / "drone" / session_id
    candidates.append(repo_default)
    seen = set()
    for candidate in candidates:
        if candidate in seen:
            continue
        seen.add(candidate)
        try:
            if candidate.exists():
                return candidate
        except Exception:
            continue
    return None


def export_combined_excel(
    session_id: str,
    summary_rows: List[dict],
    saturation_overview: List[dict],
    saturation_samples: List[dict],
    telemetry_samples: List[dict],
    drone_session_dir: Optional[Path] = None,
    follower_capabilities: Optional[Dict[str, object]] = None,
    follower_capabilities_path: Optional[Path] = None,
    *,
    traffic_mode: str,
    payload_bytes: int,
    event_sample: int,
    min_delay_samples: int,
    pre_gap_s: float,
    duration_s: float,
    inter_gap_s: float,
    sat_search: str,
    sat_delivery_threshold: float,
    sat_loss_threshold_pct: float,
    sat_rtt_spike_factor: float,
) -> Optional[Path]:
    if Workbook is None:
        print("[WARN] openpyxl not available; skipping combined Excel export", file=sys.stderr)
        return None

    workbook = Workbook()
    info_sheet = workbook.active
    info_sheet.title = "run_info"
    info_sheet.append(["generated_utc", ts()])
    info_sheet.append(["session_id", session_id])
    if follower_capabilities_path:
        info_sheet.append(["follower_capabilities_path", str(follower_capabilities_path)])

    append_dict_sheet(workbook, "gcs_summary", summary_rows)
    append_dict_sheet(workbook, "saturation_overview", saturation_overview)
    append_dict_sheet(workbook, "saturation_samples", saturation_samples)
    append_dict_sheet(workbook, "telemetry_samples", telemetry_samples)

    if follower_capabilities:
        append_dict_sheet(workbook, "follower_capabilities_meta", [follower_capabilities])
        supported = follower_capabilities.get("supported_suites")
        if isinstance(supported, (list, tuple, set)):
            supported_rows = [{"suite": str(name)} for name in supported if isinstance(name, str)]
            append_dict_sheet(workbook, "follower_supported_suites", supported_rows)
        unsupported = follower_capabilities.get("unsupported_suites")
        if isinstance(unsupported, list):
            rows: List[dict] = []
            for entry in unsupported:
                if not isinstance(entry, dict):
                    continue
                row: Dict[str, object] = {}
                suite_name = entry.get("suite")
                if isinstance(suite_name, str):
                    row["suite"] = suite_name
                raw_reasons = entry.get("reasons")
                if isinstance(raw_reasons, (list, tuple, set)):
                    row["reasons"] = ",".join(str(item) for item in raw_reasons if item)
                elif raw_reasons:
                    row["reasons"] = str(raw_reasons)
                details = entry.get("details")
                if isinstance(details, dict):
                    for key, value in details.items():
                        if key not in row:
                            row[key] = value
                if row:
                    rows.append(row)
            append_dict_sheet(workbook, "follower_unsupported_suites", rows)

    def _summarize_kinematics(samples: List[dict]) -> List[dict]:
        aggregates: dict[str, dict[str, float]] = {}
        for sample in samples:
            kind = str(sample.get("kind") or "").lower()
            if kind != "kinematics":
                continue
            suite = str(sample.get("suite") or "unknown").strip() or "unknown"
            bucket = aggregates.setdefault(
                suite,
                {
                    "count": 0.0,
                    "pfc_sum": 0.0,
                    "pfc_max": 0.0,
                    "speed_sum": 0.0,
                    "speed_max": 0.0,
                    "altitude_min": float("inf"),
                    "altitude_max": float("-inf"),
                },
            )

            pfc = _as_float(sample.get("predicted_flight_constraint_w"))
            speed = _as_float(sample.get("speed_mps"))
            altitude = _as_float(sample.get("altitude_m"))

            bucket["count"] += 1.0
            if pfc is not None:
                bucket["pfc_sum"] += pfc
                bucket["pfc_max"] = max(bucket["pfc_max"], pfc)
            if speed is not None:
                bucket["speed_sum"] += speed
                bucket["speed_max"] = max(bucket["speed_max"], speed)
            if altitude is not None:
                bucket["altitude_min"] = min(bucket["altitude_min"], altitude)
                bucket["altitude_max"] = max(bucket["altitude_max"], altitude)

        summary_rows: List[dict] = []
        for suite, data in sorted(aggregates.items()):
            count = max(1.0, data["count"])
            altitude_min = "" if math.isinf(data["altitude_min"]) else data["altitude_min"]
            altitude_max = "" if math.isinf(data["altitude_max"]) else data["altitude_max"]
            summary_rows.append(
                {
                    "suite": suite,
                    "samples": int(data["count"]),
                    "pfc_avg_w": _rounded(data["pfc_sum"] / count, 3),
                    "pfc_max_w": _rounded(data["pfc_max"], 3),
                    "speed_avg_mps": _rounded(data["speed_sum"] / count, 3),
                    "speed_max_mps": _rounded(data["speed_max"], 3),
                    "altitude_min_m": _rounded(altitude_min, 3) if altitude_min != "" else "",
                    "altitude_max_m": _rounded(altitude_max, 3) if altitude_max != "" else "",
                }
            )
        return summary_rows

    kinematics_summary = _summarize_kinematics(telemetry_samples)
    append_dict_sheet(workbook, "kinematics_summary", kinematics_summary)

    paper_header = [
        "suite",
        "rekey_ms",
        "blackout_ms",
        "gap_p99_ms",
        "goodput_mbps",
        "loss_pct",
        "rtt_p50_ms",
        "rtt_p95_ms",
        "owd_p50_ms",
        "owd_p95_ms",
        "power_avg_w",
        "power_energy_j",
    ]
    paper_sheet = workbook.create_sheet("paper_tables")
    paper_sheet.append(paper_header)
    ordered_rows: "OrderedDict[str, dict]" = OrderedDict()
    for row in summary_rows:
        suite_name = str(row.get("suite") or "").strip()
        if not suite_name:
            continue
        ordered_rows[suite_name] = row
    paper_rows = list(ordered_rows.items())
    for suite_name, source_row in paper_rows:
        paper_sheet.append([
            suite_name,
            _rounded(source_row.get("rekey_ms"), 3),
            _rounded(source_row.get("blackout_ms"), 3),
            _rounded(source_row.get("gap_p99_ms"), 3),
            _rounded(source_row.get("goodput_mbps"), 3),
            _rounded(source_row.get("loss_pct"), 3),
            _rounded(source_row.get("rtt_p50_ms"), 3),
            _rounded(source_row.get("rtt_p95_ms"), 3),
            _rounded(source_row.get("owd_p50_ms"), 3),
            _rounded(source_row.get("owd_p95_ms"), 3),
            _rounded(source_row.get("power_avg_w"), 6),
            _rounded(source_row.get("power_energy_j"), 6),
        ])

    notes_header = [
        "generated_utc",
        "session_id",
        "traffic_mode",
        "payload_bytes",
        "event_sample",
        "min_delay_samples",
        "pre_gap_s",
        "duration_s",
        "inter_gap_s",
        "sat_search",
        "sat_delivery_threshold",
        "sat_loss_threshold_pct",
        "sat_rtt_spike_factor",
    ]
    notes_sheet = workbook.create_sheet("paper_notes")
    notes_sheet.append(notes_header)
    notes_sheet.append([
        ts(),
        session_id,
        traffic_mode,
        payload_bytes,
        event_sample,
        min_delay_samples,
        round(pre_gap_s, 3),
        round(duration_s, 3),
        round(inter_gap_s, 3),
        sat_search,
        sat_delivery_threshold,
        sat_loss_threshold_pct,
        sat_rtt_spike_factor,
    ])

    if SUMMARY_CSV.exists():
        append_csv_sheet(workbook, SUMMARY_CSV, "gcs_summary_csv")

    if drone_session_dir is None:
        drone_session_dir = locate_drone_session_dir(session_id)
    if drone_session_dir:
        info_sheet.append(["drone_session_dir", str(drone_session_dir)])
        for csv_path in sorted(drone_session_dir.glob("*.csv")):
            append_csv_sheet(workbook, csv_path, csv_path.stem[:31])
    else:
        info_sheet.append(["drone_session_dir", "not_found"])

    if paper_rows and BarChart is not None and Reference is not None:
        row_count = len(paper_rows) + 1
        suite_categories = Reference(paper_sheet, min_col=1, min_row=2, max_row=row_count)

        rekey_chart = BarChart()
        rekey_chart.title = "Rekey vs Blackout (ms)"
        rekey_chart.add_data(
            Reference(paper_sheet, min_col=2, max_col=3, min_row=1, max_row=row_count),
            titles_from_data=True,
        )
        rekey_chart.set_categories(suite_categories)
        rekey_chart.y_axis.title = "Milliseconds"
        rekey_chart.x_axis.title = "Suite"
        paper_sheet.add_chart(rekey_chart, "H2")

        power_chart = BarChart()
        power_chart.title = "Avg Power (W)"
        power_chart.add_data(
            Reference(paper_sheet, min_col=11, max_col=11, min_row=1, max_row=row_count),
            titles_from_data=True,
        )
        power_chart.set_categories(suite_categories)
        power_chart.y_axis.title = "Watts"
        power_chart.x_axis.title = "Suite"
        paper_sheet.add_chart(power_chart, "H18")

    if summary_rows and LineChart is not None and Reference is not None and "gcs_summary" in workbook.sheetnames:
        summary_sheet = workbook["gcs_summary"]
        header_row = next(summary_sheet.iter_rows(min_row=1, max_row=1, values_only=True), None)
        if header_row:
            try:
                pass_col = header_row.index("pass") + 1
                throughput_col = header_row.index("throughput_mbps") + 1
            except ValueError:
                pass_col = throughput_col = None
            if pass_col and throughput_col and len(summary_rows) >= 1:
                chart = LineChart()
                chart.title = "Throughput (Mb/s) vs pass index"
                chart.add_data(
                    Reference(
                        summary_sheet,
                        min_col=throughput_col,
                        min_row=1,
                        max_row=len(summary_rows) + 1,
                    ),
                    titles_from_data=True,
                )
                chart.set_categories(
                    Reference(
                        summary_sheet,
                        min_col=pass_col,
                        min_row=2,
                        max_row=len(summary_rows) + 1,
                    )
                )
                chart.x_axis.title = "Pass"
                chart.y_axis.title = "Throughput (Mb/s)"
                summary_sheet.add_chart(chart, "L2")

    combined_root = resolve_under_root(COMBINED_OUTPUT_DIR)
    combined_dir = combined_root / session_id
    combined_dir.mkdir(parents=True, exist_ok=True)
    info_sheet.append(["gcs_session_dir", str(combined_dir)])
    target_path = combined_dir / f"{session_id}_combined.xlsx"
    for attempt in range(3):
        try:
            buffer = io.BytesIO()
            workbook.save(buffer)
            _atomic_write_bytes(target_path, buffer.getvalue())
            return target_path
        except Exception as exc:  # pragma: no cover - platform specific
            if attempt == 2:
                print(f"[WARN] failed to write combined workbook {target_path}: {exc}", file=sys.stderr)
            time.sleep(0.1)
    return None


def main(argv: Optional[Iterable[str]] = None) -> None:
    args = _parse_cli_args(argv)
    log_runtime_environment("gcs_scheduler")
    OUTDIR.mkdir(parents=True, exist_ok=True)
    SUITES_OUTDIR.mkdir(parents=True, exist_ok=True)
    PROXY_STATUS_PATH.parent.mkdir(parents=True, exist_ok=True)
    PROXY_SUMMARY_PATH.parent.mkdir(parents=True, exist_ok=True)

    if args.post_fetch_only:
        session_id = args.post_fetch_only.strip()
        if not session_id:
            print("[WARN] --post-fetch-only requires a non-empty session id", file=sys.stderr)
            return
        print(f"[{ts()}] post_fetch requested for session {session_id}")
        _post_run_fetch_artifacts(session_id=session_id)
        if args.generate_report:
            session_dir = OUTDIR / session_id
            session_dir.mkdir(parents=True, exist_ok=True)
            _post_run_generate_reports(session_id=session_id, session_dir=session_dir)
        print(f"[{ts()}] post_fetch completed for session {session_id}")
        return

    auto = AUTO_GCS_CONFIG

    # Fast verification mode: shorten duration and disable power capture
    # Precedence: explicit --duration-s overrides verification mode
    if getattr(args, "duration_s", None):
        requested = float(getattr(args, "duration_s"))
        if requested <= 0:
            print(f"[WARN] ignoring non-positive --duration-s={requested}", file=sys.stderr)
        else:
            auto["duration_s"] = requested
            # Disable power capture automatically for very short runs (<12s)
            if requested < 12.0:
                auto["power_capture"] = False
            print(f"[{ts()}] OVERRIDE duration_s={requested:.2f}s power_capture={'enabled' if auto.get('power_capture', True) else 'disabled'}")
    elif getattr(args, "verify", False):
        print(f"[{ts()}] VERIFICATION MODE ENABLED: forcing duration=10s, power_capture=False")
        auto["duration_s"] = 10.0
        auto["power_capture"] = False

    traffic_mode = str(auto.get("traffic") or "blast").lower()
    traffic_engine = str(auto.get("traffic_engine") or "native").lower()
    iperf3_config = auto.get("iperf3") or {}
    if not isinstance(iperf3_config, dict):
        iperf3_config = {}
    if traffic_engine not in {"native", "iperf3"}:
        print(
            f"[WARN] unsupported traffic_engine={traffic_engine}; defaulting to native",
            file=sys.stderr,
        )
        traffic_engine = "native"
    pre_gap = float(auto.get("pre_gap_s") or 1.0)
    inter_gap = float(auto.get("inter_gap_s") or 15.0)
    duration = float(auto.get("duration_s") or 15.0)
    payload_bytes = int(auto.get("payload_bytes") or 256)
    configured_event_sample = int(auto.get("event_sample") or 100)
    event_sample = max(0, configured_event_sample)
    passes = int(auto.get("passes") or 1)
    rate_pps = int(auto.get("rate_pps") or 0)
    bandwidth_mbps = float(auto.get("bandwidth_mbps") or 0.0)
    constant_rate_defaulted = False
    max_rate_mbps = float(auto.get("max_rate_mbps") or 200.0)
    if traffic_mode == "constant" and bandwidth_mbps <= 0 and rate_pps <= 0:
        bandwidth_mbps = CONSTANT_RATE_MBPS_DEFAULT
        constant_rate_defaulted = True
    if bandwidth_mbps > 0:
        denominator = max(payload_bytes * 8, 1)
        rate_pps = max(1, int((bandwidth_mbps * 1_000_000) / denominator))
    if traffic_mode == "constant" and rate_pps <= 0:
        raise ValueError("AUTO_GCS.rate_pps or bandwidth_mbps must be positive for constant traffic")

    sat_search_cfg = str(auto.get("sat_search") or SATURATION_SEARCH_MODE).lower()
    if sat_search_cfg not in {"auto", "linear", "bisect"}:
        sat_search_cfg = SATURATION_SEARCH_MODE
    sat_delivery_threshold = float(auto.get("sat_delivery_threshold") or SATURATION_DELIVERY_THRESHOLD)
    sat_loss_threshold = float(auto.get("sat_loss_threshold_pct") or SATURATION_LOSS_THRESHOLD)
    sat_spike_factor = float(auto.get("sat_rtt_spike_factor") or SATURATION_RTT_SPIKE)

    min_delay_samples = MIN_DELAY_SAMPLES

    if duration <= 0:
        raise ValueError("AUTO_GCS.duration_s must be positive")
    if pre_gap < 0:
        raise ValueError("AUTO_GCS.pre_gap_s must be >= 0")
    if inter_gap < 0:
        raise ValueError("AUTO_GCS.inter_gap_s must be >= 0")
    if rate_pps < 0:
        raise ValueError("AUTO_GCS.rate_pps must be >= 0")
    if passes <= 0:
        raise ValueError("AUTO_GCS.passes must be >= 1")

    if traffic_mode not in {"blast", "constant", "mavproxy", "saturation"}:
        raise ValueError(f"Unsupported traffic mode: {traffic_mode}")

    constant_target_bandwidth_mbps = 0.0
    if traffic_mode == "constant":
        if bandwidth_mbps > 0:
            constant_target_bandwidth_mbps = bandwidth_mbps
        elif rate_pps > 0:
            constant_target_bandwidth_mbps = (rate_pps * payload_bytes * 8) / 1_000_000
    run_target_bandwidth_mbps = (
        constant_target_bandwidth_mbps if traffic_mode == "constant" else max(0.0, bandwidth_mbps)
    )

    suites_override = auto.get("suites")
    suites = resolve_suites(suites_override)
    suites = _apply_nist_level_filter(suites, args)
    exclude_tokens_raw = auto.get("aead_exclude_tokens") or []
    exclude_tokens: Set[str] = set()
    if isinstance(exclude_tokens_raw, str):
        candidate_iter = [exclude_tokens_raw]
    elif isinstance(exclude_tokens_raw, (list, tuple, set)):
        candidate_iter = exclude_tokens_raw
    else:
        candidate_iter = []
    for token in candidate_iter:
        if not isinstance(token, str):
            continue
        token_norm = token.strip().lower()
        if token_norm:
            exclude_tokens.add(token_norm)
    if exclude_tokens:
        filtered_suites: List[str] = []
        excluded_records: List[Tuple[str, str]] = []
        for suite_id in suites:
            try:
                suite_info = suites_mod.get_suite(suite_id)
                token = str(suite_info.get("aead_token") or "").strip().lower()
            except Exception:
                token = ""
            if token and token in exclude_tokens:
                excluded_records.append((suite_id, token))
                continue
            filtered_suites.append(suite_id)
        if excluded_records:
            for suite_id, token in excluded_records:
                print(
                    f"[WARN] excluding suite {suite_id}: AEAD token '{token}' blocked via AUTO_GCS.aead_exclude_tokens",
                    file=sys.stderr,
                )
        suites = filtered_suites
    if not suites:
        raise RuntimeError("No suites selected for execution")

    suites, preflight_skips = preflight_filter_suites(suites)
    if preflight_skips:
        for entry in preflight_skips:
            suite_label = entry.get("suite")
            reason_label = entry.get("reason")
            detail_payload = entry.get("details") or {}
            detail_hint = ""
            if isinstance(detail_payload, dict) and detail_payload:
                parts: List[str] = []
                hint_text = detail_payload.get("aead_hint")
                if hint_text:
                    parts.append(str(hint_text))
                for key in ("kem_name", "sig_name", "aead_token"):
                    val = detail_payload.get(key)
                    if val:
                        parts.append(f"{key}={val}")
                if parts:
                    detail_hint = f" ({'; '.join(parts)})"
            print(
                f"[WARN] filtering out suite {suite_label}: {reason_label}{detail_hint}",
                file=sys.stderr,
            )
        print(
            "[INFO] Run `python tools/verify_crypto.py` for a full availability report.",
            file=sys.stderr,
        )
    if not suites:
        raise RuntimeError("No suites remain after preflight capability filtering")

    follower_capabilities: Dict[str, object] = {}
    follower_capability_skips: List[Dict[str, object]] = []
    follower_capabilities_path: Optional[Path] = None

    session_prefix = str(auto.get("session_prefix") or "session")
    env_session_id = os.environ.get("GCS_SESSION_ID")
    session_id = env_session_id or f"{session_prefix}_{int(time.time())}"
    session_source = "env" if env_session_id else "generated"

    power_capture_enabled = bool(auto.get("power_capture", True))

    telemetry_enabled = bool(auto.get("telemetry_enabled", True))
    telemetry_target_host_cfg = auto.get("telemetry_target_host")
    telemetry_target_host = str(telemetry_target_host_cfg or "").strip()
    if not telemetry_target_host:
        bind_candidate = str(auto.get("telemetry_bind_host") or "").strip()
        if bind_candidate and bind_candidate not in {"0.0.0.0", "::", "*"}:
            telemetry_target_host = bind_candidate
    if not telemetry_target_host:
        telemetry_target_host = DRONE_HOST
    telemetry_port_cfg = auto.get("telemetry_port")
    telemetry_port = TELEMETRY_PORT if telemetry_port_cfg in (None, "") else int(telemetry_port_cfg)

    print(
        f"[{ts()}] traffic={traffic_mode} duration={duration:.1f}s pre_gap={pre_gap:.1f}s "
        f"inter_gap={inter_gap:.1f}s payload={payload_bytes}B event_sample={event_sample} passes={passes} "
        f"rate_pps={rate_pps} sat_search={sat_search_cfg}"
    )
    if traffic_mode == "constant":
        target_msg = f"[{ts()}] constant-rate target {constant_target_bandwidth_mbps:.2f} Mbps (~{rate_pps} pps)"
        if constant_rate_defaulted:
            target_msg += " [default]"
        print(target_msg)
    elif bandwidth_mbps > 0:
        print(f"[{ts()}] bandwidth target {bandwidth_mbps:.2f} Mbps -> approx {rate_pps} pps")
    print(f"[{ts()}] power capture: {'enabled' if power_capture_enabled else 'disabled'}")

    reachable = False
    for attempt in range(8):
        try:
            resp = ctl_send({"cmd": "ping"}, timeout=1.0, retries=1)
            if resp.get("ok"):
                reachable = True
                break
        except Exception:
            pass
        time.sleep(0.5)
    follower_session_id: Optional[str] = None
    if reachable:
        print(f"[{ts()}] follower reachable at {DRONE_HOST}:{CONTROL_PORT}")
        try:
            session_resp = ctl_send({"cmd": "session_info"}, timeout=1.2, retries=2, backoff=0.3)
            if session_resp.get("ok"):
                candidate = str(session_resp.get("session_id") or "").strip()
                if candidate:
                    follower_session_id = candidate
        except Exception as exc:
            print(f"[WARN] session_info fetch failed: {exc}", file=sys.stderr)
        try:
            caps_resp = ctl_send({"cmd": "capabilities"}, timeout=1.5, retries=2, backoff=0.4)
            if caps_resp.get("ok"):
                raw_caps = caps_resp.get("capabilities") or {}
                if isinstance(raw_caps, dict):
                    follower_capabilities = dict(raw_caps)
                    suites_filtered, follower_capability_skips = filter_suites_for_follower(suites, follower_capabilities)
                    suites = suites_filtered
                    print(
                        f"[{ts()}] follower reports {len(follower_capabilities.get('supported_suites') or [])} supported suites",
                        flush=True,
                    )
                    kem_list = follower_capabilities.get("enabled_kems")
                    if isinstance(kem_list, (list, tuple, set)):
                        kem_display = ", ".join(str(item) for item in kem_list)
                        print(f"[{ts()}] follower KEMs -> {kem_display}")
                    sig_list = follower_capabilities.get("enabled_sigs")
                    if isinstance(sig_list, (list, tuple, set)):
                        sig_display = ", ".join(str(item) for item in sig_list)
                        print(f"[{ts()}] follower signatures -> {sig_display}")
                    aead_list = follower_capabilities.get("available_aeads")
                    if isinstance(aead_list, (list, tuple, set)):
                        aead_display = ", ".join(str(item) for item in aead_list)
                        print(f"[{ts()}] follower AEAD tokens -> {aead_display}")
                    missing_kems = follower_capabilities.get("missing_kems")
                    if missing_kems:
                        print(f"[WARN] follower missing KEMs: {missing_kems}", file=sys.stderr)
                    missing_sigs = follower_capabilities.get("missing_sigs")
                    if missing_sigs:
                        print(f"[WARN] follower missing signatures: {missing_sigs}", file=sys.stderr)
                    missing_aeads = follower_capabilities.get("missing_aead_reasons")
                    if not missing_aeads:
                        missing_aeads = follower_capabilities.get("missing_aeads")
                    if missing_aeads:
                        print(f"[WARN] follower missing AEADs: {missing_aeads}", file=sys.stderr)
                else:
                    print(
                        f"[WARN] follower capabilities response malformed: {type(raw_caps).__name__}",
                        file=sys.stderr,
                    )
            else:
                print("[WARN] follower capabilities request failed (no ok flag)", file=sys.stderr)
        except Exception as exc:
            print(f"[WARN] capabilities fetch failed: {exc}", file=sys.stderr)
    else:
        print(f"[WARN] follower not reachable at {DRONE_HOST}:{CONTROL_PORT}", file=sys.stderr)

    if follower_session_id:
        if env_session_id and follower_session_id != env_session_id:
            print(
                f"[WARN] follower session_id={follower_session_id} disagrees with GCS_SESSION_ID={env_session_id}; using env override",
                file=sys.stderr,
            )
        else:
            session_id = follower_session_id
            session_source = "drone"

    print(f"[{ts()}] session_id={session_id} (source={session_source})")
    os.environ["GCS_SESSION_ID"] = session_id

    if follower_capability_skips:
        for entry in follower_capability_skips:
            suite_label = entry.get("suite")
            reason_label = entry.get("reason")
            print(
                f"[WARN] follower rejects suite {suite_label}: {reason_label}",
                file=sys.stderr,
            )
    if follower_capabilities:
        try:
            session_cap_dir = OUTDIR / session_id
            session_cap_dir.mkdir(parents=True, exist_ok=True)
            follower_capabilities_path = session_cap_dir / "follower_capabilities.json"
            data_bytes = json.dumps(follower_capabilities, indent=2, sort_keys=True).encode("utf-8")
            _atomic_write_bytes(follower_capabilities_path, data_bytes)
            print(f"[{ts()}] follower capabilities snapshot -> {follower_capabilities_path}")
        except Exception as exc:
            follower_capabilities_path = None
            print(f"[WARN] failed to persist follower capabilities: {exc}", file=sys.stderr)

    if not suites:
        raise RuntimeError("No suites remain after follower capability filtering")

    initial_suite = preferred_initial_suite(suites)
    if initial_suite and suites[0] != initial_suite:
        suites = [initial_suite] + [s for s in suites if s != initial_suite]
        print(f"[{ts()}] reordered suites to start with {initial_suite} (from CONFIG)")

    drone_session_dir = locate_drone_session_dir(session_id)
    if drone_session_dir:
        print(f"[{ts()}] follower session dir -> {drone_session_dir}")
    else:
        print(f"[WARN] follower session dir missing for session {session_id}", file=sys.stderr)

    session_excel_dir = resolve_under_root(EXCEL_OUTPUT_DIR) / session_id

    offset_ns = 0
    offset_warmup_s = 0.0
    try:
        sync = timesync()
        offset_ns = sync["offset_ns"]
        print(f"[{ts()}] clocks synced: offset_ns={offset_ns} ns, link_rtt~{sync['rtt_ns']} ns")
        if abs(offset_ns) > CLOCK_OFFSET_THRESHOLD_NS:
            offset_warmup_s = 1.0
            print(
                f"[WARN] clock offset {offset_ns / 1_000_000:.1f} ms exceeds {CLOCK_OFFSET_THRESHOLD_NS / 1_000_000:.1f} ms; extending warmup",
                file=sys.stderr,
            )
            print(
                f"[{ts()}] clock skew banner: |offset|={offset_ns / 1_000_000:.1f} ms -> first measurement pass may be noisy",
                flush=True,
            )
    except Exception as exc:
        print(f"[WARN] timesync failed: {exc}", file=sys.stderr)

    telemetry_collector: Optional[TelemetryCollector] = None
    if telemetry_enabled:
        telemetry_collector = TelemetryCollector(telemetry_target_host, telemetry_port)
        telemetry_collector.start()
        print(f"[{ts()}] telemetry subscriber -> {telemetry_target_host}:{telemetry_port}")
    else:
        print(f"[{ts()}] telemetry collector disabled via AUTO_GCS configuration")

    if not bool(auto.get("launch_proxy", True)):
        raise NotImplementedError("AUTO_GCS.launch_proxy=False is not supported")

    gcs_proc: Optional[subprocess.Popen] = None
    log_handle = None
    gcs_log_path: Optional[Path] = None
    gcs_proc, log_handle, gcs_log_path = start_gcs_proxy(suites[0])
    combined_path: Optional[Path] = None

    try:
        ready = wait_handshake(timeout=20.0)
        print(f"[{ts()}] initial handshake ready? {ready}")

        summary_rows: List[dict] = []
        saturation_reports: List[dict] = []
        all_rate_samples: List[dict] = []
        telemetry_samples: List[dict] = []

        if traffic_mode == "saturation":
            for idx, suite in enumerate(suites):
                try:
                    rekey_ms, rekey_mark_ns, rekey_ok_ns = activate_suite(
                        gcs_proc,
                        suite,
                        is_first=(idx == 0),
                        gcs_log_handle=log_handle,
                        gcs_log_path=gcs_log_path,
                    )
                except SuiteSkipped as exc:
                    print(f"[WARN] skipping suite {suite}: {exc}", file=sys.stderr)
                    _log_event({
                        "suite": suite,
                        "phase": "skipped",
                        "error_code": "suite_skipped",
                        "message": str(exc),
                        "remediation_hint": "Verify follower capabilities or regenerate keys"
                    })
                    _append_suite_text(suite, f"[{ts()}] SKIPPED suite={suite} reason={exc}")
                    if inter_gap > 0 and idx < len(suites) - 1:
                        time.sleep(inter_gap)
                    continue
                outdir = suite_outdir(suite)
                tester = SaturationTester(
                    suite=suite,
                    payload_bytes=payload_bytes,
                    duration_s=duration,
                    event_sample=event_sample,
                    offset_ns=offset_ns,
                    output_dir=outdir,
                    max_rate_mbps=int(max_rate_mbps),
                    search_mode=sat_search_cfg,
                    delivery_threshold=sat_delivery_threshold,
                    loss_threshold=sat_loss_threshold,
                    spike_factor=sat_spike_factor,
                    min_delay_samples=min_delay_samples,
                )
                summary = tester.run()
                summary["rekey_ms"] = rekey_ms
                if rekey_mark_ns is not None:
                    summary["rekey_mark_ns"] = rekey_mark_ns
                if rekey_ok_ns is not None:
                    summary["rekey_ok_ns"] = rekey_ok_ns
                excel_path = tester.export_excel(session_id, session_excel_dir)
                if excel_path:
                    summary["excel_path"] = str(excel_path)
                saturation_reports.append(summary)
                all_rate_samples.extend(dict(record) for record in tester.records)
                if inter_gap > 0 and idx < len(suites) - 1:
                    time.sleep(inter_gap)
            report_path = OUTDIR / f"saturation_summary_{session_id}.json"
            summary_bytes = json.dumps(saturation_reports, indent=2).encode("utf-8")
            try:
                _atomic_write_bytes(report_path, summary_bytes)
                print(f"[{ts()}] saturation summary written to {report_path}")
            except Exception as exc:
                print(f"[WARN] failed to update {report_path}: {exc}", file=sys.stderr)
        else:
            for pass_index in range(passes):
                for idx, suite in enumerate(suites):
                    try:
                        row = run_suite(
                            gcs_proc,
                            suite,
                            is_first=(pass_index == 0 and idx == 0),
                            duration_s=duration,
                            payload_bytes=payload_bytes,
                            event_sample=event_sample,
                            offset_ns=offset_ns,
                            pass_index=pass_index,
                            traffic_mode=traffic_mode,
                            traffic_engine=traffic_engine,
                            iperf3_config=iperf3_config,
                            pre_gap=pre_gap,
                            inter_gap_s=inter_gap,
                            rate_pps=rate_pps,
                            target_bandwidth_mbps=run_target_bandwidth_mbps,
                            power_capture_enabled=power_capture_enabled,
                            clock_offset_warmup_s=offset_warmup_s,
                            min_delay_samples=min_delay_samples,
                            telemetry_collector=telemetry_collector,
                            gcs_log_handle=log_handle,
                            gcs_log_path=gcs_log_path,
                        )
                    except SuiteSkipped as exc:
                        print(f"[WARN] skipping suite {suite}: {exc}", file=sys.stderr)
                        _log_event({
                            "suite": suite,
                            "phase": "skipped",
                            "pass_index": pass_index,
                            "error_code": "suite_skipped",
                            "message": str(exc),
                            "remediation_hint": "Verify follower capabilities or regenerate keys"
                        })
                        _append_suite_text(suite, f"[{ts()}] SKIPPED suite={suite} pass={pass_index} reason={exc}")
                        is_last_suite = idx == len(suites) - 1
                        is_last_pass = pass_index == passes - 1
                        if inter_gap > 0 and not (is_last_suite and is_last_pass):
                            time.sleep(inter_gap)
                        continue
                    except Exception as exc:
                        # Unexpected failure (handshake/rekey/traffic). Log and continue.
                        print(f"[ERROR] suite {suite} failed: {exc}", file=sys.stderr)
                        _log_event({
                            "suite": suite,
                            "phase": "failure",
                            "pass_index": pass_index,
                            "error_code": "suite_failure",
                            "message": str(exc),
                            "remediation_hint": "Check proxy logs, confirm follower running, inspect keys"
                        })
                        _append_suite_text(suite, f"[{ts()}] FAILURE suite={suite} pass={pass_index} error={exc}")
                        is_last_suite = idx == len(suites) - 1
                        is_last_pass = pass_index == passes - 1
                        if inter_gap > 0 and not (is_last_suite and is_last_pass):
                            time.sleep(inter_gap)
                        continue
                    summary_rows.append(row)
                    is_last_suite = idx == len(suites) - 1
                    is_last_pass = pass_index == passes - 1
                    if inter_gap > 0 and not (is_last_suite and is_last_pass):
                        time.sleep(inter_gap)

            if summary_rows:
                blackout_records, step_payloads = _enrich_summary_rows(
                    summary_rows,
                    session_id=session_id,
                    drone_session_dir=drone_session_dir,
                    traffic_mode=traffic_mode,
                    pre_gap_s=pre_gap,
                    duration_s=duration,
                    inter_gap_s=inter_gap,
                )
                _append_blackout_records(blackout_records)
                _append_step_results(step_payloads)

            write_summary(summary_rows)

        if telemetry_collector and telemetry_collector.enabled:
            telemetry_samples = telemetry_collector.snapshot()

        if auto.get("export_combined_excel", True):
            combined_path = export_combined_excel(
                session_id=session_id,
                summary_rows=summary_rows,
                saturation_overview=saturation_reports,
                saturation_samples=all_rate_samples,
                telemetry_samples=telemetry_samples,
                drone_session_dir=drone_session_dir,
                follower_capabilities=follower_capabilities,
                follower_capabilities_path=follower_capabilities_path,
                traffic_mode=traffic_mode,
                payload_bytes=payload_bytes,
                event_sample=event_sample,
                min_delay_samples=min_delay_samples,
                pre_gap_s=pre_gap,
                duration_s=duration,
                inter_gap_s=inter_gap,
                sat_search=sat_search_cfg,
                sat_delivery_threshold=sat_delivery_threshold,
                sat_loss_threshold_pct=sat_loss_threshold,
                sat_rtt_spike_factor=sat_spike_factor,
            )
            if combined_path:
                print(f"[{ts()}] combined workbook written to {combined_path}")

    finally:
        try:
            ctl_send({"cmd": "stop"})
        except Exception:
            pass

        if gcs_proc and gcs_proc.stdin:
            try:
                gcs_proc.stdin.write("quit\n")
                gcs_proc.stdin.flush()
            except Exception:
                pass
        if gcs_proc:
            try:
                gcs_proc.wait(timeout=5)
            except Exception:
                gcs_proc.kill()

        if log_handle:
            try:
                log_handle.close()
            except Exception:
                pass

        session_dir = _post_run_collect_local(
            session_id,
            gcs_log_path=gcs_log_path,
            combined_workbook=combined_path,
        )
        _post_run_generate_reports(session_id, session_dir=session_dir)
        _post_run_fetch_artifacts(session_id=session_id)

        if telemetry_collector:
            telemetry_collector.stop()


if __name__ == "__main__":
    # Test plan:
    # 1. Launch the scheduler with the follower running; verify telemetry collector binds and follower connects.
    # 2. Exercise multiple suites to confirm rekey waits for follower confirmation and no failed rekeys occur.
    # 3. Delete output directories before a run to ensure the scheduler recreates all paths automatically.
    # 4. Stop the telemetry collector briefly and confirm the follower reconnects without aborting the run.
    main()

==================================================

core\aead.py
==================================================
"""
AEAD framing for PQC drone-GCS secure proxy.

Provides authenticated encryption (AES-256-GCM) with wire header bound as AAD,
deterministic 96-bit counter IVs, sliding replay window, and epoch support for rekeys.
"""

import struct
from dataclasses import dataclass
from typing import Optional, Tuple

from cryptography.hazmat.primitives.ciphers.aead import AESGCM
try:
    from cryptography.hazmat.primitives.ciphers.aead import ChaCha20Poly1305
except ImportError:  # pragma: no cover - ChaCha unavailable on very old crypto builds
    ChaCha20Poly1305 = None
from cryptography.exceptions import InvalidTag

try:  # pragma: no cover - native extension optional
    from core import _ascon_native as _ascon_native_module
except Exception:  # pragma: no cover - extension not built or unavailable
    _ascon_native_module = None

try:  # pragma: no cover - pure python fallback
    import pyascon as _pyascon_module  # type: ignore
except Exception:  # pragma: no cover - not available
    _pyascon_module = None

from core.config import CONFIG
from core.exceptions import SequenceOverflow, AeadError


_SUPPORTED_AEAD_TOKENS = {"aesgcm", "chacha20poly1305", "ascon128a"}
_RETIRED_AEAD_TOKENS = {
    "aes128gcm": "use aesgcm (AES-256-GCM) for final deployments",
    "ascon128": "use ascon128a (native C backend) for MTU-scale support",
}


# Exception types
class HeaderMismatch(Exception):
    """Header validation failed (version, IDs, or session_id mismatch)."""
    pass


class AeadAuthError(Exception):
    """AEAD authentication failed during decryption."""
    pass


class ReplayError(Exception):
    """Packet replay detected or outside acceptable window."""
    pass


# Constants
HEADER_STRUCT = "!BBBBB8sQB"
# Compute header length from structure to avoid drift when struct changes.
HEADER_LEN = struct.calcsize(HEADER_STRUCT)
# IV is still logically 12 bytes (1 epoch + 11 seq bytes) but is NO LONGER transmitted on wire.
# Wire format: header(22) || ciphertext+tag
IV_LEN = 0  # length of IV bytes present on wire (0 after optimization)




def _canonicalize_aead_token(token: str) -> str:
    candidate = token.lower()
    if candidate in _RETIRED_AEAD_TOKENS:
        raise ValueError(f"AEAD token '{token}' is retired: {_RETIRED_AEAD_TOKENS[candidate]}")
    if candidate not in _SUPPORTED_AEAD_TOKENS:
        raise ValueError(f"unknown AEAD token: {token}")
    return candidate


class _AsconAdapter:
    """Ascon adapter backing the 'ascon128a' token with native C fallbacks."""

    def __init__(self, key: bytes, variant: str):
        if len(key) < 16:
            raise ValueError("Ascon requires at least 16 bytes of key material")
        strict = bool(CONFIG.get("ASCON_STRICT_KEY_SIZE", False))
        if strict and len(key) != 16:
            raise ValueError("ASCON_STRICT_KEY_SIZE enabled: key must be exactly 16 bytes")
        self._key = key[:16]
        algo_map = {
            "ascon128": "Ascon-AEAD128",  # legacy alias retained for internal callers
            "ascon128a": "Ascon-AEAD128a",
        }
        self._algo_str = algo_map.get(variant, "Ascon-AEAD128")
        self._fallback_variant = {
            "Ascon-AEAD128": "Ascon-128",
            "Ascon-AEAD128a": "Ascon-128a",
        }.get(self._algo_str, "Ascon-128")
        variant_name = self._algo_str

        if _ascon_native_module is not None and hasattr(_ascon_native_module, "encrypt") and hasattr(_ascon_native_module, "decrypt"):
            def _native_encrypt(
                key_bytes: bytes,
                nonce_bytes: bytes,
                aad_bytes: bytes,
                plaintext_bytes: bytes,
                algo: str = variant_name,
            ) -> bytes:
                return _ascon_native_module.encrypt(
                    key_bytes, nonce_bytes, aad_bytes, plaintext_bytes, algo
                )

            def _native_decrypt(
                key_bytes: bytes,
                nonce_bytes: bytes,
                aad_bytes: bytes,
                ciphertext_bytes: bytes,
                algo: str = variant_name,
            ) -> bytes:
                result = _ascon_native_module.decrypt(
                    key_bytes, nonce_bytes, aad_bytes, ciphertext_bytes, algo
                )
                if result is None:
                    raise InvalidTag("Ascon authentication failed")
                return result

            self._enc = _native_encrypt
            self._dec = _native_decrypt
        elif _pyascon_module is not None:
            def _py_encrypt(
                key_bytes: bytes,
                nonce_bytes: bytes,
                aad_bytes: bytes,
                plaintext_bytes: bytes,
                algo: str = variant_name,
            ) -> bytes:
                return _pyascon_module.ascon_encrypt(key_bytes, nonce_bytes, aad_bytes, plaintext_bytes, algo)

            def _py_decrypt(
                key_bytes: bytes,
                nonce_bytes: bytes,
                aad_bytes: bytes,
                ciphertext_bytes: bytes,
                algo: str = variant_name,
            ) -> bytes:
                result = _pyascon_module.ascon_decrypt(key_bytes, nonce_bytes, aad_bytes, ciphertext_bytes, algo)
                if result is None:
                    raise InvalidTag("Ascon authentication failed")
                return result

            self._enc = _py_encrypt
            self._dec = _py_decrypt
        else:
            raise ImportError("No Ascon backend available (native module missing and pyascon not importable)")

    def encrypt(self, nonce: bytes, plaintext: bytes, aad: bytes) -> bytes:
        if len(nonce) < 16:
            nonce = nonce + b"\x00" * (16 - len(nonce))
        return self._enc(self._key, nonce[:16], aad, plaintext, self._algo_str)

    def decrypt(self, nonce: bytes, ciphertext: bytes, aad: bytes) -> bytes:
        if len(nonce) < 16:
            nonce = nonce + b"\x00" * (16 - len(nonce))
        pt = self._dec(self._key, nonce[:16], aad, ciphertext, self._algo_str)
        if pt is None:
            raise InvalidTag("Ascon authentication failed")
        return pt


def _instantiate_aead(token: str, key: bytes) -> Tuple[object, int]:
    """Return AEAD primitive and required nonce length for the suite token."""

    normalized = _canonicalize_aead_token(token)

    if normalized == "aesgcm":
        if len(key) != 32:
            raise ValueError("AES-256-GCM requires 32-byte key material")
        return AESGCM(key), 12

    if normalized == "chacha20poly1305":
        if ChaCha20Poly1305 is None:
            raise ImportError("ChaCha20-Poly1305 not available in cryptography build")
        if len(key) != 32:
            raise ValueError("ChaCha20-Poly1305 requires 32-byte key material")
        return ChaCha20Poly1305(key), 12

    if normalized == "ascon128a":
        return _AsconAdapter(key, normalized), 16

    raise AeadError(f"unsupported AEAD token: {token}")


def _build_nonce(epoch: int, seq: int, nonce_len: int) -> bytes:
    base = bytes([epoch & 0xFF]) + seq.to_bytes(11, "big")
    if nonce_len == 12:
        return base
    if nonce_len > 12:
        return base + b"\x00" * (nonce_len - 12)
    raise ValueError("nonce length must be >= 12 bytes")


@dataclass(frozen=True)
class AeadIds:
    kem_id: int
    kem_param: int
    sig_id: int
    sig_param: int

    def __post_init__(self):
        for field_name, value in [("kem_id", self.kem_id), ("kem_param", self.kem_param), 
                                  ("sig_id", self.sig_id), ("sig_param", self.sig_param)]:
            if not isinstance(value, int) or not (0 <= value <= 255):
                raise ValueError(f"{field_name} must be int in range 0-255")


@dataclass
class Sender:
    version: int
    ids: AeadIds
    session_id: bytes
    epoch: int
    key_send: bytes
    aead_token: str = "aesgcm"
    _seq: int = 0

    def __post_init__(self):
        if not isinstance(self.version, int) or self.version != CONFIG["WIRE_VERSION"]:
            raise ValueError(f"version must equal CONFIG WIRE_VERSION ({CONFIG['WIRE_VERSION']})")
        
        if not isinstance(self.ids, AeadIds):
            raise TypeError("ids must be AeadIds instance")
        
        if not isinstance(self.session_id, bytes) or len(self.session_id) != 8:
            raise ValueError("session_id must be exactly 8 bytes")
        
        if not isinstance(self.epoch, int) or not (0 <= self.epoch <= 255):
            raise ValueError("epoch must be int in range 0-255")
        
        if not isinstance(self.key_send, bytes):
            raise TypeError("key_send must be bytes")
        
        if not isinstance(self._seq, int) or self._seq < 0:
            raise ValueError("_seq must be non-negative int")

        self._aead_token = _canonicalize_aead_token(self.aead_token)
        self._cipher, self._nonce_len = _instantiate_aead(self._aead_token, self.key_send)

    @property
    def seq(self):
        """Current sequence number."""
        return self._seq

    def pack_header(self, seq: int) -> bytes:
        """Pack header with given sequence number."""
        if not isinstance(seq, int) or seq < 0:
            raise ValueError("seq must be non-negative int")
        
        return struct.pack(
            HEADER_STRUCT,
            self.version,
            self.ids.kem_id,
            self.ids.kem_param, 
            self.ids.sig_id,
            self.ids.sig_param,
            self.session_id,
            seq,
            self.epoch
        )

    def encrypt(self, plaintext: bytes) -> bytes:
        """Encrypt plaintext returning: header || ciphertext + tag.

        Deterministic IV (epoch||seq) is derived locally and NOT sent on wire to
        reduce overhead (saves 12 bytes per packet). Receiver reconstructs it.
        """
        if not isinstance(plaintext, bytes):
            raise TypeError("plaintext must be bytes")
        
        # Proactive rekey threshold to avoid IV exhaustion.
        # Default threshold is 2^63 if not configured; this gives operators time to rekey.
        try:
            threshold = int(CONFIG.get("REKEY_SEQ_THRESHOLD", 1 << 63))
        except Exception:
            threshold = 1 << 63
        if self._seq >= threshold:
            raise SequenceOverflow("approaching IV exhaustion; trigger rekey")
        
        # Pack header with current sequence
        header = self.pack_header(self._seq)

        iv = _build_nonce(self.epoch, self._seq, self._nonce_len)

        try:
            ciphertext = self._cipher.encrypt(iv, plaintext, header)
        except Exception as e:
            raise AeadError(f"AEAD encryption failed: {e}")
        
        # Increment sequence on success
        self._seq += 1
        
        # Return optimized wire format: header || ciphertext+tag (IV omitted)
        return header + ciphertext

    def bump_epoch(self) -> None:
        """Increase epoch and reset sequence.

        Safety policy: forbid wrapping 255->0 with the same key to avoid IV reuse.
        Callers should perform a new handshake to rotate keys before wrap.
        """
        if self.epoch == 255:
            raise AeadError("epoch wrap forbidden without rekey; perform handshake to rotate keys")
        self.epoch += 1
        self._seq = 0


@dataclass
class Receiver:
    version: int
    ids: AeadIds
    session_id: bytes
    epoch: int
    key_recv: bytes
    window: int
    strict_mode: bool = False  # True = raise exceptions, False = return None
    aead_token: str = "aesgcm"
    _high: int = -1
    _mask: int = 0

    def __post_init__(self):
        if not isinstance(self.version, int) or self.version != CONFIG["WIRE_VERSION"]:
            raise ValueError(f"version must equal CONFIG WIRE_VERSION ({CONFIG['WIRE_VERSION']})")
        
        if not isinstance(self.ids, AeadIds):
            raise TypeError("ids must be AeadIds instance")
        
        if not isinstance(self.session_id, bytes) or len(self.session_id) != 8:
            raise ValueError("session_id must be exactly 8 bytes")
        
        if not isinstance(self.epoch, int) or not (0 <= self.epoch <= 255):
            raise ValueError("epoch must be int in range 0-255")
        
        if not isinstance(self.key_recv, bytes):
            raise TypeError("key_recv must be bytes")
        
        if not isinstance(self.window, int) or self.window < 64:
            raise ValueError(f"window must be int >= 64")
        
        if not isinstance(self._high, int):
            raise TypeError("_high must be int")
        
        if not isinstance(self._mask, int) or self._mask < 0:
            raise ValueError("_mask must be non-negative int")

        self._aead_token = _canonicalize_aead_token(self.aead_token)
        self._cipher, self._nonce_len = _instantiate_aead(self._aead_token, self.key_recv)
        self._last_error: Optional[str] = None

    def _check_replay(self, seq: int) -> None:
        """Check if sequence number should be accepted (anti-replay)."""
        if seq > self._high:
            # Future packet - shift window forward
            shift = seq - self._high
            if shift >= self.window:
                # Window completely shifts: reset mask to only include the newest packet
                # Mask bit 0 corresponds to the current highest sequence number
                self._mask = 1
            else:
                # Partial shift
                self._mask = (self._mask << shift) | 1
                # Mask to window size to prevent overflow
                self._mask &= (1 << self.window) - 1
            self._high = seq
        elif seq > self._high - self.window:
            # Within window - check if already seen
            offset = self._high - seq
            bit_pos = offset
            if self._mask & (1 << bit_pos):
                raise ReplayError(f"duplicate packet seq={seq}")
            # Mark as seen
            self._mask |= (1 << bit_pos)
        else:
            # Too old - outside window
            raise ReplayError(f"packet too old seq={seq}, high={self._high}, window={self.window}")

    def decrypt(self, wire: bytes) -> bytes:
        """Validate header, perform anti-replay, reconstruct IV, decrypt.

        Returns plaintext bytes or None (silent mode) on failure.
        """
        if not isinstance(wire, bytes):
            raise ValueError("wire must be bytes")
        
        if len(wire) < HEADER_LEN:
            raise ValueError("wire too short for header")
        
        # Extract header
        header = wire[:HEADER_LEN]
        
        # Unpack and validate header
        try:
            fields = struct.unpack(HEADER_STRUCT, header)
            version, kem_id, kem_param, sig_id, sig_param, session_id, seq, epoch = fields
        except struct.error as e:
            raise ValueError(f"header unpack failed: {e}")
        
        # Validate header fields
        if version != self.version:
            self._last_error = "header"
            if self.strict_mode:
                raise HeaderMismatch(f"version mismatch: expected {self.version}, got {version}")
            return None
        
        if (kem_id, kem_param, sig_id, sig_param) != (self.ids.kem_id, self.ids.kem_param, self.ids.sig_id, self.ids.sig_param):
            self._last_error = "header"
            if self.strict_mode:
                raise HeaderMismatch(f"crypto ID mismatch")
            return None
        
        if session_id != self.session_id:
            self._last_error = "session"
            return None  # Wrong session - always fail silently for security
        
        if epoch != self.epoch:
            self._last_error = "session"
            return None  # Wrong epoch - always fail silently for rekeying
        
        # Check replay protection
        try:
            self._check_replay(seq)
        except ReplayError:
            self._last_error = "replay"
            if self.strict_mode:
                raise
            return None
        
        # Reconstruct deterministic IV instead of reading from wire
        iv = _build_nonce(epoch, seq, self._nonce_len)
        ciphertext = wire[HEADER_LEN:]
        
        # Decrypt with header as AAD
        try:
            plaintext = self._cipher.decrypt(iv, ciphertext, header)
        except InvalidTag:
            self._last_error = "auth"
            if self.strict_mode:
                raise AeadAuthError("AEAD authentication failed")
            return None
        except Exception as e:
            raise AeadError(f"AEAD decryption failed: {e}")
        self._last_error = None
        return plaintext

    def reset_replay(self) -> None:
        """Clear replay protection state."""
        self._high = -1
        self._mask = 0

    def bump_epoch(self) -> None:
        """Increase epoch and reset replay state.
        
        Safety policy: forbid wrapping 255->0 with the same key to avoid IV reuse.
        Callers should perform a new handshake to rotate keys before wrap.
        """
        if self.epoch == 255:
            raise AeadError("epoch wrap forbidden without rekey; perform handshake to rotate keys")
        self.epoch += 1
        self.reset_replay()

    def last_error_reason(self) -> Optional[str]:
        return getattr(self, "_last_error", None)
==================================================

core\async_proxy.py
==================================================
"""
Selectors-based network transport proxy.

Responsibilities:
1. Perform authenticated TCP handshake (PQC KEM + signature) using `core.handshake`.
2. Bridge plaintext UDP <-> encrypted UDP (AEAD framing) both directions.
3. Enforce replay window and per-direction sequence via `core.aead`.

Note: This module uses the low-level `selectors` stdlib facility—not `asyncio`—to
remain dependency-light and fully deterministic for test harnesses. The filename
is retained for backward compatibility; a future refactor may rename it to
`selector_proxy.py` and/or introduce an asyncio variant.
"""

from __future__ import annotations

import hashlib
import json
import queue
import socket
import selectors
import struct
import sys
import threading
import time
from contextlib import contextmanager
from pathlib import Path
from typing import Callable, Dict, Optional, Tuple

from core.config import CONFIG
from core.suites import SUITES, get_suite, header_ids_for_suite, list_suites
try:
    # Optional helper (if you implemented it)
    from core.suites import header_ids_from_names  # type: ignore
except Exception:
    header_ids_from_names = None  # type: ignore

from core.handshake import client_drone_handshake, server_gcs_handshake
from core.exceptions import HandshakeVerifyError, AeadError
from core.logging_utils import get_logger

from core.aead import (
    AeadAuthError,
    AeadIds,
    HeaderMismatch,
    Receiver,
    ReplayError,
    Sender,
)
from core.aead import HEADER_STRUCT as AEAD_HEADER_STRUCT, HEADER_LEN as AEAD_HEADER_LEN
from core.exceptions import ConfigError, SequenceOverflow

from core.policy_engine import (
    ControlResult,
    ControlState,
    coordinator_role_from_config,
    create_control_state,
    handle_control,
    is_coordinator,
    record_rekey_result,
    request_prepare,
    set_coordinator_role,
)

from core.control_tcp import start_control_server_if_enabled

logger = get_logger("pqc")


class ProxyCounters:
    """Simple counters for proxy statistics."""

    def __init__(self) -> None:
        self.ptx_out = 0      # plaintext packets sent out to app
        self.ptx_in = 0       # plaintext packets received from app
        self.enc_out = 0      # encrypted packets sent to peer
        self.enc_in = 0       # encrypted packets received from peer
        self.drops = 0        # total drops
        # Granular drop reasons
        self.drop_replay = 0
        self.drop_auth = 0
        self.drop_header = 0
        self.drop_session_epoch = 0
        self.drop_other = 0
        self.drop_src_addr = 0
        self.rekeys_ok = 0
        self.rekeys_fail = 0
        self.last_rekey_ms = 0
        self.last_rekey_suite: Optional[str] = None
        self.handshake_metrics: Dict[str, object] = {}
        self._primitive_templates = {
            "count": 0,
            "total_ns": 0,
            "min_ns": None,
            "max_ns": 0,
            "total_in_bytes": 0,
            "total_out_bytes": 0,
        }
        self.primitive_metrics: Dict[str, Dict[str, object]] = {
            "aead_encrypt": dict(self._primitive_templates),
            "aead_decrypt_ok": dict(self._primitive_templates),
            "aead_decrypt_fail": dict(self._primitive_templates),
        }

    @staticmethod
    def _ns_to_ms(value: object) -> float:
        try:
            ns = float(value)
        except (TypeError, ValueError):
            return 0.0
        if ns <= 0.0:
            return 0.0
        return round(ns / 1_000_000.0, 6)

    def _part_b_metrics(self) -> Dict[str, object]:
        handshake = self.handshake_metrics
        if not isinstance(handshake, dict) or not handshake:
            return {}

        primitives = handshake.get("primitives") or {}
        if not isinstance(primitives, dict):
            primitives = {}

        kem = primitives.get("kem") if isinstance(primitives.get("kem"), dict) else {}
        sig = primitives.get("signature") if isinstance(primitives.get("signature"), dict) else {}
        artifacts = handshake.get("artifacts") if isinstance(handshake.get("artifacts"), dict) else {}

        summary: Dict[str, object] = {}

        def _emit(prefix: str, source: Dict[str, object], key: str, legacy_key: Optional[str] = None) -> None:
            ns_value = source.get(key)
            ms_value = self._ns_to_ms(ns_value)
            summary[f"{prefix}_max_ms"] = ms_value
            summary[f"{prefix}_avg_ms"] = ms_value
            if legacy_key:
                summary[legacy_key] = ms_value

        _emit("kem_keygen", kem, "keygen_ns", "kem_keygen_ms")
        _emit("kem_encaps", kem, "encap_ns", "kem_encaps_ms")
        _emit("kem_decaps", kem, "decap_ns", "kem_decap_ms")
        _emit("sig_sign", sig, "sign_ns", "sig_sign_ms")
        _emit("sig_verify", sig, "verify_ns", "sig_verify_ms")

        summary["pub_key_size_bytes"] = int(
            kem.get("public_key_bytes")
            or artifacts.get("public_key_bytes")
            or 0
        )
        summary["ciphertext_size_bytes"] = int(kem.get("ciphertext_bytes", 0) or 0)
        summary["sig_size_bytes"] = int(
            sig.get("signature_bytes")
            or artifacts.get("signature_bytes")
            or 0
        )
        summary["shared_secret_size_bytes"] = int(kem.get("shared_secret_bytes", 0) or 0)

        def _avg_ns_for(key: str) -> float:
            stats = self.primitive_metrics.get(key)
            if not isinstance(stats, dict):
                return 0.0
            count = int(stats.get("count", 0) or 0)
            total_ns = int(stats.get("total_ns", 0) or 0)
            if count <= 0 or total_ns <= 0:
                return 0.0
            return total_ns / max(count, 1)

        summary["aead_encrypt_avg_ms"] = self._ns_to_ms(_avg_ns_for("aead_encrypt"))
        summary["aead_decrypt_avg_ms"] = self._ns_to_ms(_avg_ns_for("aead_decrypt_ok"))
        summary["aead_encrypt_ms"] = summary["aead_encrypt_avg_ms"]
        summary["aead_decrypt_ms"] = summary["aead_decrypt_avg_ms"]

        summary["rekey_ms"] = self._ns_to_ms(handshake.get("handshake_total_ns"))

        total_ns = 0
        for key in ("keygen_ns", "encap_ns", "decap_ns"):
            value = kem.get(key)
            if isinstance(value, (int, float)) and value > 0:
                total_ns += int(value)
        for key in ("sign_ns", "verify_ns"):
            value = sig.get(key)
            if isinstance(value, (int, float)) and value > 0:
                total_ns += int(value)
        summary["primitive_total_ms"] = self._ns_to_ms(total_ns)

        return summary

    def to_dict(self) -> Dict[str, object]:
        def _serialize(stats: Dict[str, object]) -> Dict[str, object]:
            return {
                "count": int(stats.get("count", 0) or 0),
                "total_ns": int(stats.get("total_ns", 0) or 0),
                "min_ns": int(stats.get("min_ns") or 0),
                "max_ns": int(stats.get("max_ns", 0) or 0),
                "total_in_bytes": int(stats.get("total_in_bytes", 0) or 0),
                "total_out_bytes": int(stats.get("total_out_bytes", 0) or 0),
            }

        result = {
            "ptx_out": self.ptx_out,
            "ptx_in": self.ptx_in,
            "enc_out": self.enc_out,
            "enc_in": self.enc_in,
            "drops": self.drops,
            "drop_replay": self.drop_replay,
            "drop_auth": self.drop_auth,
            "drop_header": self.drop_header,
            "drop_session_epoch": self.drop_session_epoch,
            "drop_other": self.drop_other,
            "drop_src_addr": self.drop_src_addr,
            "rekeys_ok": self.rekeys_ok,
            "rekeys_fail": self.rekeys_fail,
            "last_rekey_ms": self.last_rekey_ms,
            "last_rekey_suite": self.last_rekey_suite or "",
            "handshake_metrics": self.handshake_metrics,
            "primitive_metrics": {name: _serialize(stats) for name, stats in self.primitive_metrics.items()},
        }

        part_b = self._part_b_metrics()
        if part_b:
            result["part_b_metrics"] = part_b
            for key, value in part_b.items():
                result.setdefault(key, value)

        return result

    def _update_primitive(self, key: str, duration_ns: int, in_bytes: int, out_bytes: int) -> None:
        stats = self.primitive_metrics.setdefault(key, dict(self._primitive_templates))
        stats["count"] = int(stats.get("count", 0) or 0) + 1
        stats["total_ns"] = int(stats.get("total_ns", 0) or 0) + max(0, int(duration_ns))
        current_min = stats.get("min_ns")
        if current_min in (None, 0) or (isinstance(current_min, int) and duration_ns < current_min):
            stats["min_ns"] = max(0, int(duration_ns))
        current_max = stats.get("max_ns", 0) or 0
        if duration_ns > current_max:
            stats["max_ns"] = max(0, int(duration_ns))
        stats["total_in_bytes"] = int(stats.get("total_in_bytes", 0) or 0) + max(0, int(in_bytes))
        stats["total_out_bytes"] = int(stats.get("total_out_bytes", 0) or 0) + max(0, int(out_bytes))

    def record_encrypt(self, duration_ns: int, plaintext_bytes: int, ciphertext_bytes: int) -> None:
        self._update_primitive("aead_encrypt", duration_ns, plaintext_bytes, ciphertext_bytes)

    def record_decrypt_ok(self, duration_ns: int, ciphertext_bytes: int, plaintext_bytes: int) -> None:
        self._update_primitive("aead_decrypt_ok", duration_ns, ciphertext_bytes, plaintext_bytes)

    def record_decrypt_fail(self, duration_ns: int, ciphertext_bytes: int) -> None:
        self._update_primitive("aead_decrypt_fail", duration_ns, ciphertext_bytes, 0)


def _dscp_to_tos(dscp: Optional[int]) -> Optional[int]:
    """Convert DSCP value to TOS byte for socket options."""
    if dscp is None:
        return None
    try:
        d = int(dscp)
        if 0 <= d <= 63:
            return d << 2  # DSCP occupies high 6 bits of TOS/Traffic Class
    except (TypeError, ValueError):
        return None
    return None


def _parse_header_fields(
    expected_version: int,
    aead_ids: AeadIds,
    session_id: bytes,
    wire: bytes,
) -> Tuple[str, Optional[int]]:
    """
    Try to unpack the header and classify the most likely drop reason *without* AEAD work.
    Returns (reason, seq_if_available).
    """
    HEADER_STRUCT = AEAD_HEADER_STRUCT
    HEADER_LEN = AEAD_HEADER_LEN
    if len(wire) < HEADER_LEN:
        return ("header_too_short", None)
    try:
        (version, kem_id, kem_param, sig_id, sig_param, sess, seq, epoch) = struct.unpack(
            HEADER_STRUCT, wire[:HEADER_LEN]
        )
    except struct.error:
        return ("header_unpack_error", None)
    if version != expected_version:
        return ("version_mismatch", seq)
    if (kem_id, kem_param, sig_id, sig_param) != (
        aead_ids.kem_id,
        aead_ids.kem_param,
        aead_ids.sig_id,
        aead_ids.sig_param,
    ):
        return ("crypto_id_mismatch", seq)
    if sess != session_id:
        return ("session_mismatch", seq)
    # If we got here, header matches; any decrypt failure that returns None is auth/tag failure.
    return ("auth_fail_or_replay", seq)


class _TokenBucket:
    """Per-IP rate limiter using token bucket algorithm."""
    def __init__(self, capacity: int, refill_per_sec: float) -> None:
        self.capacity = max(1, capacity)
        self.refill = max(0.01, float(refill_per_sec))
        self.tokens: Dict[str, float] = {}      # ip -> tokens
        self.last: Dict[str, float] = {}        # ip -> last timestamp
        # Track last-seen to allow TTL-based pruning of state for long-running servers
        self._seen_ts: Dict[str, float] = {}

    def allow(self, ip: str) -> bool:
        """Check if request from IP should be allowed."""
        now = time.monotonic()
        t = self.tokens.get(ip, self.capacity)
        last = self.last.get(ip, now)
        # refill
        t = min(self.capacity, t + (now - last) * self.refill)
        self.last[ip] = now
        self._seen_ts[ip] = now
        if t >= 1.0:
            t -= 1.0
            self.tokens[ip] = t
            return True
        self.tokens[ip] = t
        return False

    def prune(self, idle_seconds: float) -> None:
        """Remove entries not seen within idle_seconds to prevent unbounded growth."""
        cutoff = time.monotonic() - float(idle_seconds)
        for ip in list(self._seen_ts.keys()):
            if self._seen_ts.get(ip, 0) < cutoff:
                self._seen_ts.pop(ip, None)
                self.tokens.pop(ip, None)
                self.last.pop(ip, None)


def _validate_config(cfg: dict) -> None:
    """Validate required configuration keys are present."""
    required_keys = [
        "TCP_HANDSHAKE_PORT", "UDP_DRONE_RX", "UDP_GCS_RX",
        "DRONE_PLAINTEXT_TX", "DRONE_PLAINTEXT_RX",
        "GCS_PLAINTEXT_TX", "GCS_PLAINTEXT_RX",
        "DRONE_HOST", "GCS_HOST", "REPLAY_WINDOW",
    ]
    for key in required_keys:
        if key not in cfg:
            raise ConfigError(f"CONFIG missing: {key}")


def _perform_handshake(
    role: str,
    suite: dict,
    gcs_sig_secret: Optional[object],
    gcs_sig_public: Optional[bytes],
    cfg: dict,
    stop_after_seconds: Optional[float] = None,
    ready_event: Optional[threading.Event] = None,
    *,
    accept_deadline_s: Optional[float] = None,
    io_timeout_s: Optional[float] = None,
) -> Tuple[
    bytes,
    bytes,
    bytes,
    bytes,
    bytes,
    Optional[str],
    Optional[str],
    Tuple[str, int],
    Dict[str, object],
]:
    """Perform TCP handshake and return keys, session details, and authenticated peer address.

    accept_deadline_s limits how long the GCS waits for an inbound TCP connect.
    io_timeout_s controls per-socket I/O timeouts for handshake reads/writes.

    Backward compatibility: stop_after_seconds is treated as accept_deadline_s
    when accept_deadline_s is not explicitly provided.
    """

    if accept_deadline_s is None and stop_after_seconds is not None:
        accept_deadline_s = stop_after_seconds

    if io_timeout_s is None:
        try:
            io_timeout = float(cfg.get("REKEY_HANDSHAKE_TIMEOUT", 20.0))
        except (TypeError, ValueError):
            io_timeout = 20.0
    else:
        try:
            io_timeout = float(io_timeout_s)
        except (TypeError, ValueError):
            io_timeout = float(cfg.get("REKEY_HANDSHAKE_TIMEOUT", 20.0))
    if io_timeout < 10.0:
        io_timeout = 10.0

    if role == "gcs":
        if gcs_sig_secret is None:
            raise ConfigError("GCS signature secret not provided")

        server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server_sock.bind(("0.0.0.0", cfg["TCP_HANDSHAKE_PORT"]))
        server_sock.listen(32)

        if ready_event:
            ready_event.set()

        timeout = accept_deadline_s if accept_deadline_s is not None else 30.0
        deadline: Optional[float] = None
        if accept_deadline_s is not None:
            deadline = time.monotonic() + accept_deadline_s

        gate = _TokenBucket(
            cfg.get("HANDSHAKE_RL_BURST", 5),
            cfg.get("HANDSHAKE_RL_REFILL_PER_SEC", 1),
        )
        prune_interval = max(5.0, float(cfg.get("HANDSHAKE_RL_PRUNE_INTERVAL_S", 60.0)))
        prune_idle_s = max(prune_interval, float(cfg.get("HANDSHAKE_RL_IDLE_TTL_S", 600.0)))
        next_prune = time.monotonic() + prune_interval

        try:
            try:
                while True:
                    if deadline is not None:
                        remaining = deadline - time.monotonic()
                        if remaining <= 0:
                            raise socket.timeout
                        server_sock.settimeout(max(0.01, remaining))
                    else:
                        server_sock.settimeout(timeout)

                    now_monotonic = time.monotonic()
                    if now_monotonic >= next_prune:
                        gate.prune(prune_idle_s)
                        next_prune = now_monotonic + prune_interval

                    try:
                        conn, addr = server_sock.accept()
                    except socket.timeout:
                        # If an explicit accept deadline is configured, treat expiry as a
                        # legacy-style config/runtime error (keeps older harnesses stable).
                        # Otherwise, continue waiting indefinitely for the initial drone connect.
                        if accept_deadline_s is not None:
                            raise ConfigError("No drone connection received within timeout")
                        continue
                    try:
                        ip, _port = addr
                        allowed_ips = {str(cfg["DRONE_HOST"])}
                        allowlist = cfg.get("DRONE_HOST_ALLOWLIST", []) or []
                        if isinstance(allowlist, (list, tuple, set)):
                            for entry in allowlist:
                                allowed_ips.add(str(entry))
                        else:
                            allowed_ips.add(str(allowlist))
                        strict_ip = bool(cfg.get("STRICT_HANDSHAKE_IP", True))
                        if strict_ip:
                            if ip not in allowed_ips:
                                logger.warning(
                                    "Rejected handshake from unauthorized IP",
                                    extra={"role": role, "expected": sorted(allowed_ips), "received": ip},
                                )
                                conn.close()
                                continue
                        else:
                            # Accept connection but log and record received IP for diagnostics
                            if ip not in allowed_ips:
                                logger.warning(
                                    "Handshake IP allowlist disabled; accepting connection from unexpected IP",
                                    extra={"role": role, "expected": sorted(allowed_ips), "received": ip},
                                )

                        if not gate.allow(ip):
                            try:
                                conn.settimeout(0.2)
                                conn.sendall(b"\x00")
                            except OSError:
                                pass
                            finally:
                                conn.close()
                            logger.warning(
                                "Handshake rate-limit drop",
                                extra={"role": role, "ip": ip},
                            )
                            continue

                        try:
                            result = server_gcs_handshake(conn, suite, gcs_sig_secret, timeout=io_timeout)
                        except HandshakeVerifyError:
                            logger.warning(
                                "Rejected drone handshake with failed authentication",
                                extra={"role": role, "expected": cfg["DRONE_HOST"], "received": ip},
                            )
                            continue
                        # Support either 5-tuple or 7-tuple
                        metrics_payload: Dict[str, object] = {}
                        if len(result) >= 7:
                            k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
                            if len(result) >= 8 and isinstance(result[7], dict):
                                metrics_payload = result[7]
                        else:
                            k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                            kem_name = sig_name = None
                        if not metrics_payload:
                            metrics_payload = {}
                        peer_addr = (ip, cfg["UDP_DRONE_RX"])
                        return (
                            k_d2g,
                            k_g2d,
                            nseed_d2g,
                            nseed_g2d,
                            session_id,
                            kem_name,
                            sig_name,
                            peer_addr,
                            metrics_payload,
                        )
                    finally:
                        try:
                            conn.close()
                        except OSError:
                            pass
            finally:
                pass
        finally:
            server_sock.close()

    elif role == "drone":
        if gcs_sig_public is None:
            raise ValueError("GCS signature public key not provided")

        client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        try:
            try:
                client_sock.settimeout(io_timeout)
            except OSError:
                pass
            client_sock.connect((cfg["GCS_HOST"], cfg["TCP_HANDSHAKE_PORT"]))
            peer_ip, _peer_port = client_sock.getpeername()
            result = client_drone_handshake(client_sock, suite, gcs_sig_public, timeout=io_timeout)
            metrics_payload: Dict[str, object] = {}
            if len(result) >= 7:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id, kem_name, sig_name = result[:7]
                if len(result) >= 8 and isinstance(result[7], dict):
                    metrics_payload = result[7]
            else:
                k_d2g, k_g2d, nseed_d2g, nseed_g2d, session_id = result
                kem_name = sig_name = None
            if not metrics_payload:
                metrics_payload = {}
            peer_addr = (peer_ip, cfg["UDP_GCS_RX"])
            return (
                k_d2g,
                k_g2d,
                nseed_d2g,
                nseed_g2d,
                session_id,
                kem_name,
                sig_name,
                peer_addr,
                metrics_payload,
            )
        finally:
            client_sock.close()
    else:
        raise ValueError(f"Invalid role: {role}")


@contextmanager
def _setup_sockets(role: str, cfg: dict, *, encrypted_peer: Optional[Tuple[str, int]] = None):
    """Setup and cleanup all UDP sockets for the proxy."""
    sockets = {}
    try:
        if role == "drone":
            # Encrypted socket - receive from GCS
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_DRONE_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind((cfg["DRONE_PLAINTEXT_HOST"], cfg["DRONE_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (reuse ingress socket to ensure correct source port)
            sockets["plaintext_out"] = ptx_in_sock

            # Peer addresses
            sockets["encrypted_peer"] = encrypted_peer or (cfg["GCS_HOST"], cfg["UDP_GCS_RX"])
            sockets["plaintext_peer"] = (cfg["DRONE_PLAINTEXT_HOST"], cfg["DRONE_PLAINTEXT_RX"])

        elif role == "gcs":
            # Encrypted socket - receive from Drone
            enc_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            enc_sock.bind(("0.0.0.0", cfg["UDP_GCS_RX"]))
            enc_sock.setblocking(False)
            tos = _dscp_to_tos(cfg.get("ENCRYPTED_DSCP"))
            if tos is not None:
                try:
                    enc_sock.setsockopt(socket.IPPROTO_IP, socket.IP_TOS, tos)
                except Exception:
                    pass
            sockets["encrypted"] = enc_sock

            # Plaintext ingress - receive from local app
            ptx_in_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            ptx_in_sock.bind((cfg["GCS_PLAINTEXT_HOST"], cfg["GCS_PLAINTEXT_TX"]))
            ptx_in_sock.setblocking(False)
            sockets["plaintext_in"] = ptx_in_sock

            # Plaintext egress - send to local app (reuse ingress socket to ensure correct source port)
            sockets["plaintext_out"] = ptx_in_sock

            # Peer addresses
            sockets["encrypted_peer"] = encrypted_peer or (cfg["DRONE_HOST"], cfg["UDP_DRONE_RX"])
            sockets["plaintext_peer"] = (cfg["GCS_PLAINTEXT_HOST"], cfg["GCS_PLAINTEXT_RX"])
        else:
            raise ValueError(f"Invalid role: {role}")

        yield sockets
    finally:
        # Close unique sockets
        closed = set()
        for sock in list(sockets.values()):
            if isinstance(sock, socket.socket) and sock not in closed:
                try:
                    sock.close()
                    closed.add(sock)
                except Exception:
                    pass


def _compute_aead_ids(suite: dict, kem_name: Optional[str], sig_name: Optional[str]) -> AeadIds:
    if kem_name and sig_name and header_ids_from_names:
        ids_tuple = header_ids_from_names(kem_name, sig_name)  # type: ignore
    else:
        ids_tuple = header_ids_for_suite(suite)
    return AeadIds(*ids_tuple)


def _build_sender_receiver(
    role: str,
    ids: AeadIds,
    session_id: bytes,
    k_d2g: bytes,
    k_g2d: bytes,
    cfg: dict,
):
    aead_token = cfg.get("SUITE_AEAD_TOKEN")
    if aead_token is None:
        raise ValueError("SUITE_AEAD_TOKEN missing from proxy config context")

    if role == "drone":
        sender = Sender(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_d2g, aead_token=aead_token)
        receiver = Receiver(
            CONFIG["WIRE_VERSION"],
            ids,
            session_id,
            0,
            k_g2d,
            cfg["REPLAY_WINDOW"],
            aead_token=aead_token,
        )
    else:
        sender = Sender(CONFIG["WIRE_VERSION"], ids, session_id, 0, k_g2d, aead_token=aead_token)
        receiver = Receiver(
            CONFIG["WIRE_VERSION"],
            ids,
            session_id,
            0,
            k_d2g,
            cfg["REPLAY_WINDOW"],
            aead_token=aead_token,
        )
    return sender, receiver


def _launch_manual_console(control_state: ControlState, *, quiet: bool) -> Tuple[threading.Event, Tuple[threading.Thread, ...]]:
    suites_catalog = sorted(list_suites().keys())
    stop_event = threading.Event()

    def status_loop() -> None:
        last_line = ""
        while not stop_event.is_set():
            with control_state.lock:
                state = control_state.state
                suite_id = control_state.current_suite
            line = f"[{state}] {suite_id}"
            if line != last_line and not quiet:
                sys.stderr.write(f"\r{line:<80}")
                sys.stderr.flush()
                last_line = line
            time.sleep(0.5)
        if not quiet:
            sys.stderr.write("\r" + " " * 80 + "\r")
            sys.stderr.flush()

    def operator_loop() -> None:
        if not sys.stdin or not hasattr(sys.stdin, "isatty") or not sys.stdin.isatty():
            # Avoid blocking forever in service / redirected-stdin environments.
            if not quiet:
                print("Manual control disabled: stdin is not a TTY.")
            stop_event.set()
            return
        if not quiet:
            print("Manual control ready. Type a suite ID, 'list', 'status', or 'quit'.")
        while not stop_event.is_set():
            try:
                line = input("rekey> ")
            except EOFError:
                break
            if line is None:
                continue
            line = line.strip()
            if not line:
                continue
            lowered = line.lower()
            if lowered in {"quit", "exit"}:
                break
            if lowered == "list":
                if not quiet:
                    print("Available suites:")
                    for sid in suites_catalog:
                        print(f"  {sid}")
                continue
            if lowered == "status":
                with control_state.lock:
                    summary = f"state={control_state.state} suite={control_state.current_suite}"
                    if control_state.last_status:
                        summary += f" last_status={control_state.last_status}"
                if not quiet:
                    print(summary)
                continue
            try:
                target_suite = get_suite(line)
                rid = request_prepare(control_state, target_suite["suite_id"])
                if not quiet:
                    print(f"prepare queued for {target_suite['suite_id']} rid={rid}")
            except RuntimeError as exc:
                if not quiet:
                    print(f"Busy: {exc}")
            except Exception as exc:
                if not quiet:
                    print(f"Invalid suite: {exc}")

        stop_event.set()

    status_thread = threading.Thread(target=status_loop, daemon=True)
    operator_thread = threading.Thread(target=operator_loop, daemon=True)
    status_thread.start()
    operator_thread.start()
    return stop_event, (status_thread, operator_thread)


def run_proxy(
    *,
    role: str,
    suite: dict,
    cfg: dict,
    gcs_sig_secret: Optional[object] = None,
    gcs_sig_public: Optional[bytes] = None,
    stop_after_seconds: Optional[float] = None,
    manual_control: bool = False,
    quiet: bool = False,
    ready_event: Optional[threading.Event] = None,
    status_file: Optional[str] = None,
    load_gcs_secret: Optional[Callable[[Dict[str, object]], object]] = None,
    load_gcs_public: Optional[Callable[[Dict[str, object]], bytes]] = None,
) -> Dict[str, object]:
    """
    Start a blocking proxy process for `role` in {"drone","gcs"}.

    Performs the TCP handshake, bridges plaintext/encrypted UDP, and processes
    in-band control messages for rekey negotiation. Returns counters on clean exit.
    """
    if role not in {"drone", "gcs"}:
        raise ValueError(f"Invalid role: {role}")

    _validate_config(cfg)

    cfg = dict(cfg)
    cfg["SUITE_AEAD_TOKEN"] = suite.get("aead_token", "aesgcm")

    counters = ProxyCounters()
    counters_lock = threading.Lock()
    start_time = time.time()

    status_path: Optional[Path] = None
    if status_file:
        status_path = Path(status_file).expanduser()

    def write_status(payload: Dict[str, object]) -> None:
        if status_path is None:
            return
        import time as _time
        attempts = 2
        status_path.parent.mkdir(parents=True, exist_ok=True)
        tmp_path = status_path.with_suffix(status_path.suffix + ".tmp")
        data = json.dumps(payload)
        for attempt in range(attempts):
            try:
                tmp_path.write_text(data, encoding="utf-8")
                tmp_path.replace(status_path)
                return
            except PermissionError:
                # Common on Windows when antivirus/indexer holds the file briefly.
                if attempt + 1 < attempts:
                    _time.sleep(0.05)
                    continue
                logger.warning(
                    "Failed to write status file due to PermissionError",
                    extra={"role": role, "path": str(status_path)},
                )
                return
            except Exception as exc:
                logger.warning(
                    "Failed to write status file",
                    extra={"role": role, "error": str(exc), "path": str(status_path)},
                )
                return

    if role == "drone" and gcs_sig_public is None:
        if load_gcs_public is None:
            raise ConfigError("GCS signature public key not provided (provide peer key or loader)")
        gcs_sig_public = load_gcs_public(suite)

    handshake_result = _perform_handshake(
        role,
        suite,
        gcs_sig_secret,
        gcs_sig_public,
        cfg,
        accept_deadline_s=stop_after_seconds,
        io_timeout_s=cfg.get("REKEY_HANDSHAKE_TIMEOUT", 20.0),
        ready_event=ready_event,
    )

    if len(handshake_result) >= 9:
        (
            k_d2g,
            k_g2d,
            _nseed_d2g,
            _nseed_g2d,
            session_id,
            kem_name,
            sig_name,
            peer_addr,
            handshake_metrics,
        ) = handshake_result
    else:
        (
            k_d2g,
            k_g2d,
            _nseed_d2g,
            _nseed_g2d,
            session_id,
            kem_name,
            sig_name,
            peer_addr,
        ) = handshake_result
        handshake_metrics = {}

    suite_id = suite.get("suite_id")
    if not suite_id:
        try:
            suite_id = next((sid for sid, s in SUITES.items() if dict(s) == suite), "unknown")
        except Exception:
            suite_id = "unknown"

    sess_status_display = (
        session_id.hex()
        if cfg.get("LOG_SESSION_ID", False)
        else hashlib.sha256(session_id).hexdigest()[:8] + "..."
    )
    status_payload = {
        "status": "handshake_ok",
        "suite": suite_id,
        "session_id": sess_status_display,
    }
    if handshake_metrics:
        status_payload["handshake_metrics"] = handshake_metrics
    write_status(status_payload)

    sess_display = (
        session_id.hex()
        if cfg.get("LOG_SESSION_ID", False)
        else hashlib.sha256(session_id).hexdigest()[:8] + "..."
    )

    with counters_lock:
        counters.handshake_metrics = dict(handshake_metrics) if handshake_metrics else {}

    logger.info(
        "PQC handshake completed successfully",
        extra={
            "suite_id": suite_id,
            "peer_role": ("drone" if role == "gcs" else "gcs"),
            "session_id": sess_display,
        },
    )

    # Periodically persist counters to the status file while the proxy runs.
    # This allows external automation (scheduler) to observe enc_in/enc_out
    # during long-running experiments without waiting for process exit.
    stop_status_writer = threading.Event()

    def _status_writer() -> None:
        while not stop_status_writer.is_set():
            try:
                with counters_lock:
                    payload = {
                        "status": "running",
                        "suite": suite_id,
                        "counters": counters.to_dict(),
                        "ts_ns": time.time_ns(),
                    }
                write_status(payload)
            except Exception:
                logger.debug("status writer failed", extra={"role": role})
            # sleep with event to allow quick shutdown
            stop_status_writer.wait(1.0)

    status_thread: Optional[threading.Thread] = None
    try:
        status_thread = threading.Thread(target=_status_writer, daemon=True)
        status_thread.start()
    except Exception:
        status_thread = None

    aead_ids = _compute_aead_ids(suite, kem_name, sig_name)
    sender, receiver = _build_sender_receiver(role, aead_ids, session_id, k_d2g, k_g2d, cfg)

    control_state = create_control_state(role, suite_id)
    coordinator_role = coordinator_role_from_config(cfg)
    try:
        set_coordinator_role(control_state, coordinator_role)
    except Exception:
        # Fail closed to legacy behaviour (GCS-coordinated) if coordinator setup fails.
        coordinator_role = "gcs"
        try:
            set_coordinator_role(control_state, coordinator_role)
        except Exception:
            pass
    context_lock = threading.RLock()
    active_context: Dict[str, object] = {
        "suite": suite_id,
        "suite_dict": suite,
        "session_id": session_id,
        "aead_ids": aead_ids,
        "sender": sender,
        "receiver": receiver,
        "peer_addr": peer_addr,
        "peer_match_strict": bool(cfg.get("STRICT_UDP_PEER_MATCH", True)),
    }

    active_rekeys: set[str] = set()
    rekey_guard = threading.Lock()

    if manual_control and is_coordinator(role=role, coordinator_role=coordinator_role) and not cfg.get("ENABLE_PACKET_TYPE"):
        logger.warning("ENABLE_PACKET_TYPE is disabled; control-plane packets may not be processed correctly.")

    manual_stop: Optional[threading.Event] = None
    manual_threads: Tuple[threading.Thread, ...] = ()
    if manual_control and is_coordinator(role=role, coordinator_role=coordinator_role):
        manual_stop, manual_threads = _launch_manual_console(control_state, quiet=quiet)

    # Optional TCP control server (legacy JSON protocol) for external schedulers.
    # Enables commands like {"cmd":"rekey","suite":"cs-..."}.
    # Only the coordinator role accepts 'rekey' (non-coordinator returns coordinator_only).
    tcp_control_enabled = bool(cfg.get("ENABLE_TCP_CONTROL", False))
    control_server = start_control_server_if_enabled(
        role=role,
        cfg=cfg,
        control_state=control_state,
        quiet=quiet,
        enabled=tcp_control_enabled,
    )

    def _launch_rekey(target_suite_id: str, rid: str) -> None:
        with rekey_guard:
            if rid in active_rekeys:
                return
            active_rekeys.add(rid)

        logger.info(
            "Control rekey negotiation started",
            extra={"role": role, "suite_id": target_suite_id, "rid": rid},
        )

        def worker() -> None:
            nonlocal gcs_sig_public
            try:
                new_suite = get_suite(target_suite_id)
                new_secret = None
                new_public: Optional[bytes] = None
                if role == "gcs" and load_gcs_secret is not None:
                    try:
                        new_secret = load_gcs_secret(new_suite)
                    except FileNotFoundError as exc:
                        with context_lock:
                            current_suite = active_context["suite"]
                        with counters_lock:
                            counters.rekeys_fail += 1
                        record_rekey_result(control_state, rid, current_suite, success=False)
                        logger.warning(
                            "Control rekey rejected: missing signing secret",
                            extra={
                                "role": role,
                                "suite_id": target_suite_id,
                                "rid": rid,
                                "error": str(exc),
                            },
                        )
                        with rekey_guard:
                            active_rekeys.discard(rid)
                        return
                    except Exception as exc:
                        with context_lock:
                            current_suite = active_context["suite"]
                        with counters_lock:
                            counters.rekeys_fail += 1
                        record_rekey_result(control_state, rid, current_suite, success=False)
                        logger.warning(
                            "Control rekey rejected: signing secret load failed",
                            extra={
                                "role": role,
                                "suite_id": target_suite_id,
                                "rid": rid,
                                "error": str(exc),
                            },
                        )
                        with rekey_guard:
                            active_rekeys.discard(rid)
                        return
            except (ValueError, KeyError) as exc:
                with context_lock:
                    current_suite = active_context["suite"]
                with counters_lock:
                    counters.rekeys_fail += 1
                record_rekey_result(control_state, rid, current_suite, success=False)
                logger.warning(
                    "Control rekey rejected: unknown suite",
                    extra={"role": role, "suite_id": target_suite_id, "rid": rid, "error": str(exc)},
                )
                with rekey_guard:
                    active_rekeys.discard(rid)
                return

            if role == "drone" and load_gcs_public is not None:
                try:
                    new_public = load_gcs_public(new_suite)
                except FileNotFoundError as exc:
                    with context_lock:
                        current_suite = active_context["suite"]
                    with counters_lock:
                        counters.rekeys_fail += 1
                    record_rekey_result(control_state, rid, current_suite, success=False)
                    logger.warning(
                        "Control rekey rejected: missing signing public key",
                        extra={
                            "role": role,
                            "suite_id": target_suite_id,
                            "rid": rid,
                            "error": str(exc),
                        },
                    )
                    with rekey_guard:
                        active_rekeys.discard(rid)
                    return
                except Exception as exc:
                    with context_lock:
                        current_suite = active_context["suite"]
                    with counters_lock:
                        counters.rekeys_fail += 1
                    record_rekey_result(control_state, rid, current_suite, success=False)
                    logger.warning(
                        "Control rekey rejected: signing public key load failed",
                        extra={
                            "role": role,
                            "suite_id": target_suite_id,
                            "rid": rid,
                            "error": str(exc),
                        },
                    )
                    with rekey_guard:
                        active_rekeys.discard(rid)
                    return

            prev_token: Optional[str] = cfg.get("SUITE_AEAD_TOKEN")
            try:
                timeout = cfg.get("REKEY_HANDSHAKE_TIMEOUT", 20.0)
                if role == "gcs" and new_secret is not None:
                    base_secret = new_secret
                else:
                    base_secret = gcs_sig_secret
                public_key = new_public if new_public is not None else gcs_sig_public
                if role == "drone" and public_key is None:
                    raise ConfigError("GCS public key not available for rekey")
                rk_result = _perform_handshake(
                    role,
                    new_suite,
                    base_secret,
                    public_key,
                    cfg,
                    accept_deadline_s=float(timeout),
                    io_timeout_s=float(timeout),
                )
                if len(rk_result) >= 9:
                    (
                        new_k_d2g,
                        new_k_g2d,
                        _nd1,
                        _nd2,
                        new_session_id,
                        new_kem_name,
                        new_sig_name,
                        new_peer_addr,
                        new_handshake_metrics,
                    ) = rk_result
                else:
                    (
                        new_k_d2g,
                        new_k_g2d,
                        _nd1,
                        _nd2,
                        new_session_id,
                        new_kem_name,
                        new_sig_name,
                        new_peer_addr,
                    ) = rk_result
                    new_handshake_metrics = {}
                if new_handshake_metrics:
                    new_handshake_metrics = dict(new_handshake_metrics)

                cfg["SUITE_AEAD_TOKEN"] = new_suite.get("aead_token", "aesgcm")
                new_ids = _compute_aead_ids(new_suite, new_kem_name, new_sig_name)
                new_sender, new_receiver = _build_sender_receiver(
                    role, new_ids, new_session_id, new_k_d2g, new_k_g2d, cfg
                )

                with context_lock:
                    active_context.update(
                        {
                            "sender": new_sender,
                            "receiver": new_receiver,
                            "session_id": new_session_id,
                            "aead_ids": new_ids,
                            "suite": new_suite["suite_id"],
                            "suite_dict": new_suite,
                            "peer_addr": new_peer_addr,
                        }
                    )
                    sockets["encrypted_peer"] = new_peer_addr

                with counters_lock:
                    counters.rekeys_ok += 1
                    counters.last_rekey_ms = int(time.time() * 1000)
                    counters.last_rekey_suite = new_suite["suite_id"]
                    counters.handshake_metrics = dict(new_handshake_metrics) if new_handshake_metrics else {}
                if role == "drone" and new_public is not None:
                    gcs_sig_public = new_public
                record_rekey_result(control_state, rid, new_suite["suite_id"], success=True)
                new_sess_status_display = (
                    new_session_id.hex()
                    if cfg.get("LOG_SESSION_ID", False)
                    else hashlib.sha256(new_session_id).hexdigest()[:8] + "..."
                )
                status_payload = {
                    "status": "rekey_ok",
                    "new_suite": new_suite["suite_id"],
                    "session_id": new_sess_status_display,
                }
                if new_handshake_metrics:
                    status_payload["handshake_metrics"] = new_handshake_metrics
                write_status(status_payload)
                new_sess_display = (
                    new_session_id.hex()
                    if cfg.get("LOG_SESSION_ID", False)
                    else hashlib.sha256(new_session_id).hexdigest()[:8] + "..."
                )
                logger.info(
                    "Control rekey successful",
                    extra={
                        "role": role,
                        "suite_id": new_suite["suite_id"],
                        "rid": rid,
                        "session_id": new_sess_display,
                    },
                )
            except Exception as exc:
                if prev_token is not None:
                    cfg["SUITE_AEAD_TOKEN"] = prev_token
                with context_lock:
                    current_suite = active_context["suite"]
                with counters_lock:
                    counters.rekeys_fail += 1
                record_rekey_result(control_state, rid, current_suite, success=False)
                logger.warning(
                    "Control rekey failed",
                    extra={"role": role, "suite_id": target_suite_id, "rid": rid, "error": str(exc)},
                )
            finally:
                with rekey_guard:
                    active_rekeys.discard(rid)

        threading.Thread(target=worker, daemon=True).start()

    with _setup_sockets(role, cfg, encrypted_peer=peer_addr) as sockets:
        selector = selectors.DefaultSelector()
        selector.register(sockets["encrypted"], selectors.EVENT_READ, data="encrypted")
        selector.register(sockets["plaintext_in"], selectors.EVENT_READ, data="plaintext_in")

        # Dynamic peer address for plaintext app (MAVProxy)
        # Initialize with configured default, but update based on ingress traffic
        # to support MAVProxy's ephemeral ports (when using --out).
        app_peer_addr = sockets["plaintext_peer"]

        def send_control(payload: dict) -> None:
            body = json.dumps(payload, separators=(",", ":"), sort_keys=True).encode("utf-8")
            frame = b"\x02" + body
            with context_lock:
                current_sender = active_context["sender"]
            try:
                wire = current_sender.encrypt(frame)
            except Exception as exc:
                counters.drops += 1
                counters.drop_other += 1
                logger.warning("Failed to encrypt control payload", extra={"role": role, "error": str(exc)})
                return
            try:
                sockets["encrypted"].sendto(wire, sockets["encrypted_peer"])
                counters.enc_out += 1
            except socket.error as exc:
                counters.drops += 1
                counters.drop_other += 1
                logger.warning("Failed to send control payload", extra={"role": role, "error": str(exc)})

        try:
            while True:
                if stop_after_seconds is not None and (time.time() - start_time) >= stop_after_seconds:
                    break

                while True:
                    try:
                        control_payload = control_state.outbox.get_nowait()
                    except queue.Empty:
                        break
                    send_control(control_payload)

                events = selector.select(timeout=0.1)
                for key, _mask in events:
                    sock = key.fileobj
                    data_type = key.data

                    if data_type == "plaintext_in":
                        try:
                            payload, addr = sock.recvfrom(16384)
                            if not payload:
                                continue
                            
                            # Update dynamic peer address to reply to the correct source
                            app_peer_addr = addr

                            with counters_lock:
                                counters.ptx_in += 1

                            payload_out = (b"\x01" + payload) if cfg.get("ENABLE_PACKET_TYPE") else payload
                            with context_lock:
                                current_sender = active_context["sender"]
                            encrypt_start_ns = time.perf_counter_ns()
                            try:
                                wire = current_sender.encrypt(payload_out)
                            except SequenceOverflow as exc:
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_other += 1
                                logger.warning(
                                    "Sequence space exhausted; requesting rekey",
                                    extra={
                                        "role": role,
                                        "error": str(exc),
                                    },
                                )
                                with context_lock:
                                    current_suite = active_context.get("suite")
                                if current_suite:
                                    try:
                                        rid = request_prepare(control_state, current_suite)
                                    except RuntimeError:
                                        logger.debug(
                                            "Rekey already in progress after sequence exhaustion",
                                            extra={"role": role},
                                        )
                                    else:
                                        logger.info(
                                            "Triggered control-plane rekey due to sequence exhaustion",
                                            extra={"role": role, "suite": current_suite, "rid": rid},
                                        )
                                continue
                            except Exception as exc:
                                encrypt_elapsed_ns = time.perf_counter_ns() - encrypt_start_ns
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_other += 1
                                logger.warning(
                                    "Encrypt failed",
                                    extra={
                                        "role": role,
                                        "error": str(exc),
                                        "payload_len": len(payload_out),
                                    },
                                )
                                continue
                            encrypt_elapsed_ns = time.perf_counter_ns() - encrypt_start_ns
                            ciphertext_len = len(wire)
                            plaintext_len = len(payload_out)
                            with counters_lock:
                                counters.record_encrypt(encrypt_elapsed_ns, plaintext_len, ciphertext_len)

                            try:
                                sockets["encrypted"].sendto(wire, sockets["encrypted_peer"])
                                with counters_lock:
                                    counters.enc_out += 1
                            except socket.error:
                                with counters_lock:
                                    counters.drops += 1
                        except socket.error:
                            continue

                    elif data_type == "encrypted":
                        try:
                            wire, addr = sock.recvfrom(16384)
                            if not wire:
                                continue

                            with context_lock:
                                current_receiver = active_context["receiver"]
                                expected_peer = active_context.get("peer_addr")
                                strict_match = bool(active_context.get("peer_match_strict", True))

                            src_ip, src_port = addr
                            if expected_peer is not None:
                                exp_ip, exp_port = expected_peer  # type: ignore[misc]
                                mismatch = False
                                if strict_match:
                                    mismatch = src_ip != exp_ip or src_port != exp_port
                                else:
                                    mismatch = src_ip != exp_ip
                                if mismatch:
                                    with counters_lock:
                                        counters.drops += 1
                                        counters.drop_src_addr += 1
                                    logger.debug(
                                        "Dropped encrypted packet from unauthorized source",
                                        extra={"role": role, "expected": expected_peer, "received": addr},
                                    )
                                    continue

                            with counters_lock:
                                counters.enc_in += 1

                            cipher_len = len(wire)
                            decrypt_start_ns = time.perf_counter_ns()
                            try:
                                plaintext = current_receiver.decrypt(wire)
                            except ReplayError:
                                decrypt_elapsed_ns = time.perf_counter_ns() - decrypt_start_ns
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_replay += 1
                                    counters.record_decrypt_fail(decrypt_elapsed_ns, cipher_len)
                                continue
                            except HeaderMismatch:
                                decrypt_elapsed_ns = time.perf_counter_ns() - decrypt_start_ns
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_header += 1
                                    counters.record_decrypt_fail(decrypt_elapsed_ns, cipher_len)
                                continue
                            except AeadAuthError:
                                decrypt_elapsed_ns = time.perf_counter_ns() - decrypt_start_ns
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_auth += 1
                                    counters.record_decrypt_fail(decrypt_elapsed_ns, cipher_len)
                                continue
                            except AeadError as exc:
                                decrypt_elapsed_ns = time.perf_counter_ns() - decrypt_start_ns
                                with counters_lock:
                                    counters.drops += 1
                                    reason, _seq = _parse_header_fields(
                                        CONFIG["WIRE_VERSION"], current_receiver.ids, current_receiver.session_id, wire
                                    )
                                    if reason in (
                                        "version_mismatch",
                                        "crypto_id_mismatch",
                                        "header_too_short",
                                        "header_unpack_error",
                                    ):
                                        counters.drop_header += 1
                                    elif reason == "session_mismatch":
                                        counters.drop_session_epoch += 1
                                    else:
                                        counters.drop_auth += 1
                                    counters.record_decrypt_fail(decrypt_elapsed_ns, cipher_len)
                                logger.warning(
                                    "Decrypt failed (classified)",
                                    extra={
                                        "role": role,
                                        "reason": reason,
                                        "wire_len": len(wire),
                                        "error": str(exc),
                                    },
                                )
                                continue
                            except Exception as exc:
                                decrypt_elapsed_ns = time.perf_counter_ns() - decrypt_start_ns
                                with counters_lock:
                                    counters.drops += 1
                                    counters.drop_other += 1
                                    counters.record_decrypt_fail(decrypt_elapsed_ns, cipher_len)
                                logger.warning(
                                    "Decrypt failed (other)",
                                    extra={"role": role, "error": str(exc), "wire_len": len(wire)},
                                )
                                continue

                            decrypt_elapsed_ns = time.perf_counter_ns() - decrypt_start_ns
                            if plaintext is None:
                                with counters_lock:
                                    counters.drops += 1
                                    last_reason = current_receiver.last_error_reason()
                                    # Bug #7 fix: Proper error classification without redundancy
                                    if last_reason == "auth":
                                        counters.drop_auth += 1
                                    elif last_reason == "header":
                                        counters.drop_header += 1
                                    elif last_reason == "replay":
                                        counters.drop_replay += 1
                                    elif last_reason == "session":
                                        counters.drop_session_epoch += 1
                                    elif last_reason is None or last_reason == "unknown":
                                        # Only parse header if receiver didn't classify it
                                        reason, _seq = _parse_header_fields(
                                            CONFIG["WIRE_VERSION"],
                                            current_receiver.ids,
                                            current_receiver.session_id,
                                            wire,
                                        )
                                        if reason in (
                                            "version_mismatch",
                                            "crypto_id_mismatch",
                                            "header_too_short",
                                            "header_unpack_error",
                                        ):
                                            counters.drop_header += 1
                                        elif reason == "session_mismatch":
                                            counters.drop_session_epoch += 1
                                        elif reason == "auth_fail_or_replay":
                                            counters.drop_auth += 1
                                        else:
                                            counters.drop_other += 1
                                    else:
                                        # Unrecognized last_reason value
                                        counters.drop_other += 1
                                    counters.record_decrypt_fail(decrypt_elapsed_ns, cipher_len)
                                continue

                            plaintext_len = len(plaintext)
                            with counters_lock:
                                counters.record_decrypt_ok(decrypt_elapsed_ns, cipher_len, plaintext_len)

                            # Control-plane handling: only interpret leading 0x02 as control
                            # when ENABLE_PACKET_TYPE is enabled. When disabled, payloads must
                            # be transparent and delivered unchanged to the application.
                            if cfg.get("ENABLE_PACKET_TYPE") and plaintext and plaintext[0] == 0x02:
                                try:
                                    control_json = json.loads(plaintext[1:].decode("utf-8"))
                                except (UnicodeDecodeError, json.JSONDecodeError):
                                    with counters_lock:
                                        counters.drops += 1
                                        counters.drop_other += 1
                                    continue
                                result = handle_control(control_json, role, control_state)
                                for note in result.notes:
                                    if note.startswith("prepare_fail"):
                                        with counters_lock:
                                            counters.rekeys_fail += 1
                                for payload in result.send:
                                    control_state.outbox.put(payload)
                                if result.start_handshake:
                                    suite_next, rid = result.start_handshake
                                    _launch_rekey(suite_next, rid)
                                continue

                            if cfg.get("ENABLE_PACKET_TYPE") and plaintext:
                                ptype = plaintext[0]
                                if ptype == 0x01:
                                    out_bytes = plaintext[1:]
                                else:
                                    with counters_lock:
                                        counters.drops += 1
                                        counters.drop_other += 1
                                    continue
                            else:
                                out_bytes = plaintext

                            sockets["plaintext_out"].sendto(out_bytes, app_peer_addr)
                            with counters_lock:
                                counters.ptx_out += 1
                        except socket.error:
                            with counters_lock:
                                counters.drops += 1
                                counters.drop_other += 1
                            continue
        except KeyboardInterrupt:
            pass
        finally:
            selector.close()
            if manual_stop:
                manual_stop.set()
                for thread in manual_threads:
                    thread.join(timeout=0.5)
            if control_server is not None:
                try:
                    control_server.stop()
                except Exception:
                    pass

        # Final status write and stop the status writer thread if running
        try:
            with counters_lock:
                write_status({
                    "status": "stopped",
                    "suite": suite_id,
                    "counters": counters.to_dict(),
                    "ts_ns": time.time_ns(),
                })
        except Exception:
            pass

        if 'stop_status_writer' in locals() and stop_status_writer is not None:
            try:
                stop_status_writer.set()
            except Exception:
                pass
        if 'status_thread' in locals() and status_thread is not None and status_thread.is_alive():
            try:
                status_thread.join(timeout=1.0)
            except Exception:
                pass

        return counters.to_dict()

==================================================

core\config.py
==================================================
"""
Core configuration constants for PQC drone-GCS secure proxy.

Single source of truth for all network ports, hosts, and runtime parameters.
"""

import os
from ipaddress import ip_address
from typing import Dict, Any
from core.exceptions import ConfigError


# Baseline host defaults reused throughout the configuration payload.
# Keep both LAN and Tailscale addresses handy so schedulers can pin the
# appropriate interface per testbed. Defaults target the LAN endpoints.
# Localhost-only topology override for smoke/local tests can be applied
# by temporarily pointing *_HOST_LAN values at 127.0.0.1. For normal
# lab runs, keep these set to the actual LAN-facing addresses.
_DRONE_HOST_LAN = "192.168.0.105"   # uavpi drone LAN IP (wlan0 from `ip addr`)
_DRONE_HOST_TAILSCALE = "100.101.93.23"
_GCS_HOST_LAN = "192.168.0.102"    # GCS Windows LAN IP (from ipconfig)
_GCS_HOST_TAILSCALE = "100.106.181.122"

# Default to LAN hosts for operational runs (Tailscale kept for SSH only)
_DEFAULT_DRONE_HOST = _DRONE_HOST_LAN
_DEFAULT_GCS_HOST = _GCS_HOST_LAN

# Environment-sourced default credential to avoid embedding lab passwords in source control.
_LAB_PASSWORD_DEFAULT = os.getenv("PQC_LAB_PASSWORD", "uavpi")


# Default configuration - all required keys with correct types
CONFIG = {
    # Handshake (TCP)
    "TCP_HANDSHAKE_PORT": 46000,

    # Encrypted UDP data-plane (network)
    "UDP_DRONE_RX": 46012,   # drone binds here; GCS sends here
    "UDP_GCS_RX": 46011,     # gcs binds here; Drone sends here

    # Plaintext UDP (local loopback to apps/FC)
    "DRONE_PLAINTEXT_TX": 47003,  # app→drone-proxy (to encrypt out)
    "DRONE_PLAINTEXT_RX": 47004,  # drone-proxy→app (after decrypt)
    "GCS_PLAINTEXT_TX": 47001,    # app→gcs-proxy
    "GCS_PLAINTEXT_RX": 47002,    # gcs-proxy→app
    # Use localhost for plaintext bindings to ensure compatibility with local MAVProxy
    "DRONE_PLAINTEXT_HOST": "127.0.0.1",
    "GCS_PLAINTEXT_HOST": "127.0.0.1",

    # Hosts
    "DRONE_HOST": _DEFAULT_DRONE_HOST,
    "GCS_HOST": _DEFAULT_GCS_HOST,
    "DRONE_HOST_LAN": _DRONE_HOST_LAN,
    "DRONE_HOST_TAILSCALE": _DRONE_HOST_TAILSCALE,
    "GCS_HOST_LAN": _GCS_HOST_LAN,
    "GCS_HOST_TAILSCALE": _GCS_HOST_TAILSCALE,

    # Pre-shared key (hex) for drone authentication during handshake.
    # Default is a placeholder; override in production via environment variable.
    # Intentionally default to empty; require injection via environment in non-dev.
    "DRONE_PSK": "",

    # Crypto/runtime
    "REPLAY_WINDOW": 1024,
    "WIRE_VERSION": 1,      # header version byte (frozen)
    # Allow slower suites to finish the rekey handshake without timing out
    "REKEY_HANDSHAKE_TIMEOUT": 45.0,

    # --- Bare scheduler defaults (scheduler/bare/*) ---
    # Dwell time per suite before automatic rotation (seconds).
    # Both drone_follower and gcs_scheduler read this for consistency.
    "BARE_SUITE_DWELL_S": 10.0,
    # Confirmation timeout for local proxy state change after rekey request.
    "BARE_CONFIRM_TIMEOUT_S": 10.0,
    # Poll interval for status checks during dwell period.
    "BARE_POLL_INTERVAL_S": 2.0,

    # --- Optional hardening / QoS knobs (NOT required; safe defaults) ---
    # Limit TCP handshake attempts accepted per IP at the GCS (server) side.
    # Model: token bucket; BURST tokens max, refilling at REFILL_PER_SEC tokens/sec.
    "HANDSHAKE_RL_BURST": 5,
    "HANDSHAKE_RL_REFILL_PER_SEC": 1,

    # Mark encrypted UDP with DSCP EF (46) to prioritize on WMM-enabled APs.
    # Set to None to disable. Implementation multiplies by 4 to form TOS.
    "ENCRYPTED_DSCP": 46,

    # Feature flag: if True, proxy prefixes app->proxy plaintext with 1 byte packet type.
    # 0x01 = MAVLink/data (forward to local app); 0x02 = control (route to policy engine).
    # When False (default), proxy passes bytes unchanged (backward compatible).
    "ENABLE_PACKET_TYPE": True,

    # Enable exposure of ASCON AEAD variants in suite registry and runtime probing.
    # ENABLE_ASCON gates all Ascon tokens; ENABLE_ASCON128A further enables the 'ascon128a'
    # variant (kept experimental). Both default False to preserve legacy test matrix unless
    # explicitly activated via environment variables.
    # LOCAL TEST OVERRIDE: enable Ascon variants for extended AEAD smoke coverage.
    "ENABLE_ASCON": True,
    "ENABLE_ASCON128A": True,

    # Enforce 16-byte key usage for ASCON-128 when enabled. Default False preserves
    # legacy behaviour while exposing the knob via CONFIG/env overrides.
    "ASCON_STRICT_KEY_SIZE": False,

    # HMAC auth shared secret for MAV schedulers (string, UTF-8). Override via env MAV_AUTH_KEY.
    "MAV_AUTH_KEY": "",
    # Optional allow list for MAV scheduler control channels. Accepts list/tuple or comma string.
    "MAV_ALLOWED_SENDERS": [],

    # Direct MAVProxy wiring defaults for lab setups. These are used by
    # auto/mav helpers to build a plain Pixhawk↔GCS MAVLink path that is
    # independent of the PQC proxy. Adjust these if wiring or tools move.
    "MAV_FC_DEVICE": "/dev/ttyACM0",  # Pixhawk USB serial on the drone Pi
    "MAV_FC_BAUD": 57600,              # Pixhawk serial baud rate
    # GCS-side UDP ports where the drone sends MAVLink (LAN-facing).
    "MAV_GCS_IN_PORT_1": 14550,
    "MAV_GCS_IN_PORT_2": 14551,
    # GCS MAVProxy bind host for incoming MAVLink from the drone.
    "MAV_GCS_LISTEN_HOST": "0.0.0.0",
    # Local loopback host/ports on the GCS for ground tools (e.g., QGC).
    "MAV_LOCAL_HOST": "127.0.0.1",
    "MAV_LOCAL_OUT_PORT_1": 14550,
    "MAV_LOCAL_OUT_PORT_2": 14551,
    # Explicit drone host/port for client-style GCS master (two-way heartbeat).
    # Using explicit remote prevents passive listener stalls on some platforms.
    "MAV_DRONE_HOST": _DEFAULT_DRONE_HOST,
    "MAV_DRONE_UDP_PORT": 14550,

    # Enforce strict matching of encrypted UDP peer IP/port with the authenticated handshake peer.
    # Disable (set to False) only when operating behind NAT where source ports may differ.
    "STRICT_UDP_PEER_MATCH": True,
    "STRICT_HANDSHAKE_IP": True,

    # Log real session IDs only when explicitly enabled (default False masks them to hashes).
    "LOG_SESSION_ID": False,

    # --- Simple automation defaults (tools/auto/*_simple.py) ---
    # Optional: enable TCP JSON control listener inside the core proxy.
    # When enabled, each side may expose a listener (DRONE_CONTROL_* / GCS_CONTROL_*).
    # Only the configured CONTROL_COORDINATOR_ROLE will accept "cmd":"rekey";
    # non-coordinator nodes respond with coordinator_only.
    "ENABLE_TCP_CONTROL": False,
    # Rekey coordinator/dominator role for the in-band two-phase commit.
    # This does NOT change the TCP handshake roles (GCS still serves, drone still connects);
    # it only defines which side initiates prepare/commit and triggers the rekey handshake.
    # Allowed: "gcs" (default, legacy) or "drone".
    "CONTROL_COORDINATOR_ROLE": "gcs",
    # Control server bind host for the drone follower. Default to the
    # configured drone host (LAN) so remote GCS schedulers can reach
    # the control RPCs by default. Use env override `DRONE_CONTROL_HOST`
    # for special cases (loopback-only testing).
    "DRONE_CONTROL_HOST": _DEFAULT_DRONE_HOST,
    "DRONE_CONTROL_PORT": 48080,
    # Optional control listener settings for the GCS host.
    # Used by core's TCP control server (when enabled) and by tooling.
    # Bind to 0.0.0.0 by default to accept local + remote commands.
    "GCS_CONTROL_HOST": "0.0.0.0",
    "GCS_CONTROL_PORT": 48080,
    # Telemetry port for GCS -> Drone feedback channel (UDP)
    "GCS_TELEMETRY_PORT": 52080,
        # Encrypted-plane control channel used by certain schedulers to route
        # drone-originated control/status back to the GCS when ENABLE_PACKET_TYPE is set.
        # Keep distinct from the plaintext follower RPC port to avoid conflicts.
     "DRONE_TO_GCS_CTL_PORT": 48181,
    "SIMPLE_VERIFY_TIMEOUT_S": 5.0,
    "SIMPLE_PACKETS_PER_SUITE": 1,
    "SIMPLE_PACKET_DELAY_S": 0.0,
    "SIMPLE_SUITE_DWELL_S": 0.0,
    # Default initial suite used by simple automation drivers and schedulers.
    # Keep suite IDs centralized in core.suites; tooling should fall back to it.
    "SIMPLE_INITIAL_SUITE": None,

    # Primitive benchmark coverage lists used by metrics tools and tests.
    # Keep these aligned with supported algorithms on target hardware.
    "PRIMITIVE_TEST_KEMS": [
        "ML-KEM-768",
        "Kyber512",
        "HQC-192",
    ],
    "PRIMITIVE_TEST_SIGS": [
        "ML-DSA-65",
        "Falcon-512",
        "SPHINCS+-SHA2-128s-simple",
    ],
    "PRIMITIVE_TEST_AEADS": [
        "aesgcm",
        "chacha20poly1305",
    ],
    # Automation defaults for tools/auto orchestration scripts
    "AUTO_DRONE": {
        # Session IDs default to "<prefix>_<unix>" unless DRONE_SESSION_ID env overrides
        "session_prefix": "run",
        # Optional explicit initial suite override (None -> defer to tooling defaults).
        "initial_suite": None,
        # Enable follower monitors (perf/pidstat/psutil) by default
        "monitors_enabled": True,
        # Apply CPU governor tweaks unless disabled
        "cpu_optimize": True,
        # Enable telemetry publisher back to the scheduler
        "telemetry_enabled": True,
        # Optional explicit telemetry host/port (None -> derive from CONTROL_HOST defaults)
        "telemetry_host": None,
        "telemetry_port": 52080,
        # Override monitoring output base directory (None -> DEFAULT_MONITOR_BASE)
        "monitor_output_base": None,
        # Optional environment exports applied before creating the power monitor
        "power_env": {
            # Maintain 1 kHz sampling by default; backend remains auto unless overridden
            "DRONE_POWER_BACKEND": "ina219",
            "DRONE_POWER_SAMPLE_HZ": "1000",
            "INA219_I2C_BUS": "1",
            "INA219_ADDR": "0x40",
            "INA219_SHUNT_OHM": "0.1",
        },
        # Synthetic flight model defaults used when power telemetry
        # translates PQC utilization into flight endurance estimates.
        "mock_mass_kg": 6.5,
        "kinematics_horizontal_mps": 13.0,
        "kinematics_vertical_mps": 3.5,
        "kinematics_cycle_s": 18.0,
        "kinematics_yaw_rate_dps": 45.0,
        # Toggle MAVProxy launch vs. legacy UDP echo helper.
        # MAVProxy stays enabled by default to keep parity with lab setups.
        "mavproxy_enabled": True,
        "udp_echo_enabled": False,
    },

    "AUTO_GCS": {
        # Session IDs default to "<prefix>_<unix>" unless GCS_SESSION_ID env overrides
        "session_prefix": "run",  # string prefix for run IDs
        # Traffic profile: "blast", "constant", "mavproxy", or "saturation"
        "traffic": "constant",  # modes: constant|blast|mavproxy|saturation
        # Traffic engine: "native" (built-in blaster) or "iperf3" (external client)
    "traffic_engine": "native",  # generator: native|iperf3
        # Duration for active traffic window per suite (seconds)
        # For cores+DVFS sweeps we default to short, aggressive 10s windows.
        "duration_s": 10.0,  # positive float seconds
        # Delay after rekey before starting traffic (seconds)
        "pre_gap_s": 1.0,  # non-negative float seconds (keep a short warmup)
        # Delay between suites (seconds). Shorten for faster cores+DVFS sweeps.
        "inter_gap_s": 5.0,  # non-negative float seconds
        # UDP payload size (bytes) for blaster calculations
        # Use a near-MTU payload to stress the data plane.
        "payload_bytes": 1200,  # payload bytes (>0)
        # Sample every Nth send/receive event (0 disables)
        "event_sample": 100,  # packets between samples (>=0)
        # Number of full passes across suite list. For DVFS sweeps, set this
        # to match the number of DVFS combos (e.g. 13 for 600-1800 MHz in
        # 100 MHz steps) so each pass runs the full suite set at a fixed
        # CPU frequency.
        "passes": 13,  # positive integer
        # Explicit packets-per-second override; 0 means best-effort
        "rate_pps": 0,  # packets/sec (>=0)
        # Optional bandwidth target in Mbps (converted to PPS if > 0)
        # Default to ~10 Mbps to exercise realistic airlink load.
        "bandwidth_mbps": 10.0,  # Mbps target (>=0)
        # Max rate explored during saturation sweeps (Mbps)
        "max_rate_mbps": 200.0,  # saturation upper bound Mbps (>0)
        # Optional ordered suite subset (None -> all suites from core.suites, including ChaCha20-Poly1305 and ASCON variants)
        # Set to None to run the full suite matrix
    "suites": None,
        # Launch local GCS proxy under scheduler control
        "launch_proxy": True,  # bool controls local proxy launch
        # Enable local proxy monitors (perf/pidstat/psutil)
        "monitors_enabled": True,  # bool controlling monitor sidecars
        # Start telemetry collector on the scheduler side
        "telemetry_enabled": True,  # bool gating telemetry collector
        # Bind/port for telemetry collector (defaults to CONFIG values)
        "telemetry_bind_host": "0.0.0.0",  # bind address string
        "telemetry_port": 52080,  # telemetry listen port (1-65535)
        # Emit combined Excel workbook when run completes
        "export_combined_excel": True,  # bool to generate combined workbook
        # Optional iperf3 configuration used when traffic_engine == "iperf3"
        "iperf3": {
            "server_host": None,  # override iperf3 server host or None for default
            "server_port": 5201,  # iperf3 UDP port (1-65535)
            "binary": "iperf3",  # iperf3 executable path/name
            "extra_args": [],  # additional CLI args list
            "force_cli": False,  # bool to force CLI output mode
        },
    # Blocklist of AEAD tokens to exclude from automation runs (case-insensitive)
    "aead_exclude_tokens": [],
            # Optional post-run fetch of drone artifacts (logs, power captures)
            "post_fetch": {
                # Legacy remote-fetch pipeline is disabled; artifacts must be synced via Git.
                "enabled": False,
                "host": _DEFAULT_DRONE_HOST,
                "username": "dev",
                "password": os.getenv("AUTO_GCS_POST_FETCH_PASSWORD", _LAB_PASSWORD_DEFAULT),
                "key": None,
                "strategy": "disabled",
                "port": 22,
                "logs_remote": "~/research/logs/auto/drone",
                "logs_local": "logs/auto",
                "output_remote": "~/research/output/drone",
                "output_local": "output/drone",
            },
            # Enable remote power fetch and set the SCP/SFTP target
        # Power fetch now relies on locally synced artifacts instead of remote copy.
        "power_fetch_enabled": False,
        "power_fetch_target": f"dev@{_DEFAULT_DRONE_HOST}",
        "artifact_fetch_strategy": "auto",  # Default fetch strategy for artifacts (auto selects best available)
        "post_report": {
            "enabled": True,  # bool toggling post-run report generation
            "script": "tools/report_constant_run.py",  # reporting script path
            "output_dir": "output/gcs",  # base output directory
            "table_name": "run_summary_table.md",  # Markdown table filename
            "text_name": "run_suite_summaries.txt",  # narrative summary filename
        },
        # Non-interactive SFTP password for POWER fetch (used by gcs_scheduler._sftp_fetch)
        # Set to None to prefer key/agent-based auth. For development convenience we
        # populate it here; in production prefer using an SSH agent or per-run env var.
    "power_fetch_password": os.getenv("AUTO_GCS_POWER_FETCH_PASSWORD", _LAB_PASSWORD_DEFAULT),
        # Optional explicit private key for power fetch operations (overrides agent lookup)
        "power_fetch_key": None,
    },

    # Allow plaintext host bindings to be non-loopback by default so LAN runners work
    # without env overrides. Set to False to force loopback-only bindings.
    "ALLOW_NON_LOOPBACK_PLAINTEXT": True,
}


# Required keys with their expected types
_REQUIRED_KEYS = {
    "TCP_HANDSHAKE_PORT": int,
    "UDP_DRONE_RX": int,
    "UDP_GCS_RX": int,
    "DRONE_PLAINTEXT_TX": int,
    "DRONE_PLAINTEXT_RX": int,
    "GCS_PLAINTEXT_TX": int,
    "GCS_PLAINTEXT_RX": int,
    "DRONE_HOST": str,
    "GCS_HOST": str,
    "DRONE_PLAINTEXT_HOST": str,
    "GCS_PLAINTEXT_HOST": str,
    "REPLAY_WINDOW": int,
    "WIRE_VERSION": int,
    "ENABLE_PACKET_TYPE": bool,
    "ENABLE_ASCON": bool,
    "ENABLE_ASCON128A": bool,
    "STRICT_UDP_PEER_MATCH": bool,
    "STRICT_HANDSHAKE_IP": bool,
    "LOG_SESSION_ID": bool,
    "DRONE_PSK": str,
    "REKEY_HANDSHAKE_TIMEOUT": float,
    "ASCON_STRICT_KEY_SIZE": bool,
    "DRONE_TO_GCS_CTL_PORT": int,
    "DRONE_CONTROL_HOST": str,
    "DRONE_CONTROL_PORT": int,
    "GCS_CONTROL_HOST": str,
    "GCS_CONTROL_PORT": int,
    "GCS_TELEMETRY_PORT": int,
}

# Env-overridable keys that are not part of _REQUIRED_KEYS but still need type parsing.
_ENV_OPTIONAL_TYPES = {
    "ENABLE_TCP_CONTROL": bool,
    "CONTROL_COORDINATOR_ROLE": str,
}

# Keys that can be overridden by environment variables
_ENV_OVERRIDABLE = {
        "ENABLE_TCP_CONTROL",
    "CONTROL_COORDINATOR_ROLE",
    "DRONE_HOST",
    "GCS_HOST",
    "TCP_HANDSHAKE_PORT",
    "UDP_DRONE_RX", 
    "UDP_GCS_RX",
    "DRONE_PLAINTEXT_TX",  # Added for testing/benchmarking flexibility
    "DRONE_PLAINTEXT_RX",  # Added for testing/benchmarking flexibility  
    "GCS_PLAINTEXT_TX",    # Added for testing/benchmarking flexibility
    "GCS_PLAINTEXT_RX",    # Added for testing/benchmarking flexibility
    "DRONE_CONTROL_PORT",
    "DRONE_CONTROL_HOST",
    "GCS_CONTROL_PORT",
    "GCS_CONTROL_HOST",
    "GCS_TELEMETRY_PORT",
    "DRONE_TO_GCS_CTL_PORT",
    "ENABLE_PACKET_TYPE",
    "ENABLE_ASCON",
    "ENABLE_ASCON128A",
    "STRICT_UDP_PEER_MATCH",
    "STRICT_HANDSHAKE_IP",
    "LOG_SESSION_ID",
    "DRONE_PSK",
    "ASCON_STRICT_KEY_SIZE",
}


def validate_config(cfg: Dict[str, Any]) -> None:
    """
    Ensure all required keys exist with correct types/ranges.
    Raise NotImplementedError("<reason>") on any violation.
    No return value on success.
    """
    # Check all required keys exist
    missing_keys = set(_REQUIRED_KEYS.keys()) - set(cfg.keys())
    if missing_keys:
        raise ConfigError(f"CONFIG missing required keys: {', '.join(sorted(missing_keys))}")
    
    # Check types for all keys
    for key, expected_type in _REQUIRED_KEYS.items():
        value = cfg[key]
        if key == "REKEY_HANDSHAKE_TIMEOUT":
            if not isinstance(value, (int, float)):
                raise ConfigError(
                    f"CONFIG[{key}] must be float seconds, got {type(value).__name__}"
                )
            continue
        if not isinstance(value, expected_type):
            raise ConfigError(f"CONFIG[{key}] must be {expected_type.__name__}, got {type(value).__name__}")
    
    # Validate port ranges
    for key in _REQUIRED_KEYS:
        if key.endswith("_PORT") or key.endswith("_RX") or key.endswith("_TX"):
            port = cfg[key]
            if not (1 <= port <= 65535):
                raise ConfigError(f"CONFIG[{key}] must be valid port (1-65535), got {port}")
    
    # Validate specific constraints
    if cfg["WIRE_VERSION"] != 1:
        raise ConfigError(f"CONFIG[WIRE_VERSION] must be 1 (frozen), got {cfg['WIRE_VERSION']}")
    
    if cfg["REPLAY_WINDOW"] < 64:
        raise ConfigError(f"CONFIG[REPLAY_WINDOW] must be >= 64, got {cfg['REPLAY_WINDOW']}")
    if cfg["REPLAY_WINDOW"] > 8192:
        raise ConfigError(f"CONFIG[REPLAY_WINDOW] must be <= 8192, got {cfg['REPLAY_WINDOW']}")
    
    # Validate hosts are valid strings (basic check)
    for host_key in ["DRONE_HOST", "GCS_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise ConfigError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        try:
            ip_address(host)
        except ValueError as exc:
            raise ConfigError(f"CONFIG[{host_key}] must be a valid IP address: {exc}")

    # Loopback hosts for plaintext path may remain hostnames (e.g., 127.0.0.1).
    # Allow override via CONFIG key or environment variable for backward compatibility
    allow_non_loopback_plaintext_env = str(os.environ.get("ALLOW_NON_LOOPBACK_PLAINTEXT", "")).strip().lower() in {
        "1",
        "true",
        "yes",
        "on",
    }
    allow_non_loopback_plaintext_cfg = bool(cfg.get("ALLOW_NON_LOOPBACK_PLAINTEXT", False))
    allow_non_loopback_plaintext = allow_non_loopback_plaintext_cfg or allow_non_loopback_plaintext_env
    for host_key in ["DRONE_PLAINTEXT_HOST", "GCS_PLAINTEXT_HOST"]:
        host = cfg[host_key]
        if not host or not isinstance(host, str):
            raise ConfigError(f"CONFIG[{host_key}] must be non-empty string, got {repr(host)}")
        if allow_non_loopback_plaintext:
            continue
        try:
            parsed = ip_address(host)
            if not parsed.is_loopback:
                raise ConfigError(
                    f"CONFIG[{host_key}] must be a loopback address unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
        except ValueError:
            if host.lower() != "localhost":
                raise ConfigError(
                    f"CONFIG[{host_key}] must be a loopback address (localhost) unless ALLOW_NON_LOOPBACK_PLAINTEXT is set"
                )
    
    # Optional keys are intentionally not required; do light validation if present
    if "ENCRYPTED_DSCP" in cfg and cfg["ENCRYPTED_DSCP"] is not None:
        if not (0 <= int(cfg["ENCRYPTED_DSCP"]) <= 63):
            raise ConfigError("CONFIG[ENCRYPTED_DSCP] must be 0..63 or None")

    if "ENABLE_TCP_CONTROL" in cfg:
        if not isinstance(cfg["ENABLE_TCP_CONTROL"], bool):
            raise ConfigError("CONFIG[ENABLE_TCP_CONTROL] must be bool")

    coord = cfg.get("CONTROL_COORDINATOR_ROLE", "gcs")
    if coord is not None:
        if not isinstance(coord, str):
            raise ConfigError("CONFIG[CONTROL_COORDINATOR_ROLE] must be str")
        coord_norm = coord.strip().lower()
        if coord_norm not in {"gcs", "drone"}:
            raise ConfigError("CONFIG[CONTROL_COORDINATOR_ROLE] must be 'gcs' or 'drone'")

    # Validate DRONE_PSK: require in non-dev environments; allow empty in dev.
    psk = cfg.get("DRONE_PSK", "")
    if os.getenv("ENV", "dev") != "dev" and not psk:
        raise ConfigError("CONFIG[DRONE_PSK] must be provided in non-dev environment")
    if psk:
        try:
            psk_bytes = bytes.fromhex(psk)
        except ValueError:
            raise ConfigError("CONFIG[DRONE_PSK] must be a hex string")
        if len(psk_bytes) != 32:
            raise ConfigError("CONFIG[DRONE_PSK] must decode to 32 bytes")


def _apply_env_overrides(cfg: Dict[str, Any]) -> Dict[str, Any]:
    """Apply environment variable overrides to config."""
    result = cfg.copy()
    
    for key in _ENV_OVERRIDABLE:
        env_var = key
        if env_var in os.environ:
            env_value = os.environ[env_var]
            expected_type = _REQUIRED_KEYS.get(key) or _ENV_OPTIONAL_TYPES.get(key)
            if expected_type is None:
                raise ConfigError(f"Unsupported env override key: {env_var}")
            
            try:
                if expected_type == int:
                    result[key] = int(env_value)
                elif expected_type == str:
                    result[key] = str(env_value)
                elif expected_type == bool:
                    lowered = str(env_value).strip().lower()
                    if lowered in {"1", "true", "yes", "on"}:
                        result[key] = True
                    elif lowered in {"0", "false", "no", "off"}:
                        result[key] = False
                    else:
                        raise ValueError(f"invalid boolean literal: {env_value}")
                elif expected_type == float:
                    result[key] = float(env_value)
                else:
                    raise ConfigError(f"Unsupported type for env override: {expected_type}")
            except ValueError:
                raise ConfigError(f"Invalid {expected_type.__name__} value for {env_var}: {env_value}")
    
    return result


# Apply environment overrides and validate
CONFIG = _apply_env_overrides(CONFIG)
validate_config(CONFIG)

==================================================

core\control_tcp.py
==================================================
"""TCP JSON control server for core proxy.

This is intentionally small and dependency-free. It exists to bridge external
controllers/schedulers that speak the legacy TCP JSON protocol:

  {"cmd": "rekey", "suite": "cs-..."}

into the core in-band control plane (policy_engine.request_prepare).

Security model:
- The listener is expected to bind on a trusted interface.
- Commands are accepted only from an allow-list of peer IPs.
- Rekey commands are further restricted: only the drone host may initiate rekey.

This module must never log secrets.
"""

from __future__ import annotations

import json
import socket
import threading
from dataclasses import dataclass
from typing import Callable, Iterable, Optional

from core.logging_utils import get_logger
from core.policy_engine import ControlState, coordinator_role_from_config, is_coordinator, request_prepare
from core.suites import get_suite


_logger = get_logger("pqc")


@dataclass(frozen=True)
class ControlTcpConfig:
    host: str
    port: int
    allowed_peers: tuple[str, ...]
    rekey_allowed_peers: tuple[str, ...]
    role: str
    coordinator_role: str


class ControlTcpServer:
    """A small threaded TCP server that reads newline-delimited JSON."""

    def __init__(
        self,
        config: ControlTcpConfig,
        control_state: ControlState,
        *,
        quiet: bool = False,
    ) -> None:
        self._cfg = config
        self._state = control_state
        self._quiet = quiet
        self._stop = threading.Event()
        self._thread: Optional[threading.Thread] = None
        self._sock: Optional[socket.socket] = None

    def start(self) -> bool:
        if self._thread and self._thread.is_alive():
            return True
        try:
            srv = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            srv.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            srv.bind((self._cfg.host, self._cfg.port))
            srv.listen(8)
            srv.settimeout(0.5)
            self._sock = srv
        except OSError as exc:
            _logger.warning(
                "TCP control listener failed to start",
                extra={
                    "role": self._cfg.role,
                    "host": self._cfg.host,
                    "port": self._cfg.port,
                    "error": str(exc),
                },
            )
            return False

        self._thread = threading.Thread(target=self._accept_loop, daemon=True)
        self._thread.start()
        if not self._quiet:
            _logger.info(
                "TCP control listener started",
                extra={
                    "role": self._cfg.role,
                    "host": self._cfg.host,
                    "port": self._cfg.port,
                    "allowed_peers": list(self._cfg.allowed_peers),
                },
            )
        return True

    def stop(self) -> None:
        self._stop.set()
        if self._sock is not None:
            try:
                self._sock.close()
            except OSError:
                pass
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=1.0)

    def _accept_loop(self) -> None:
        assert self._sock is not None
        while not self._stop.is_set():
            try:
                conn, addr = self._sock.accept()
            except socket.timeout:
                continue
            except OSError:
                break
            except Exception as exc:
                # Defensive: keep listener alive; do not log secrets.
                _logger.debug(
                    "TCP control accept loop error",
                    extra={"role": self._cfg.role, "error": str(exc)},
                )
                continue

            t = threading.Thread(target=self._client_loop, args=(conn, addr), daemon=True)
            t.start()

    def _client_loop(self, conn: socket.socket, addr: tuple[str, int]) -> None:
        peer_ip = addr[0]
        try:
            conn.settimeout(5.0)
            with conn:
                if not _is_allowed_peer(peer_ip, self._cfg.allowed_peers):
                    _send_json(conn, {"ok": False, "error": "unauthorized"})
                    return

                # Read line-by-line using raw socket recv to avoid buffering issues
                buf = b""
                while True:
                    if self._stop.is_set():
                        return
                    try:
                        chunk = conn.recv(4096)
                    except socket.timeout:
                        continue
                    if not chunk:
                        # EOF reached
                        return
                    buf += chunk
                    while b"\n" in buf:
                        line_bytes, buf = buf.split(b"\n", 1)
                        line = line_bytes.decode("utf-8", errors="replace").strip()
                        if not line:
                            continue
                        try:
                            msg = json.loads(line)
                        except json.JSONDecodeError:
                            _send_json(conn, {"ok": False, "error": "bad_json"})
                            continue
                        if not isinstance(msg, dict):
                            _send_json(conn, {"ok": False, "error": "bad_message"})
                            continue
                        try:
                            resp = self._handle_message(msg, peer_ip)
                        except Exception as exc:
                            _logger.warning(
                                "TCP control _handle_message exception",
                                extra={"role": self._cfg.role, "peer": peer_ip, "error": str(exc), "cmd": msg.get("cmd")},
                            )
                            resp = {"ok": False, "error": f"internal_error:{type(exc).__name__}"}
                        _send_json(conn, resp)
        except (OSError, ValueError, json.JSONDecodeError) as exc:
            _logger.debug(
                "TCP control client loop socket/parse error",
                extra={"role": self._cfg.role, "peer": peer_ip, "error": str(exc)},
            )
            return
        except Exception as exc:
            _logger.warning(
                "TCP control client loop error",
                extra={"role": self._cfg.role, "peer": peer_ip, "error": str(exc)},
            )
            return

    def _handle_message(self, msg: dict, peer_ip: str) -> dict:
        cmd = msg.get("cmd")
        if not isinstance(cmd, str):
            return {"ok": False, "error": "missing_cmd"}

        cmd_lower = cmd.lower().strip()

        # Log command receipt for debugging
        _logger.debug(
            "TCP control received command",
            extra={"role": self._cfg.role, "peer": peer_ip, "cmd": cmd_lower},
        )

        if cmd_lower in {"ping", "health"}:
            return {"ok": True, "role": self._cfg.role, "coordinator_role": self._cfg.coordinator_role}

        if cmd_lower == "status":
            with self._state.lock:
                return {
                    "ok": True,
                    "role": self._cfg.role,
                    "state": self._state.state,
                    "suite": self._state.current_suite,
                    "stats": dict(self._state.stats),
                    "active_rid": self._state.active_rid,
                    "last_rekey_ms": self._state.last_rekey_ms,
                    "last_rekey_suite": self._state.last_rekey_suite,
                    "last_status": self._state.last_status,
                }

        if cmd_lower == "rekey":
            if not _is_allowed_rekey_peer(
                peer_ip,
                rekey_allowed_peers=self._cfg.rekey_allowed_peers,
                server_role=self._cfg.role,
            ):
                return {"ok": False, "error": "unauthorized_rekey"}
            if not is_coordinator(role=self._cfg.role, coordinator_role=self._cfg.coordinator_role):
                return {"ok": False, "error": "coordinator_only", "coordinator_role": self._cfg.coordinator_role}
            suite = msg.get("suite")
            if not isinstance(suite, str) or not suite.strip():
                return {"ok": False, "error": "missing_suite"}
            try:
                suite_dict = get_suite(suite)
                suite_id = suite_dict.get("suite_id") if isinstance(suite_dict, dict) else None
                if not isinstance(suite_id, str) or not suite_id.strip():
                    return {"ok": False, "error": "invalid_suite"}
                rid = request_prepare(self._state, suite_id)
                return {"ok": True, "rid": rid, "suite": suite_id}
            except RuntimeError as exc:
                return {"ok": False, "error": f"busy:{exc}"}
            except Exception as exc:
                _logger.debug(
                    "TCP control rekey failed",
                    extra={
                        "role": self._cfg.role,
                        "peer": peer_ip,
                        "error": str(exc),
                        "error_type": type(exc).__name__,
                    },
                )
                return {"ok": False, "error": f"rekey_failed:{type(exc).__name__}"}

        return {"ok": False, "error": "unknown_cmd"}


def _send_json(conn: socket.socket, payload: dict) -> None:
    try:
        data = json.dumps(payload, separators=(",", ":"), sort_keys=True)
    except (TypeError, ValueError):
        data = "{\"ok\":false,\"error\":\"encode_fail\"}"
    try:
        conn.sendall((data + "\n").encode("utf-8", errors="replace"))
    except OSError:
        pass


def _is_allowed_peer(peer_ip: str, allowed_peers: Iterable[str]) -> bool:
    for allowed in allowed_peers:
        if peer_ip == allowed:
            return True
    # Always allow loopback.
    return peer_ip in {"127.0.0.1", "::1"}


def _is_allowed_rekey_peer(*, peer_ip: str, rekey_allowed_peers: Iterable[str], server_role: str) -> bool:
    """Return True if this peer may request cmd=rekey.

    Policy:
    - Only the drone host(s) may initiate rekey.
    - Additionally allow loopback only when the control listener runs on the drone itself,
      so local drone tooling can drive rekeys without exposing that power to the GCS host.
    """

    for allowed in rekey_allowed_peers:
        if peer_ip == allowed:
            return True

    if server_role == "drone" and peer_ip in {"127.0.0.1", "::1"}:
        return True
    return False


def build_allowed_peers(*, cfg: dict) -> tuple[str, ...]:
    """Build peer allow-list from CONFIG.

    Includes LAN + tailscale endpoints when present.
    """

    peers: list[str] = []
    for key in (
        "DRONE_HOST",
        "GCS_HOST",
        "DRONE_HOST_LAN",
        "GCS_HOST_LAN",
        "DRONE_HOST_TAILSCALE",
        "GCS_HOST_TAILSCALE",
    ):
        value = cfg.get(key)
        if isinstance(value, str) and value and value not in peers:
            peers.append(value)
    return tuple(peers)


def build_rekey_allowed_peers(*, cfg: dict) -> tuple[str, ...]:
    """Build allow-list for cmd=rekey.

    Restrict to drone endpoints only.
    """

    peers: list[str] = []
    for key in (
        "DRONE_HOST",
        "DRONE_HOST_LAN",
        "DRONE_HOST_TAILSCALE",
    ):
        value = cfg.get(key)
        if isinstance(value, str) and value and value not in peers:
            peers.append(value)
    return tuple(peers)


def start_control_server_if_enabled(
    *,
    role: str,
    cfg: dict,
    control_state: ControlState,
    quiet: bool,
    enabled: bool,
) -> Optional[ControlTcpServer]:
    if not enabled:
        return None

    host_key = "GCS_CONTROL_HOST" if role == "gcs" else "DRONE_CONTROL_HOST"
    port_key = "GCS_CONTROL_PORT" if role == "gcs" else "DRONE_CONTROL_PORT"
    host = str(cfg.get(host_key) or "0.0.0.0")
    port = int(cfg.get(port_key) or 48080)

    coordinator_role = coordinator_role_from_config(cfg)

    server = ControlTcpServer(
        ControlTcpConfig(
            host=host,
            port=port,
            allowed_peers=build_allowed_peers(cfg=cfg),
            rekey_allowed_peers=build_rekey_allowed_peers(cfg=cfg),
            role=role,
            coordinator_role=coordinator_role,
        ),
        control_state,
        quiet=quiet,
    )
    if not server.start():
        return None
    return server

==================================================

core\exceptions.py
==================================================
"""Project-specific exception types for clearer error semantics."""

class ConfigError(NotImplementedError, ValueError):
    """Configuration validation errors (subclass of NotImplementedError for legacy callers)."""
    pass

class SequenceOverflow(Exception):
    """Sequence space exhausted or nearing exhaustion."""
    pass

class HandshakeError(Exception):
    """Handshake protocol level errors."""
    pass

class AeadError(Exception):
    """AEAD-related errors."""
    pass

class HandshakeFormatError(HandshakeError):
    pass

class HandshakeVerifyError(HandshakeError):
    pass

==================================================

core\handshake.py
==================================================
from dataclasses import dataclass, field
import hashlib
import hmac
import os
import struct
import time
from typing import Dict, Optional
from core.config import CONFIG
from core.suites import get_suite
from core.logging_utils import get_logger
try:
    from oqs.oqs import KeyEncapsulation, Signature
except Exception:
    KeyEncapsulation = None  # type: ignore
    Signature = None  # type: ignore
from core.exceptions import HandshakeError, HandshakeFormatError, HandshakeVerifyError

logger = get_logger("pqc")


def _ns_to_ms(value: object) -> float:
    try:
        ns = float(value)
    except (TypeError, ValueError):
        return 0.0
    if ns <= 0.0:
        return 0.0
    return round(ns / 1_000_000.0, 6)


def _finalize_handshake_metrics(metrics: Optional[Dict[str, object]]) -> None:
    """Augment handshake metrics with flattened Part B fields."""

    if not isinstance(metrics, dict):  # defensive guard
        return

    primitives = metrics.setdefault("primitives", {})
    if not isinstance(primitives, dict):
        primitives = {}
        metrics["primitives"] = primitives

    kem_metrics = primitives.setdefault("kem", {})
    if not isinstance(kem_metrics, dict):
        kem_metrics = {}
        primitives["kem"] = kem_metrics

    sig_metrics = primitives.setdefault("signature", {})
    if not isinstance(sig_metrics, dict):
        sig_metrics = {}
        primitives["signature"] = sig_metrics

    artifacts = metrics.setdefault("artifacts", {})
    if not isinstance(artifacts, dict):
        artifacts = {}
        metrics["artifacts"] = artifacts

    def _export_time(prefix: str, source: Dict[str, object], key: str, legacy_key: Optional[str] = None) -> float:
        ns_value = source.get(key)
        ms_value = _ns_to_ms(ns_value)
        metrics[f"{prefix}_max_ms"] = ms_value
        metrics[f"{prefix}_avg_ms"] = ms_value
        if legacy_key:
            metrics.setdefault(legacy_key, ms_value)
        return ms_value

    kem_keygen_ms = _export_time("kem_keygen", kem_metrics, "keygen_ns", "kem_keygen_ms")
    kem_encaps_ms = _export_time("kem_encaps", kem_metrics, "encap_ns", "kem_encaps_ms")
    kem_decaps_ms = _export_time("kem_decaps", kem_metrics, "decap_ns", "kem_decap_ms")
    sig_sign_ms = _export_time("sig_sign", sig_metrics, "sign_ns", "sig_sign_ms")
    sig_verify_ms = _export_time("sig_verify", sig_metrics, "verify_ns", "sig_verify_ms")

    metrics["pub_key_size_bytes"] = int(kem_metrics.get("public_key_bytes") or artifacts.get("public_key_bytes") or 0)
    metrics["ciphertext_size_bytes"] = int(kem_metrics.get("ciphertext_bytes") or 0)
    metrics["sig_size_bytes"] = int(sig_metrics.get("signature_bytes") or artifacts.get("signature_bytes") or 0)
    metrics["shared_secret_size_bytes"] = int(kem_metrics.get("shared_secret_bytes") or 0)

    handshake_total_ns = metrics.get("handshake_total_ns")
    metrics["rekey_ms"] = _ns_to_ms(handshake_total_ns)

    primitive_total = kem_keygen_ms + kem_encaps_ms + kem_decaps_ms + sig_sign_ms + sig_verify_ms
    metrics["primitive_total_ms"] = round(primitive_total, 6)

@dataclass(frozen=True)
class ServerHello:
    version: int
    kem_name: bytes
    sig_name: bytes
    session_id: bytes
    kem_pub: bytes
    signature: bytes
    challenge: bytes
    metrics: Optional[Dict[str, object]] = None

@dataclass
class ServerEphemeral:
    kem_name: str
    sig_name: str
    session_id: bytes
    kem_obj: object  # oqs.KeyEncapsulation instance
    challenge: bytes
    metrics: Dict[str, object] = field(default_factory=dict)

def build_server_hello(
    suite_id: str,
    server_sig_obj,
    *,
    metrics: Optional[Dict[str, object]] = None,
):
    if KeyEncapsulation is None or Signature is None:
        raise RuntimeError("oqs-python not available (KeyEncapsulation/Signature missing)")
    suite = get_suite(suite_id)
    if not suite:
        raise ValueError("suite_id not found")
    version = CONFIG["WIRE_VERSION"]
    kem_name = suite["kem_name"].encode("utf-8")
    sig_name = suite["sig_name"].encode("utf-8")
    if not kem_name or not sig_name:
        raise ValueError("kem_name/sig_name empty")
    if not hasattr(server_sig_obj, "sign"):
        raise TypeError("server_sig_obj must provide sign()")
    session_id = os.urandom(8)
    challenge = os.urandom(8)
    metrics_ref = metrics if metrics is not None else {}
    metrics_ref.setdefault("role", "gcs")
    metrics_ref.setdefault("suite_id", suite_id)
    metrics_ref.setdefault("kem_name", suite["kem_name"])
    metrics_ref.setdefault("sig_name", suite["sig_name"])
    primitives = metrics_ref.setdefault("primitives", {})
    kem_metrics = primitives.setdefault("kem", {})
    sig_metrics = primitives.setdefault("signature", {})
    artifacts = metrics_ref.setdefault("artifacts", {})

    keygen_wall_start = time.time_ns()
    keygen_perf_start = time.perf_counter_ns()
    kem_obj = KeyEncapsulation(kem_name.decode("utf-8"))
    kem_pub = kem_obj.generate_keypair()
    keygen_perf_end = time.perf_counter_ns()
    keygen_wall_end = time.time_ns()
    kem_metrics["keygen_ns"] = keygen_perf_end - keygen_perf_start
    kem_metrics["keygen_wall_start_ns"] = keygen_wall_start
    kem_metrics["keygen_wall_end_ns"] = keygen_wall_end
    kem_metrics["public_key_bytes"] = len(kem_pub)
    # Include negotiated wire version as first byte of transcript to prevent downgrade
    transcript = (
        struct.pack("!B", version)
        + b"|pq-drone-gcs:v1|"
        + session_id
        + b"|"
        + kem_name
        + b"|"
        + sig_name
        + b"|"
        + kem_pub
        + b"|"
        + challenge
    )
    sign_wall_start = time.time_ns()
    sign_perf_start = time.perf_counter_ns()
    signature = server_sig_obj.sign(transcript)
    sign_perf_end = time.perf_counter_ns()
    sign_wall_end = time.time_ns()
    sig_metrics["sign_ns"] = sign_perf_end - sign_perf_start
    sig_metrics["sign_wall_start_ns"] = sign_wall_start
    sig_metrics["sign_wall_end_ns"] = sign_wall_end
    sig_metrics["signature_bytes"] = len(signature)
    wire = struct.pack("!B", version)
    wire += struct.pack("!H", len(kem_name)) + kem_name
    wire += struct.pack("!H", len(sig_name)) + sig_name
    wire += session_id
    wire += challenge
    wire += struct.pack("!I", len(kem_pub)) + kem_pub
    wire += struct.pack("!H", len(signature)) + signature
    artifacts["server_hello_bytes"] = len(wire)
    artifacts.setdefault("public_key_bytes", len(kem_pub))
    artifacts.setdefault("signature_bytes", len(signature))
    artifacts.setdefault("challenge_bytes", len(challenge))
    ephemeral = ServerEphemeral(
        kem_name=kem_name.decode("utf-8"),
        sig_name=sig_name.decode("utf-8"),
        session_id=session_id,
        kem_obj=kem_obj,
        challenge=challenge,
        metrics=metrics_ref,
    )
    return wire, ephemeral

def parse_and_verify_server_hello(
    wire: bytes,
    expected_version: int,
    server_sig_pub: bytes,
    *,
    metrics: Optional[Dict[str, object]] = None,
) -> ServerHello:
    if Signature is None:
        raise RuntimeError("oqs-python not available (Signature missing)")
    try:
        offset = 0
        version = wire[offset]
        offset += 1
        if version != expected_version:
            raise HandshakeFormatError("bad wire version")
        kem_name_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        kem_name = wire[offset:offset+kem_name_len]
        offset += kem_name_len
        sig_name_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        sig_name = wire[offset:offset+sig_name_len]
        offset += sig_name_len
        session_id = wire[offset:offset+8]
        offset += 8
        challenge = wire[offset:offset+8]
        offset += 8
        kem_pub_len = struct.unpack_from("!I", wire, offset)[0]
        offset += 4
        kem_pub = wire[offset:offset+kem_pub_len]
        offset += kem_pub_len
        sig_len = struct.unpack_from("!H", wire, offset)[0]
        offset += 2
        signature = wire[offset:offset+sig_len]
        offset += sig_len
    except Exception:
        raise HandshakeFormatError("malformed server hello")
    transcript = (
        struct.pack("!B", version)
        + b"|pq-drone-gcs:v1|"
        + session_id
        + b"|"
        + kem_name
        + b"|"
        + sig_name
        + b"|"
        + kem_pub
        + b"|"
        + challenge
    )
    metrics_ref = metrics
    if metrics_ref is not None:
        metrics_ref.setdefault("role", metrics_ref.get("role", "drone"))
        primitives = metrics_ref.setdefault("primitives", {})
        sig_metrics = primitives.setdefault("signature", {})
        kem_metrics = primitives.setdefault("kem", {})
        kem_metrics.setdefault("public_key_bytes", len(kem_pub))
        artifacts_ref = metrics_ref.setdefault("artifacts", {})
        artifacts_ref.setdefault("public_key_bytes", len(kem_pub))
    else:
        sig_metrics = None
    sig = None
    try:
        verify_wall_start = time.time_ns() if sig_metrics is not None else None
        verify_perf_start = time.perf_counter_ns() if sig_metrics is not None else None
        if Signature is None:
            raise RuntimeError("oqs-python not available (Signature missing)")
        sig = Signature(sig_name.decode("utf-8"))
        if not sig.verify(transcript, signature, server_sig_pub):
            raise HandshakeVerifyError("bad signature")
        if sig_metrics is not None and verify_perf_start is not None and verify_wall_start is not None:
            verify_perf_end = time.perf_counter_ns()
            verify_wall_end = time.time_ns()
            sig_metrics["verify_ns"] = verify_perf_end - verify_perf_start
            sig_metrics["verify_wall_start_ns"] = verify_wall_start
            sig_metrics["verify_wall_end_ns"] = verify_wall_end
            sig_metrics["signature_bytes"] = len(signature)
    except HandshakeVerifyError:
        raise
    except Exception as exc:
        raise HandshakeVerifyError(f"signature verification failed: {exc}")
    finally:
        if sig is not None and hasattr(sig, "free"):
            try:
                sig.free()
            except Exception:
                pass
    return ServerHello(
        version=version,
        kem_name=kem_name,
        sig_name=sig_name,
        session_id=session_id,
        kem_pub=kem_pub,
        signature=signature,
        challenge=challenge,
        metrics=metrics_ref,
    )

def _drone_psk_bytes() -> bytes:
    # Prefer environment injection for secrets. In non-dev environments require it.
    psk_hex = os.getenv("DRONE_PSK", CONFIG.get("DRONE_PSK", ""))
    if os.getenv("ENV", "dev") != "dev" and not psk_hex:
        raise RuntimeError("DRONE_PSK must be provided via environment in non-dev environment")
    if not psk_hex:
        # Allow empty during dev/testing but callers will likely fail to authenticate.
        return b""
    try:
        psk = bytes.fromhex(psk_hex)
    except ValueError as exc:
        raise ValueError(f"Invalid DRONE_PSK hex: {exc}")
    if len(psk) != 32:
        raise ValueError("DRONE_PSK must decode to 32 bytes")
    return psk


def client_encapsulate(server_hello: ServerHello, *, metrics: Optional[Dict[str, object]] = None):
    kem = None
    try:
        if KeyEncapsulation is None:
            raise RuntimeError("oqs-python not available (KeyEncapsulation missing)")
        kem = KeyEncapsulation(server_hello.kem_name.decode("utf-8"))
        metrics_ref = metrics if metrics is not None else getattr(server_hello, "metrics", None)
        encap_wall_start = time.time_ns() if metrics_ref is not None else None
        encap_perf_start = time.perf_counter_ns() if metrics_ref is not None else None
        kem_ct, shared_secret = kem.encap_secret(server_hello.kem_pub)
        if metrics_ref is not None and encap_perf_start is not None and encap_wall_start is not None:
            encap_perf_end = time.perf_counter_ns()
            encap_wall_end = time.time_ns()
            primitives = metrics_ref.setdefault("primitives", {})
            kem_metrics = primitives.setdefault("kem", {})
            kem_metrics["encap_ns"] = encap_perf_end - encap_perf_start
            kem_metrics["encap_wall_start_ns"] = encap_wall_start
            kem_metrics["encap_wall_end_ns"] = encap_wall_end
            kem_metrics["ciphertext_bytes"] = len(kem_ct)
            kem_metrics.setdefault("shared_secret_bytes", len(shared_secret))
        return kem_ct, shared_secret
    except Exception as exc:
        raise HandshakeError(f"client_encapsulate failed: {exc}")
    finally:
        if kem is not None and hasattr(kem, "free"):
            try:
                kem.free()
            except Exception:
                pass


def server_decapsulate(
    ephemeral: ServerEphemeral,
    kem_ct: bytes,
    *,
    metrics: Optional[Dict[str, object]] = None,
):
    kem_obj = getattr(ephemeral, "kem_obj", None)
    try:
        if kem_obj is None:
            raise HandshakeError("server_decapsulate missing kem_obj")
        metrics_ref = metrics if metrics is not None else getattr(ephemeral, "metrics", None)
        decap_wall_start = time.time_ns() if metrics_ref is not None else None
        decap_perf_start = time.perf_counter_ns() if metrics_ref is not None else None
        shared_secret = kem_obj.decap_secret(kem_ct)
        if metrics_ref is not None and decap_perf_start is not None and decap_wall_start is not None:
            decap_perf_end = time.perf_counter_ns()
            decap_wall_end = time.time_ns()
            primitives = metrics_ref.setdefault("primitives", {})
            kem_metrics = primitives.setdefault("kem", {})
            kem_metrics["decap_ns"] = decap_perf_end - decap_perf_start
            kem_metrics["decap_wall_start_ns"] = decap_wall_start
            kem_metrics["decap_wall_end_ns"] = decap_wall_end
            kem_metrics.setdefault("ciphertext_bytes", len(kem_ct))
            kem_metrics.setdefault("shared_secret_bytes", len(shared_secret))
        return shared_secret
    except Exception:
        raise HandshakeError("server_decapsulate failed")
    finally:
        if kem_obj is not None and hasattr(kem_obj, "free"):
            try:
                kem_obj.free()
            except Exception:
                pass
        if hasattr(ephemeral, "kem_obj"):
            ephemeral.kem_obj = None


def derive_transport_keys(
    role: str,
    session_id: bytes,
    kem_name: bytes,
    sig_name: bytes,
    shared_secret: bytes,
    *,
    metrics: Optional[Dict[str, object]] = None,
):
    if role not in {"client", "server"}:
        raise HandshakeFormatError("invalid role")
    if not (isinstance(session_id, bytes) and len(session_id) == 8):
        raise HandshakeFormatError("session_id must be 8 bytes")
    if not kem_name or not sig_name:
        raise HandshakeFormatError("kem_name/sig_name empty")
    try:
        from cryptography.hazmat.primitives.kdf.hkdf import HKDF
        from cryptography.hazmat.primitives import hashes
    except ImportError:
        raise RuntimeError("cryptography not available")
    metrics_ref = metrics
    derive_wall_start = time.time_ns() if metrics_ref is not None else None
    derive_perf_start = time.perf_counter_ns() if metrics_ref is not None else None
    info = b"pq-drone-gcs:kdf:v1|" + session_id + b"|" + kem_name + b"|" + sig_name
    hkdf = HKDF(
        algorithm=hashes.SHA256(),
        length=64,
        salt=b"pq-drone-gcs|hkdf|v1",
        info=info,
    )
    okm = hkdf.derive(shared_secret)
    if metrics_ref is not None and derive_perf_start is not None and derive_wall_start is not None:
        derive_perf_end = time.perf_counter_ns()
        derive_wall_end = time.time_ns()
        prefix = "server" if role == "server" else "client"
        metrics_ref[f"kdf_{prefix}_ns"] = derive_perf_end - derive_perf_start
        metrics_ref[f"kdf_{prefix}_wall_start_ns"] = derive_wall_start
        metrics_ref[f"kdf_{prefix}_wall_end_ns"] = derive_wall_end
    key_d2g = okm[:32]
    key_g2d = okm[32:64]

    if role == "client":
        # Drone acts as client; return (send_to_gcs, receive_from_gcs).
        return key_d2g, key_g2d
    else:  # server == GCS
        # GCS perspective: send_to_drone first, receive_from_drone second.
        return key_g2d, key_d2g
def server_gcs_handshake(conn, suite, gcs_sig_secret, *, timeout: float = 10.0):
    """Authenticated GCS side handshake.

    Requires a ready oqs.Signature object (with generated key pair). Fails fast if not.
    """
    from oqs.oqs import Signature
    import struct

    try:
        conn.settimeout(float(timeout))
    except Exception:
        conn.settimeout(10.0)

    if not isinstance(gcs_sig_secret, Signature):
        raise ValueError("gcs_sig_secret must be an oqs.Signature object with a loaded keypair")

    suite_id = suite.get("suite_id") if isinstance(suite, dict) else None
    if not suite_id:
        raise ValueError("suite must include suite_id")

    handshake_metrics: Dict[str, object] = {
        "role": "gcs",
        "suite_id": suite_id,
        "kem_name": suite.get("kem_name"),
        "sig_name": suite.get("sig_name"),
    }
    handshake_wall_start = time.time_ns()
    handshake_perf_start = time.perf_counter_ns()
    hello_wire, ephemeral = build_server_hello(suite_id, gcs_sig_secret, metrics=handshake_metrics)
    handshake_metrics["handshake_wall_start_ns"] = handshake_wall_start
    artifacts = handshake_metrics.setdefault("artifacts", {})
    artifacts.setdefault("server_hello_bytes", len(hello_wire))
    conn.sendall(struct.pack("!I", len(hello_wire)) + hello_wire)

    # Receive KEM ciphertext
    ct_len_bytes = b""
    while len(ct_len_bytes) < 4:
        chunk = conn.recv(4 - len(ct_len_bytes))
        if not chunk:
            raise ConnectionError("Connection closed reading ciphertext length")
        ct_len_bytes += chunk
    ct_len = struct.unpack("!I", ct_len_bytes)[0]
    kem_ct = b""
    while len(kem_ct) < ct_len:
        chunk = conn.recv(ct_len - len(kem_ct))
        if not chunk:
            raise ConnectionError("Connection closed reading ciphertext")
        kem_ct += chunk
    primitives = handshake_metrics.setdefault("primitives", {})
    kem_metrics = primitives.setdefault("kem", {})
    kem_metrics.setdefault("ciphertext_bytes", len(kem_ct))

    tag_len = hashlib.sha256().digest_size
    tag = b""
    while len(tag) < tag_len:
        chunk = conn.recv(tag_len - len(tag))
        if not chunk:
            raise ConnectionError("Connection closed reading drone authentication tag")
        tag += chunk
    artifacts["auth_tag_bytes"] = len(tag)

    expected_tag = hmac.new(_drone_psk_bytes(), hello_wire, hashlib.sha256).digest()
    if not hmac.compare_digest(tag, expected_tag):
        peer_ip = "unknown"
        try:
            peer_info = conn.getpeername()
            if isinstance(peer_info, tuple) and peer_info:
                peer_ip = str(peer_info[0])
            elif isinstance(peer_info, str) and peer_info:
                peer_ip = peer_info
        except (OSError, ValueError):
            peer_ip = "unknown"
        logger.warning(
            "Rejected drone handshake with bad authentication tag",
            extra={"role": "gcs", "expected_peer": CONFIG["DRONE_HOST"], "received": peer_ip},
        )
        raise HandshakeVerifyError("drone authentication failed")

    shared_secret = server_decapsulate(ephemeral, kem_ct, metrics=handshake_metrics)
    key_send, key_recv = derive_transport_keys(
        "server",
        ephemeral.session_id,
        ephemeral.kem_name.encode("utf-8"),
        ephemeral.sig_name.encode("utf-8"),
        shared_secret,
        metrics=handshake_metrics,
    )
    handshake_metrics["handshake_wall_end_ns"] = time.time_ns()
    handshake_metrics["handshake_total_ns"] = time.perf_counter_ns() - handshake_perf_start
    _finalize_handshake_metrics(handshake_metrics)
    return (
        key_recv,
        key_send,
        b"",
        b"",
        ephemeral.session_id,
        ephemeral.kem_name,
        ephemeral.sig_name,
        handshake_metrics,
    )

def client_drone_handshake(client_sock, suite, gcs_sig_public, *, timeout: float = 10.0):
    # Real handshake implementation with MANDATORY signature verification
    import struct
    
    # Add socket timeout to prevent hanging
    try:
        client_sock.settimeout(float(timeout))
    except Exception:
        client_sock.settimeout(10.0)
    
    handshake_metrics: Dict[str, object] = {
        "role": "drone",
        "suite_id": suite.get("suite_id") if isinstance(suite, dict) else None,
        "kem_name": suite.get("kem_name") if isinstance(suite, dict) else None,
        "sig_name": suite.get("sig_name") if isinstance(suite, dict) else None,
    }
    handshake_wall_start = time.time_ns()
    handshake_perf_start = time.perf_counter_ns()

    # Receive server hello with length prefix
    hello_len_bytes = b""
    while len(hello_len_bytes) < 4:
        chunk = client_sock.recv(4 - len(hello_len_bytes))
        if not chunk:
            raise ConnectionError("Connection closed reading hello length")
        hello_len_bytes += chunk
        
    hello_len = struct.unpack("!I", hello_len_bytes)[0]
    hello_wire = b""
    while len(hello_wire) < hello_len:
        chunk = client_sock.recv(hello_len - len(hello_wire))
        if not chunk:
            raise ConnectionError("Connection closed reading hello")
        hello_wire += chunk
    artifacts = handshake_metrics.setdefault("artifacts", {})
    artifacts["server_hello_bytes"] = len(hello_wire)

    # Parse and VERIFY server hello - NO BYPASS ALLOWED
    # This is critical for security - verification failure must abort
    hello = parse_and_verify_server_hello(
        hello_wire,
        CONFIG["WIRE_VERSION"],
        gcs_sig_public,
        metrics=handshake_metrics,
    )

    expected_kem = suite.get("kem_name") if isinstance(suite, dict) else None
    expected_sig = suite.get("sig_name") if isinstance(suite, dict) else None
    negotiated_kem = hello.kem_name.decode("utf-8") if isinstance(hello.kem_name, bytes) else hello.kem_name
    negotiated_sig = hello.sig_name.decode("utf-8") if isinstance(hello.sig_name, bytes) else hello.sig_name
    if expected_kem and negotiated_kem != expected_kem:
        logger.error(
            "Suite mismatch",
            extra={
                "expected_kem": expected_kem,
                "expected_sig": expected_sig,
                "negotiated_kem": negotiated_kem,
                "negotiated_sig": negotiated_sig,
            },
        )
        raise HandshakeVerifyError(
            f"Downgrade attempt detected: expected {expected_kem}, got {negotiated_kem}"
        )
    if expected_sig and negotiated_sig != expected_sig:
        logger.error(
            "Suite mismatch",
            extra={
                "expected_kem": expected_kem,
                "expected_sig": expected_sig,
                "negotiated_kem": negotiated_kem,
                "negotiated_sig": negotiated_sig,
            },
        )
        raise HandshakeVerifyError(
            f"Downgrade attempt detected: expected {expected_sig}, got {negotiated_sig}"
        )

    # Encapsulate and send KEM ciphertext + authentication tag
    kem_ct, shared_secret = client_encapsulate(hello, metrics=handshake_metrics)
    primitives = handshake_metrics.setdefault("primitives", {})
    kem_metrics = primitives.setdefault("kem", {})
    kem_metrics.setdefault("ciphertext_bytes", len(kem_ct))
    kem_metrics.setdefault("shared_secret_bytes", len(shared_secret))
    tag = hmac.new(_drone_psk_bytes(), hello_wire, hashlib.sha256).digest()
    client_sock.sendall(struct.pack("!I", len(kem_ct)) + kem_ct + tag)
    artifacts["auth_tag_bytes"] = len(tag)
    
    # Derive transport keys
    key_send, key_recv = derive_transport_keys(
        "client",
        hello.session_id,
        hello.kem_name,
        hello.sig_name,
        shared_secret,
        metrics=handshake_metrics,
    )

    handshake_metrics["handshake_wall_start_ns"] = handshake_wall_start
    handshake_metrics["handshake_wall_end_ns"] = time.time_ns()
    handshake_metrics["handshake_total_ns"] = time.perf_counter_ns() - handshake_perf_start
    _finalize_handshake_metrics(handshake_metrics)

    # Return in expected format (nonce seeds are unused)
    return (
        key_send,
        key_recv,
        b"",
        b"",
        hello.session_id,
        hello.kem_name.decode() if isinstance(hello.kem_name, bytes) else hello.kem_name,
        hello.sig_name.decode() if isinstance(hello.sig_name, bytes) else hello.sig_name,
        handshake_metrics,
    )


==================================================

core\logging_utils.py
==================================================
import json, logging, sys, time
from pathlib import Path

class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        payload = {
            "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(record.created)),
            "level": record.levelname,
            "name": record.name,
            "msg": record.getMessage(),
        }
        if record.exc_info:
            payload["exc_info"] = self.formatException(record.exc_info)
        # Allow extra fields via record.__dict__ (filtered)
        for k, v in record.__dict__.items():
            if k not in ("msg", "args", "exc_info", "exc_text", "stack_info", "stack_level", "created",
                         "msecs", "relativeCreated", "levelno", "levelname", "pathname", "filename",
                         "module", "lineno", "funcName", "thread", "threadName", "processName", "process"):
                try:
                    json.dumps({k: v})
                    payload[k] = v
                except Exception:
                    payload[k] = str(v)
        return json.dumps(payload)

def get_logger(name: str = "pqc") -> logging.Logger:
    logger = logging.getLogger(name)
    if logger.handlers:
        return logger
    logger.setLevel(logging.INFO)
    h = logging.StreamHandler(sys.stdout)
    h.setFormatter(JsonFormatter())
    logger.addHandler(h)
    logger.propagate = False
    return logger


def configure_file_logger(role: str, logger: logging.Logger | None = None) -> Path:
    """Attach a JSON file handler and return log path."""

    active_logger = logger or get_logger()

    # Drop any previous file handlers we attached to avoid duplicate writes during tests.
    for handler in list(active_logger.handlers):
        if getattr(handler, "_pqc_file_handler", False):
            active_logger.removeHandler(handler)
            try:
                handler.close()
            except Exception:
                pass

    logs_dir = Path("logs")
    logs_dir.mkdir(exist_ok=True)
    timestamp = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
    path = logs_dir / f"{role}-{timestamp}.log"

    file_handler = logging.FileHandler(path, encoding="utf-8")
    file_handler.setFormatter(JsonFormatter())
    file_handler._pqc_file_handler = True  # type: ignore[attr-defined]
    active_logger.addHandler(file_handler)

    return path

# Very small metrics hook (no deps)
class Counter:
    def __init__(self): self.value = 0
    def inc(self, n: int = 1): self.value += n

class Gauge:
    def __init__(self): self.value = 0
    def set(self, v: float): self.value = v

class Metrics:
    def __init__(self):
        self.counters = {}
        self.gauges = {}
    def counter(self, name: str) -> Counter:
        self.counters.setdefault(name, Counter()); return self.counters[name]
    def gauge(self, name: str) -> Gauge:
        self.gauges.setdefault(name, Gauge()); return self.gauges[name]

METRICS = Metrics()

==================================================

core\policy_engine.py
==================================================
"""
In-band control-plane state machine for interactive rekey negotiation.

Implements a two-phase commit protocol carried over packet type 0x02 payloads.
"""

from __future__ import annotations

import queue
import secrets
import threading
import time
from collections import deque
from dataclasses import dataclass, field
from typing import Callable, Dict, List, Optional, Tuple


def _now_ms() -> int:
    """Return monotonic milliseconds for control timestamps."""

    return time.monotonic_ns() // 1_000_000


def _default_safe() -> bool:
    return True


@dataclass
class ControlState:
    """Mutable control-plane state shared between proxy threads."""

    role: str
    coordinator_role: str
    current_suite: str
    safe_guard: Callable[[], bool] = field(default_factory=_default_safe)
    lock: threading.Lock = field(default_factory=threading.Lock)
    outbox: "queue.Queue[dict]" = field(default_factory=queue.Queue)
    pending: Dict[str, str] = field(default_factory=dict)
    state: str = "RUNNING"
    active_rid: Optional[str] = None
    last_rekey_ms: Optional[int] = None
    last_rekey_suite: Optional[str] = None
    last_status: Optional[Dict[str, object]] = None
    stats: Dict[str, int] = field(default_factory=lambda: {
        "prepare_sent": 0,
        "prepare_received": 0,
        "rekeys_ok": 0,
        "rekeys_fail": 0,
    })
    seen_rids: deque[str] = field(default_factory=lambda: deque(maxlen=256))


@dataclass
class ControlResult:
    """Outcome of processing a control message."""

    send: List[dict] = field(default_factory=list)
    start_handshake: Optional[Tuple[str, str]] = None  # (suite_id, rid)
    notes: List[str] = field(default_factory=list)


def create_control_state(role: str, suite_id: str, *, safe_guard: Callable[[], bool] | None = None) -> ControlState:
    """Initialise ControlState with the provided role and suite."""

    guard = safe_guard or _default_safe
    return ControlState(role=role, coordinator_role="gcs", current_suite=suite_id, safe_guard=guard)


def set_coordinator_role(state: ControlState, coordinator_role: str) -> None:
    """Set the coordinator role for the in-band rekey control-plane.

    coordinator_role must be either "gcs" or "drone".
    """

    role_norm = str(coordinator_role).strip().lower()
    if role_norm not in {"gcs", "drone"}:
        raise ValueError("invalid coordinator_role")
    with state.lock:
        state.coordinator_role = role_norm


def normalize_coordinator_role(value: object, *, default: str = "gcs") -> str:
    """Normalize coordinator roles to a safe value.

    Accepts arbitrary inputs and returns either "gcs" or "drone".
    """

    try:
        role_norm = str(value).strip().lower()
    except Exception:
        role_norm = ""

    if role_norm in {"gcs", "drone"}:
        return role_norm
    return "gcs" if default not in {"gcs", "drone"} else default


def coordinator_role_from_config(cfg: dict, *, default: str = "gcs") -> str:
    """Fetch and normalize CONTROL_COORDINATOR_ROLE from a config mapping."""

    if not isinstance(cfg, dict):
        return normalize_coordinator_role(default, default="gcs")
    return normalize_coordinator_role(cfg.get("CONTROL_COORDINATOR_ROLE", default), default=default)


def is_coordinator(*, role: str, coordinator_role: str) -> bool:
    """Return True if this proxy role is the coordinator role."""

    return normalize_coordinator_role(role) == normalize_coordinator_role(coordinator_role)


def generate_rid() -> str:
    """Generate a random 64-bit hex request identifier."""

    return secrets.token_hex(8)


def enqueue_json(state: ControlState, payload: dict) -> None:
    """Place an outbound JSON payload onto the control outbox."""

    state.outbox.put(payload)


def request_prepare(state: ControlState, suite_id: str) -> str:
    """Queue a prepare_rekey message and transition to NEGOTIATING."""

    rid = generate_rid()
    now = _now_ms()
    with state.lock:
        if state.state != "RUNNING":
            raise RuntimeError("control-plane already negotiating")
        state.pending[rid] = suite_id
        state.active_rid = rid
        state.state = "NEGOTIATING"
        state.stats["prepare_sent"] += 1
    enqueue_json(
        state,
        {
            "type": "prepare_rekey",
            "suite": suite_id,
            "rid": rid,
            "t_ms": now,
        },
    )
    return rid


def record_rekey_result(state: ControlState, rid: str, suite_id: str, *, success: bool) -> None:
    """Record outcome of a rekey attempt and enqueue status update."""

    now = _now_ms()
    status_payload = {
        "type": "status",
        "state": "RUNNING",
        "suite": suite_id if success else state.current_suite,
        "rid": rid,
        "result": "ok" if success else "fail",
        "t_ms": now,
    }
    with state.lock:
        if success:
            state.current_suite = suite_id
            state.last_rekey_suite = suite_id
            state.last_rekey_ms = now
            state.stats["rekeys_ok"] += 1
        else:
            state.stats["rekeys_fail"] += 1
        state.pending.pop(rid, None)
        state.active_rid = None
        state.state = "RUNNING"
    enqueue_json(state, status_payload)


def handle_control(msg: dict, role: str, state: ControlState) -> ControlResult:
    """Process inbound control JSON and return actions for the proxy."""

    result = ControlResult()
    msg_type = msg.get("type")
    if not isinstance(msg_type, str):
        result.notes.append("missing_type")
        return result

    rid = msg.get("rid")
    now = _now_ms()

    coordinator_role = state.coordinator_role
    is_coordinator = role == coordinator_role

    if is_coordinator:
        if msg_type == "prepare_ok" and isinstance(rid, str):
            with state.lock:
                suite = state.pending.get(rid)
                if not suite:
                    result.notes.append("unknown_rid")
                    return result
                state.state = "SWAPPING"
                state.seen_rids.append(rid)
            result.send.append({
                "type": "commit_rekey",
                "suite": suite,
                "rid": rid,
                "t_ms": now,
            })
            result.start_handshake = (suite, rid)
        elif msg_type == "prepare_fail" and isinstance(rid, str):
            reason = msg.get("reason", "unknown")
            with state.lock:
                state.pending.pop(rid, None)
                state.active_rid = None
                state.state = "RUNNING"
                state.stats["rekeys_fail"] += 1
                state.seen_rids.append(rid)
            result.notes.append(f"prepare_fail:{reason}")
        elif msg_type == "status":
            with state.lock:
                state.last_status = msg
        else:
            result.notes.append(f"ignored:{msg_type}")
        return result

    if msg_type == "prepare_rekey":
        suite = msg.get("suite")
        if not isinstance(rid, str) or not isinstance(suite, str):
            result.notes.append("invalid_prepare")
            return result

        with state.lock:
            if rid in state.seen_rids:
                allow = False
            else:
                allow = state.state == "RUNNING" and state.safe_guard()
            if allow:
                state.pending[rid] = suite
                state.active_rid = rid
                state.state = "NEGOTIATING"
                state.stats["prepare_received"] += 1
                state.seen_rids.append(rid)
        if allow:
            result.send.append({
                "type": "prepare_ok",
                "rid": rid,
                "t_ms": now,
            })
        else:
            result.send.append({
                "type": "prepare_fail",
                "rid": rid,
                "reason": "unsafe",
                "t_ms": now,
            })
    elif msg_type == "commit_rekey" and isinstance(rid, str):
        with state.lock:
            suite = state.pending.get(rid)
            if not suite:
                result.notes.append("unknown_commit_rid")
                return result
            state.state = "SWAPPING"
        result.start_handshake = (suite, rid)
    elif msg_type == "status":
        with state.lock:
            state.last_status = msg
    else:
        result.notes.append(f"ignored:{msg_type}")

    return result
==================================================

core\power_monitor.py
==================================================
"""High-frequency power monitoring helpers for drone follower."""

from __future__ import annotations

import csv
import math
import os
import random
import re
import shutil
import subprocess
import threading
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, Optional, Protocol

try:  # Best-effort hardware import; unavailable on dev hosts.
    import smbus2 as smbus  # type: ignore
except ModuleNotFoundError:  # pragma: no cover - exercised on non-Pi hosts
    try:
        import smbus2 as smbus  # type: ignore
    except ModuleNotFoundError:  # pragma: no cover - exercised on hosts without I2C libs
        smbus = None  # type: ignore[assignment]

try:
    import psutil  # type: ignore
except ModuleNotFoundError:  # pragma: no cover - psutil optional on host
    psutil = None  # type: ignore[assignment]


_DEFAULT_SAMPLE_HZ = int(os.getenv("INA219_SAMPLE_HZ", "1000"))
_DEFAULT_SHUNT_OHM = float(os.getenv("INA219_SHUNT_OHM", "0.1"))
_DEFAULT_I2C_BUS = int(os.getenv("INA219_I2C_BUS", "1"))
_DEFAULT_ADDR = int(os.getenv("INA219_ADDR", "0x40"), 16)
_DEFAULT_SIGN_MODE = os.getenv("INA219_SIGN_MODE", "auto").lower()

_RPI5_HWMON_PATH_ENV = "RPI5_HWMON_PATH"
_RPI5_HWMON_NAME_ENV = "RPI5_HWMON_NAME"
_RPI5_VOLTAGE_FILE_ENV = "RPI5_VOLTAGE_FILE"
_RPI5_CURRENT_FILE_ENV = "RPI5_CURRENT_FILE"
_RPI5_POWER_FILE_ENV = "RPI5_POWER_FILE"
_RPI5_VOLTAGE_SCALE_ENV = "RPI5_VOLTAGE_SCALE"
_RPI5_CURRENT_SCALE_ENV = "RPI5_CURRENT_SCALE"
_RPI5_POWER_SCALE_ENV = "RPI5_POWER_SCALE"

_RPI5_VOLTAGE_CANDIDATES = (
    "in0_input",
    "in1_input",
    "voltage0_input",
    "voltage1_input",
    "voltage_input",
    "vbus_input",
)

_RPI5_CURRENT_CANDIDATES = (
    "curr0_input",
    "curr1_input",
    "current0_input",
    "current1_input",
    "current_input",
    "ibus_input",
)

_RPI5_POWER_CANDIDATES = (
    "power0_input",
    "power1_input",
    "power_input",
)


# Registers and config masks from INA219 datasheet.
_CFG_BUS_RANGE_32V = 0x2000
_CFG_GAIN_8_320MV = 0x1800
_CFG_MODE_SANDBUS_CONT = 0x0007

_ADC_PROFILES = {
    "highspeed": {"badc": 0x0080, "sadc": 0x0000, "settle": 0.0004, "hz": 1100},
    "balanced": {"badc": 0x0400, "sadc": 0x0018, "settle": 0.0010, "hz": 900},
    "precision": {"badc": 0x0400, "sadc": 0x0048, "settle": 0.0020, "hz": 450},
}


@dataclass
class PowerSummary:
    """Aggregate statistics for a capture window."""

    label: str
    duration_s: float
    samples: int
    avg_current_a: float
    avg_voltage_v: float
    avg_power_w: float
    energy_j: float
    sample_rate_hz: float
    csv_path: str
    start_ns: int
    end_ns: int


@dataclass
class PowerSample:
    """Single instantaneous power sample."""

    timestamp_ns: int
    current_a: float
    voltage_v: float
    power_w: float


class PowerMonitorUnavailable(RuntimeError):
    """Raised when a power monitor backend cannot be initialised."""


class PowerMonitor(Protocol):
    sample_hz: int

    @property
    def sign_factor(self) -> int:  # pragma: no cover - protocol definition only
        ...

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:  # pragma: no cover - protocol definition only
        ...

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:  # pragma: no cover - protocol definition only
        ...


def _pick_profile(sample_hz: float) -> tuple[str, dict]:
    profile_key = os.getenv("INA219_ADC_PROFILE", "auto").lower()
    if profile_key == "auto":
        if sample_hz >= 900:
            profile_key = "highspeed"
        elif sample_hz >= 500:
            profile_key = "balanced"
        else:
            profile_key = "precision"
    return profile_key if profile_key in _ADC_PROFILES else "balanced", _ADC_PROFILES.get(profile_key, _ADC_PROFILES["balanced"])


def _sanitize_label(label: str) -> str:
    return "".join(ch if ch.isalnum() or ch in {"-", "_"} else "_" for ch in label)[:64] or "capture"


class Ina219PowerMonitor:
    """Wraps basic INA219 sampling with CSV logging and summary stats."""

    def __init__(
        self,
        output_dir: Path,
        *,
        i2c_bus: int = _DEFAULT_I2C_BUS,
        address: int = _DEFAULT_ADDR,
        shunt_ohm: float = _DEFAULT_SHUNT_OHM,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        sign_mode: str = _DEFAULT_SIGN_MODE,
    ) -> None:
        if smbus is None:
            raise PowerMonitorUnavailable("smbus module not available on host")
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.address = address
        self.shunt_ohm = shunt_ohm
        self.sample_hz = sample_hz
        self._bus = None
        self._bus_lock = threading.Lock()
        self._sign_factor = 1
        self._sign_mode = sign_mode

        try:
            self._bus = smbus.SMBus(i2c_bus)
        except Exception as exc:  # pragma: no cover - requires hardware
            raise PowerMonitorUnavailable(f"failed to open I2C bus {i2c_bus}: {exc}") from exc

        try:
            self._configure(sample_hz)
            self._sign_factor = self._resolve_sign()
        except Exception as exc:  # pragma: no cover - requires hardware
            raise PowerMonitorUnavailable(f"INA219 init failed: {exc}") from exc

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")
        if self._bus is None:
            raise PowerMonitorUnavailable("power monitor not initialised")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while True:
                elapsed = time.perf_counter() - start_perf
                if elapsed >= duration_s:
                    break
                try:
                    current_a, voltage_v = self._read_current_voltage()
                except Exception as exc:  # pragma: no cover - hardware failure path
                    raise PowerMonitorUnavailable(f"INA219 read failed: {exc}") from exc

                power_w = current_a * voltage_v
                writer.writerow([time.time_ns(), f"{current_a:.6f}", f"{voltage_v:.6f}", f"{power_w:.6f}", self._sign_factor])
                if samples % 250 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = avg_power * elapsed_s
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        if self._bus is None:
            raise PowerMonitorUnavailable("power monitor not initialised")
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            timestamp_ns = time.time_ns()
            current_a, voltage_v = self._read_current_voltage()
            power_w = current_a * voltage_v
            yield PowerSample(
                timestamp_ns=timestamp_ns,
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    def _configure(self, sample_hz: float) -> None:
        profile_key, profile = _pick_profile(sample_hz)
        cfg = (
            _CFG_BUS_RANGE_32V
            | _CFG_GAIN_8_320MV
            | profile["badc"]
            | profile["sadc"]
            | _CFG_MODE_SANDBUS_CONT
        )
        payload = [(cfg >> 8) & 0xFF, cfg & 0xFF]
        with self._bus_lock:
            self._bus.write_i2c_block_data(self.address, 0x00, payload)  # type: ignore[union-attr]
        time.sleep(profile["settle"])

    def _resolve_sign(self) -> int:
        mode = self._sign_mode
        if mode.startswith("pos"):
            return 1
        if mode.startswith("neg"):
            return -1
        probe_deadline = time.time() + float(os.getenv("INA219_SIGN_PROBE_SEC", "2"))
        readings = []
        while time.time() < probe_deadline:
            vsh = self._read_shunt_voltage()
            readings.append(vsh)
            time.sleep(0.005)
        if not readings:
            return 1
        readings.sort()
        median = readings[len(readings) // 2]
        return -1 if median < -20e-6 else 1

    def _read_current_voltage(self) -> tuple[float, float]:
        vsh = self._read_shunt_voltage()
        current = (vsh / self.shunt_ohm) * self._sign_factor
        voltage = self._read_bus_voltage()
        return current, voltage

    def _read_shunt_voltage(self) -> float:
        raw = self._read_s16(0x01)
        return raw * 10e-6

    def _read_bus_voltage(self) -> float:
        raw = self._read_u16(0x02)
        return ((raw >> 3) & 0x1FFF) * 0.004

    def _read_u16(self, register: int) -> int:
        with self._bus_lock:
            hi, lo = self._bus.read_i2c_block_data(self.address, register, 2)  # type: ignore[union-attr]
        return (hi << 8) | lo

    def _read_s16(self, register: int) -> int:
        val = self._read_u16(register)
        if val & 0x8000:
            val -= 1 << 16
        return val


class Rpi5PowerMonitor:
    """Power monitor backend using Raspberry Pi 5 onboard telemetry via hwmon."""

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        sign_mode: str = _DEFAULT_SIGN_MODE,
        hwmon_path: Optional[str] = None,
        hwmon_name_hint: Optional[str] = None,
        voltage_file: Optional[str] = None,
        current_file: Optional[str] = None,
        power_file: Optional[str] = None,
        voltage_scale: Optional[float] = None,
        current_scale: Optional[float] = None,
        power_scale: Optional[float] = None,
    ) -> None:
        del sign_mode  # Pi 5 telemetry reports already-correct sign
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self._sign_factor = 1
        self._hwmon_dir = self._find_hwmon_dir(hwmon_path, hwmon_name_hint, strict=True)
        self._voltage_path, self._current_path, self._power_path = self._resolve_channels(
            voltage_file,
            current_file,
            power_file,
        )
        self._voltage_scale = self._resolve_scale(voltage_scale, _RPI5_VOLTAGE_SCALE_ENV, 1e-6)
        self._current_scale = self._resolve_scale(current_scale, _RPI5_CURRENT_SCALE_ENV, 1e-6)
        self._power_scale = self._resolve_scale(power_scale, _RPI5_POWER_SCALE_ENV, 1e-6)

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while True:
                elapsed = time.perf_counter() - start_perf
                if elapsed >= duration_s:
                    break
                current_a, voltage_v, power_w = self._read_measurements()
                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}",
                    f"{voltage_v:.6f}",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 250 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = avg_power * elapsed_s
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            timestamp_ns = time.time_ns()
            current_a, voltage_v, power_w = self._read_measurements()
            yield PowerSample(
                timestamp_ns=timestamp_ns,
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    @staticmethod
    def is_supported(
        hwmon_path: Optional[str] = None,
        hwmon_name_hint: Optional[str] = None,
    ) -> bool:
        try:
            return Rpi5PowerMonitor._find_hwmon_dir(hwmon_path, hwmon_name_hint, strict=False) is not None
        except PowerMonitorUnavailable:
            return False

    @staticmethod
    def _find_hwmon_dir(
        hwmon_path: Optional[str],
        hwmon_name_hint: Optional[str],
        *,
        strict: bool,
    ) -> Optional[Path]:
        candidates = []
        if hwmon_path:
            candidates.append(hwmon_path)
        env_path = os.getenv(_RPI5_HWMON_PATH_ENV)
        if env_path:
            candidates.append(env_path)

        for candidate in candidates:
            path = Path(candidate).expanduser()
            if path.is_dir():
                return path
            if strict:
                raise PowerMonitorUnavailable(f"hwmon path not found: {path}")

        hwmon_root = Path("/sys/class/hwmon")
        if not hwmon_root.exists():
            if strict:
                raise PowerMonitorUnavailable("/sys/class/hwmon not present on host")
            return None

        hint_source = hwmon_name_hint or os.getenv(_RPI5_HWMON_NAME_ENV) or ""
        hints = [part.strip().lower() for part in hint_source.split(",") if part.strip()]

        for entry in sorted(hwmon_root.iterdir()):
            name_file = entry / "name"
            try:
                name_value = name_file.read_text().strip().lower()
            except Exception:
                continue
            if not name_value:
                continue
            if hints:
                if any(hint in name_value for hint in hints):
                    return entry
            else:
                if "rpi" in name_value and (
                    "power" in name_value
                    or "pmic" in name_value
                    or "monitor" in name_value
                    or "volt" in name_value
                ):
                    return entry

        if strict:
            raise PowerMonitorUnavailable("unable to locate Raspberry Pi power hwmon device")
        return None

    def _resolve_channels(
        self,
        voltage_file: Optional[str],
        current_file: Optional[str],
        power_file: Optional[str],
    ) -> tuple[Path, Path, Optional[Path]]:
        search_dirs = [self._hwmon_dir]
        device_dir = self._hwmon_dir / "device"
        if device_dir.is_dir():
            search_dirs.append(device_dir)

        def pick(
            defaults: tuple[str, ...],
            override: Optional[str],
            env_var: str,
            *,
            required: bool,
        ) -> Optional[Path]:
            # Prefer explicit override paths first.
            if override:
                override_path = Path(override)
                if override_path.is_absolute() or override_path.exists():
                    if override_path.exists():
                        return override_path
                    if required:
                        raise PowerMonitorUnavailable(f"override channel path not found: {override_path}")
                else:
                    for base in search_dirs:
                        candidate = base / override
                        if candidate.exists():
                            return candidate
                    if required:
                        raise PowerMonitorUnavailable(f"override channel name not found: {override}")

            env_override = os.getenv(env_var)
            if env_override:
                for token in env_override.split(","):
                    name = token.strip()
                    if not name:
                        continue
                    env_path = Path(name)
                    if env_path.is_absolute() or env_path.exists():
                        if env_path.exists():
                            return env_path
                        continue
                    for base in search_dirs:
                        candidate = base / name
                        if candidate.exists():
                            return candidate

            for name in defaults:
                for base in search_dirs:
                    candidate = base / name
                    if candidate.exists():
                        return candidate

            if required:
                raise PowerMonitorUnavailable(f"missing required hwmon channel {defaults[0] if defaults else 'unknown'}")
            return None

        voltage_path = pick(_RPI5_VOLTAGE_CANDIDATES, voltage_file, _RPI5_VOLTAGE_FILE_ENV, required=True)
        current_path = pick(_RPI5_CURRENT_CANDIDATES, current_file, _RPI5_CURRENT_FILE_ENV, required=True)
        power_path = pick(_RPI5_POWER_CANDIDATES, power_file, _RPI5_POWER_FILE_ENV, required=False)
        if voltage_path is None or current_path is None:
            raise PowerMonitorUnavailable("incomplete hwmon channel mapping")
        return voltage_path, current_path, power_path

    def _read_measurements(self) -> tuple[float, float, float]:
        voltage_v = self._read_channel(self._voltage_path, self._voltage_scale)
        current_a = self._read_channel(self._current_path, self._current_scale)
        if self._power_path is not None:
            power_w = self._read_channel(self._power_path, self._power_scale)
        else:
            power_w = voltage_v * current_a
        return current_a, voltage_v, power_w

    def _read_channel(self, path: Path, scale: float) -> float:
        try:
            raw = path.read_text().strip()
        except FileNotFoundError as exc:
            raise PowerMonitorUnavailable(f"hwmon channel missing: {path}") from exc
        except PermissionError as exc:  # pragma: no cover - depends on host permissions
            raise PowerMonitorUnavailable(f"insufficient permissions for {path}") from exc
        if not raw:
            raise PowerMonitorUnavailable(f"empty hwmon reading from {path}")
        try:
            value = float(raw)
        except ValueError as exc:
            raise PowerMonitorUnavailable(f"invalid hwmon reading from {path}: {raw!r}") from exc
        return value * scale

    def _resolve_scale(self, explicit: Optional[float], env_name: str, default: float) -> float:
        if explicit is not None:
            return explicit
        raw = os.getenv(env_name)
        if raw is None or raw == "":
            return default
        try:
            return float(raw)
        except ValueError as exc:
            raise PowerMonitorUnavailable(f"invalid {env_name} value: {raw!r}") from exc


class Rpi5PmicPowerMonitor:
    """Power monitor backend using Raspberry Pi 5 PMIC telemetry via `vcgencmd`."""

    _RAIL_PATTERN = re.compile(
        r"^\s*(?P<name>[A-Z0-9_]+)\s+(?P<kind>current|volt)\(\d+\)=(?P<value>[0-9.]+)(?P<unit>A|V)\s*$"
    )

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = 10,
        sign_mode: str = "auto",
    ) -> None:
        del sign_mode  # PMIC telemetry is unsigned
        if sample_hz <= 0 or sample_hz > 20:
            raise PowerMonitorUnavailable("rpi5-pmic sample_hz must be between 1 and 20")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self._sign_factor = 1

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")
        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        start_wall_ns = time.time_ns()
        start_perf = time.perf_counter()

        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0
        samples = 0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            while (time.perf_counter() - start_perf) < duration_s:
                rails = self._read_once()
                voltage_v = self._choose_voltage(rails)
                power_w = self._sum_power(rails)
                current_a = self._derive_current(power_w, voltage_v)

                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}" if not math.isnan(current_a) else "nan",
                    f"{voltage_v:.6f}" if not math.isnan(voltage_v) else "nan",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 10 == 0:
                    handle.flush()

                if not math.isnan(current_a):
                    sum_current += current_a
                if not math.isnan(voltage_v):
                    sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick = start_perf + samples * dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed = max(end_perf - start_perf, 1e-9)
        avg_current = (sum_current / samples) if samples else 0.0
        avg_voltage = (sum_voltage / samples) if samples else 0.0
        avg_power = (sum_power / samples) if samples else 0.0
        energy_j = avg_power * elapsed
        sample_rate = samples / elapsed if elapsed > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        start_perf = time.perf_counter()
        samples = 0
        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break
            rails = self._read_once()
            voltage_v = self._choose_voltage(rails)
            power_w = self._sum_power(rails)
            current_a = self._derive_current(power_w, voltage_v)

            yield PowerSample(
                timestamp_ns=time.time_ns(),
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )

            samples += 1
            next_tick = start_perf + samples * dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

    def _read_once(self) -> dict[str, dict[str, Optional[float]]]:
        try:
            output = subprocess.check_output(["vcgencmd", "pmic_read_adc"], text=True, timeout=1.0)
        except FileNotFoundError as exc:
            raise PowerMonitorUnavailable("vcgencmd not found; install raspberrypi-userland") from exc
        except subprocess.SubprocessError as exc:
            raise PowerMonitorUnavailable(f"vcgencmd pmic_read_adc failed: {exc}") from exc

        rails: dict[str, dict[str, Optional[float]]] = {}
        for line in output.splitlines():
            match = self._RAIL_PATTERN.match(line)
            if not match:
                continue
            name = match.group("name")
            kind = match.group("kind")
            value = float(match.group("value"))
            rail = rails.setdefault(name, {"current_a": None, "voltage_v": None})
            if kind == "current":
                rail["current_a"] = value
            else:
                rail["voltage_v"] = value
        if not rails:
            raise PowerMonitorUnavailable("pmic_read_adc returned no rail telemetry")
        return rails

    def _sum_power(self, rails: dict[str, dict[str, Optional[float]]]) -> float:
        total = 0.0
        for rail in rails.values():
            current_a = rail.get("current_a")
            voltage_v = rail.get("voltage_v")
            if current_a is None or voltage_v is None:
                continue
            total += current_a * voltage_v
        return total

    def _choose_voltage(self, rails: dict[str, dict[str, Optional[float]]]) -> float:
        ext5 = rails.get("EXT5V_V", {}).get("voltage_v") if "EXT5V_V" in rails else None
        if ext5 is not None and ext5 > 0:
            return ext5
        return max((rail.get("voltage_v") or float("nan") for rail in rails.values()), default=float("nan"))

    def _derive_current(self, power_w: float, voltage_v: float) -> float:
        if math.isnan(voltage_v) or voltage_v <= 0:
            return float("nan")
        return power_w / voltage_v


class SyntheticPowerMonitor:
    """Synthetic fallback monitor that approximates power via host telemetry."""

    def __init__(
        self,
        output_dir: Path,
        *,
        sample_hz: int = _DEFAULT_SAMPLE_HZ,
        base_power_w: float = 18.0,
        dynamic_power_w: float = 12.0,
        voltage_v: float = 11.1,
        noise_w: float = 1.5,
    ) -> None:
        if psutil is None:
            raise PowerMonitorUnavailable("psutil module not available for synthetic backend")
        if sample_hz <= 0:
            raise PowerMonitorUnavailable("sample_hz must be > 0")

        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.sample_hz = sample_hz
        self.base_power_w = max(0.0, float(base_power_w))
        self.dynamic_power_w = max(0.0, float(dynamic_power_w))
        self.noise_w = max(0.0, float(noise_w))
        self.voltage_v = max(1e-3, float(voltage_v))
        self._sign_factor = 1
        self.backend_name = "synthetic"

    @staticmethod
    def is_supported() -> bool:
        return psutil is not None

    @property
    def sign_factor(self) -> int:
        return self._sign_factor

    def _compute_power(self, cpu_percent: float, net_bytes_per_s: float) -> float:
        cpu_term = (cpu_percent / 100.0) * self.dynamic_power_w
        net_term = min(self.dynamic_power_w * 0.5, (net_bytes_per_s / 1_000_000.0) * 4.0)
        jitter = random.uniform(-self.noise_w, self.noise_w)
        return max(0.0, self.base_power_w + cpu_term + net_term + jitter)

    def capture(
        self,
        *,
        label: str,
        duration_s: float,
        start_ns: Optional[int] = None,
    ) -> PowerSummary:
        if duration_s <= 0:
            raise ValueError("duration_s must be positive")

        if start_ns is not None:
            delay_ns = start_ns - time.time_ns()
            if delay_ns > 0:
                time.sleep(delay_ns / 1_000_000_000)

        safe_label = _sanitize_label(label)
        ts = time.strftime("%Y%m%d-%H%M%S", time.gmtime())
        csv_path = self.output_dir / f"power_{safe_label}_{ts}.csv"

        dt = 1.0 / float(self.sample_hz)
        refresh_cpu_every = max(1, int(self.sample_hz * 0.05))  # ~20 Hz refresh
        refresh_net_every = max(refresh_cpu_every * 2, int(self.sample_hz * 0.1))
        next_tick = time.perf_counter()
        start_perf = time.perf_counter()
        start_wall_ns = time.time_ns()

        samples = 0
        sum_current = 0.0
        sum_voltage = 0.0
        sum_power = 0.0

        cpu_percent = psutil.cpu_percent(interval=None)
        net = psutil.net_io_counters() if hasattr(psutil, "net_io_counters") else None
        last_net_total = (net.bytes_sent + net.bytes_recv) if net else 0
        last_net_ts = time.perf_counter()
        net_bytes_per_s = 0.0

        with open(csv_path, "w", newline="", encoding="utf-8") as handle:
            writer = csv.writer(handle)
            writer.writerow(["timestamp_ns", "current_a", "voltage_v", "power_w", "sign_factor"])

            target_samples = int(round(duration_s * self.sample_hz))
            while samples < target_samples:
                if samples % refresh_cpu_every == 0:
                    cpu_percent = psutil.cpu_percent(interval=None)

                if net and samples % refresh_net_every == 0:
                    now = time.perf_counter()
                    elapsed = max(now - last_net_ts, 1e-6)
                    net_curr = psutil.net_io_counters()
                    total = net_curr.bytes_sent + net_curr.bytes_recv
                    delta = max(0, total - last_net_total)
                    net_bytes_per_s = delta / elapsed
                    last_net_total = total
                    last_net_ts = now

                power_w = self._compute_power(cpu_percent, net_bytes_per_s)
                voltage_v = self.voltage_v
                current_a = power_w / voltage_v

                writer.writerow([
                    time.time_ns(),
                    f"{current_a:.6f}",
                    f"{voltage_v:.6f}",
                    f"{power_w:.6f}",
                    self._sign_factor,
                ])
                if samples % 500 == 0:
                    handle.flush()

                sum_current += current_a
                sum_voltage += voltage_v
                sum_power += power_w
                samples += 1

                next_tick += dt
                sleep_for = next_tick - time.perf_counter()
                if sleep_for > 0:
                    time.sleep(sleep_for)

        end_perf = time.perf_counter()
        end_wall_ns = time.time_ns()
        elapsed_s = max(end_perf - start_perf, 1e-9)
        avg_current = sum_current / samples if samples else 0.0
        avg_voltage = sum_voltage / samples if samples else 0.0
        avg_power = sum_power / samples if samples else 0.0
        energy_j = sum_power * dt
        sample_rate = samples / elapsed_s if elapsed_s > 0 else 0.0

        return PowerSummary(
            label=safe_label,
            duration_s=elapsed_s,
            samples=samples,
            avg_current_a=avg_current,
            avg_voltage_v=avg_voltage,
            avg_power_w=avg_power,
            energy_j=energy_j,
            sample_rate_hz=sample_rate,
            csv_path=str(csv_path.resolve()),
            start_ns=start_wall_ns,
            end_ns=end_wall_ns,
        )

    def iter_samples(self, duration_s: Optional[float] = None) -> Iterator[PowerSample]:
        limit = None if duration_s is None or duration_s <= 0 else duration_s
        dt = 1.0 / float(self.sample_hz)
        refresh_cpu_every = max(1, int(self.sample_hz * 0.05))
        refresh_net_every = max(refresh_cpu_every * 2, int(self.sample_hz * 0.1))
        start_perf = time.perf_counter()
        next_tick = time.perf_counter()
        samples = 0

        cpu_percent = psutil.cpu_percent(interval=None) if psutil else 0.0
        net = psutil.net_io_counters() if hasattr(psutil, "net_io_counters") else None
        last_net_total = (net.bytes_sent + net.bytes_recv) if net else 0
        last_net_ts = time.perf_counter()
        net_bytes_per_s = 0.0

        while True:
            if limit is not None and (time.perf_counter() - start_perf) >= limit:
                break

            if psutil and samples % refresh_cpu_every == 0:
                cpu_percent = psutil.cpu_percent(interval=None)

            if psutil and net and samples % refresh_net_every == 0:
                now = time.perf_counter()
                elapsed = max(now - last_net_ts, 1e-6)
                net_curr = psutil.net_io_counters()
                total = net_curr.bytes_sent + net_curr.bytes_recv
                delta = max(0, total - last_net_total)
                net_bytes_per_s = delta / elapsed
                last_net_total = total
                last_net_ts = now

            power_w = self._compute_power(cpu_percent, net_bytes_per_s)
            voltage_v = self.voltage_v
            current_a = power_w / voltage_v
            yield PowerSample(
                timestamp_ns=time.time_ns(),
                current_a=current_a,
                voltage_v=voltage_v,
                power_w=power_w,
            )

            samples += 1
            next_tick += dt
            sleep_for = next_tick - time.perf_counter()
            if sleep_for > 0:
                time.sleep(sleep_for)

def create_power_monitor(
    output_dir: Path,
    *,
    backend: str = "auto",
    sample_hz: Optional[int] = None,
    sign_mode: Optional[str] = None,
    shunt_ohm: Optional[float] = None,
    i2c_bus: Optional[int] = None,
    address: Optional[int] = None,
    hwmon_path: Optional[str] = None,
    hwmon_name_hint: Optional[str] = None,
    voltage_file: Optional[str] = None,
    current_file: Optional[str] = None,
    power_file: Optional[str] = None,
    voltage_scale: Optional[float] = None,
    current_scale: Optional[float] = None,
    power_scale: Optional[float] = None,
) -> PowerMonitor:
    resolved_backend = (backend or "auto").lower()
    env_backend = os.getenv("POWER_MONITOR_BACKEND")
    if resolved_backend == "auto" and env_backend:
        resolved_backend = env_backend.lower()

    resolved_sample_hz = int(sample_hz if sample_hz is not None else _DEFAULT_SAMPLE_HZ)
    resolved_sign_mode = (sign_mode or _DEFAULT_SIGN_MODE).lower()
    resolved_shunt = float(shunt_ohm if shunt_ohm is not None else _DEFAULT_SHUNT_OHM)
    resolved_i2c_bus = int(i2c_bus if i2c_bus is not None else _DEFAULT_I2C_BUS)
    resolved_address = address if address is not None else _DEFAULT_ADDR
    if isinstance(resolved_address, str):
        resolved_address = int(resolved_address, 0)

    ina_kwargs = {
        "i2c_bus": resolved_i2c_bus,
        "address": resolved_address,
        "shunt_ohm": resolved_shunt,
        "sample_hz": resolved_sample_hz,
        "sign_mode": resolved_sign_mode,
    }
    rpi_kwargs = {
        "sample_hz": resolved_sample_hz,
        "sign_mode": resolved_sign_mode,
        "hwmon_path": hwmon_path,
        "hwmon_name_hint": hwmon_name_hint,
        "voltage_file": voltage_file,
        "current_file": current_file,
        "power_file": power_file,
        "voltage_scale": voltage_scale,
        "current_scale": current_scale,
        "power_scale": power_scale,
    }

    if resolved_backend == "ina219":
        return Ina219PowerMonitor(output_dir, **ina_kwargs)
    if resolved_backend == "rpi5":
        return Rpi5PowerMonitor(output_dir, **rpi_kwargs)
    if resolved_backend == "rpi5-pmic":
        return Rpi5PmicPowerMonitor(output_dir, sample_hz=resolved_sample_hz, sign_mode=resolved_sign_mode)
    if resolved_backend == "synthetic":
        return SyntheticPowerMonitor(output_dir, sample_hz=resolved_sample_hz)
    if resolved_backend != "auto":
        raise ValueError(f"unknown power monitor backend: {backend}")

    rpi_error: Optional[PowerMonitorUnavailable] = None
    pmic_error: Optional[PowerMonitorUnavailable] = None
    synthetic_error: Optional[PowerMonitorUnavailable] = None
    if Rpi5PowerMonitor.is_supported(hwmon_path=hwmon_path, hwmon_name_hint=hwmon_name_hint):
        try:
            return Rpi5PowerMonitor(output_dir, **rpi_kwargs)
        except PowerMonitorUnavailable as exc:
            rpi_error = exc

    if shutil.which("vcgencmd"):
        try:
            return Rpi5PmicPowerMonitor(output_dir, sample_hz=resolved_sample_hz, sign_mode=resolved_sign_mode)
        except PowerMonitorUnavailable as exc:
            pmic_error = exc

    try:
        return Ina219PowerMonitor(output_dir, **ina_kwargs)
    except PowerMonitorUnavailable as exc:
        ina_error = exc
        if SyntheticPowerMonitor.is_supported():
            try:
                return SyntheticPowerMonitor(output_dir, sample_hz=resolved_sample_hz)
            except PowerMonitorUnavailable as syn_exc:
                synthetic_error = syn_exc
        if pmic_error is not None:
            raise pmic_error
        if rpi_error is not None:
            raise rpi_error
        if synthetic_error is not None:
            raise synthetic_error
        raise ina_error


__all__ = [
    "Ina219PowerMonitor",
    "Rpi5PowerMonitor",
    "Rpi5PmicPowerMonitor",
    "SyntheticPowerMonitor",
    "PowerMonitor",
    "PowerSummary",
    "PowerSample",
    "PowerMonitorUnavailable",
    "create_power_monitor",
]

==================================================

core\process.py
==================================================
"""
Unified Process Lifecycle Management.

Provides a robust `ManagedProcess` class that guarantees:
1. Parent ownership (child dies if parent dies).
2. Clean termination (SIGTERM -> SIGKILL).
3. Group management (no orphans).
4. Cross-platform consistency (Windows Job Objects / Linux PDEATHSIG).
"""

import sys
import os
import time
import signal
import subprocess
import threading
import logging
import atexit
from typing import Optional, List, Union, IO, Any

logger = logging.getLogger("pqc.process")

# --- Platform Specifics ---

_use_job_objects = False
_libc = None

if sys.platform.startswith("win"):
    import ctypes
    from ctypes import wintypes
    
    _kernel32 = ctypes.windll.kernel32
    
    # Job Object Constants
    _JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE = 0x00002000
    _JobObjectBasicLimitInformation = 2
    
    class _JOBOBJECT_BASIC_LIMIT_INFORMATION(ctypes.Structure):
        _fields_ = [
            ("PerProcessUserTimeLimit", wintypes.LARGE_INTEGER),
            ("PerJobUserTimeLimit", wintypes.LARGE_INTEGER),
            ("LimitFlags", wintypes.DWORD),
            ("MinimumWorkingSetSize", ctypes.c_size_t),
            ("MaximumWorkingSetSize", ctypes.c_size_t),
            ("ActiveProcessLimit", wintypes.DWORD),
            ("Affinity", ctypes.c_size_t),
            ("PriorityClass", wintypes.DWORD),
            ("SchedulingClass", wintypes.DWORD),
        ]

    class _JOBOBJECT_EXTENDED_LIMIT_INFORMATION(ctypes.Structure):
        _fields_ = [
            ("BasicLimitInformation", _JOBOBJECT_BASIC_LIMIT_INFORMATION),
            ("IoInfo", ctypes.c_void_p), # IO_COUNTERS
            ("ProcessMemoryLimit", ctypes.c_size_t),
            ("JobMemoryLimit", ctypes.c_size_t),
            ("PeakProcessMemoryUsed", ctypes.c_size_t),
            ("PeakJobMemoryUsed", ctypes.c_size_t),
        ]

    def _create_job_object():
        """Create a Windows Job Object that kills processes on close."""
        job = _kernel32.CreateJobObjectW(None, None)
        if not job:
            return None
        
        info = _JOBOBJECT_EXTENDED_LIMIT_INFORMATION()
        info.BasicLimitInformation.LimitFlags = _JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE
        
        ret = _kernel32.SetInformationJobObject(
            job,
            _JobObjectBasicLimitInformation,
            ctypes.byref(info),
            ctypes.sizeof(info)
        )
        if not ret:
            _kernel32.CloseHandle(job)
            return None
        return job

    def _assign_process_to_job(job, pid):
        """Assign a process to the job object."""
        # We need a handle to the process. subprocess.Popen gives us a handle but 
        # accessing it via ctypes requires the raw handle.
        # Popen.dethandle is not public API.
        # Instead, we open the process by PID.
        PROCESS_SET_QUOTA = 0x0100
        PROCESS_TERMINATE = 0x0001
        h_process = _kernel32.OpenProcess(PROCESS_SET_QUOTA | PROCESS_TERMINATE, False, pid)
        if not h_process:
            return False
        
        ret = _kernel32.AssignProcessToJobObject(job, h_process)
        _kernel32.CloseHandle(h_process)
        return bool(ret)

    _use_job_objects = True

else:
    # Linux / POSIX
    try:
        _libc = ctypes.CDLL("libc.so.6")
    except Exception:
        _libc = None

    def _linux_preexec():
        """Set PR_SET_PDEATHSIG to SIGTERM."""
        if _libc:
            PR_SET_PDEATHSIG = 1
            _libc.prctl(PR_SET_PDEATHSIG, signal.SIGTERM)
        # Also set session ID to allow group killing if needed
        os.setsid()

# --- Global Registry ---

_REGISTRY = set()
_REGISTRY_LOCK = threading.Lock()

def _register(proc):
    with _REGISTRY_LOCK:
        _REGISTRY.add(proc)

def _unregister(proc):
    with _REGISTRY_LOCK:
        _REGISTRY.discard(proc)

def kill_all_managed_processes():
    """Kill all registered processes. Safe to call multiple times."""
    with _REGISTRY_LOCK:
        procs = list(_REGISTRY)
    
    if not procs:
        return

    logger.info(f"Cleaning up {len(procs)} managed processes...")
    for p in procs:
        try:
            p.stop(timeout=1.0)
        except Exception as e:
            logger.error(f"Error stopping process {p}: {e}")

atexit.register(kill_all_managed_processes)


# --- ManagedProcess Class ---

class ManagedProcess:
    def __init__(self, cmd: List[str], 
                 name: str = "process",
                 cwd: Optional[str] = None,
                 env: Optional[dict] = None,
                 stdout: Union[int, IO, None] = subprocess.DEVNULL,
                 stderr: Union[int, IO, None] = subprocess.STDOUT,
                 stdin: Union[int, IO, None] = subprocess.DEVNULL,
                 new_console: bool = False):
        self.cmd = cmd
        self.name = name
        self.cwd = cwd
        self.env = env
        self.stdout = stdout
        self.stderr = stderr
        self.stdin = stdin
        self.new_console = new_console
        
        self.process: Optional[subprocess.Popen] = None
        self._job_handle = None # Windows only
        
    def start(self) -> bool:
        if self.is_running():
            return True
            
        try:
            kwargs = {
                "cwd": self.cwd,
                "env": self.env,
                "stdout": self.stdout,
                "stderr": self.stderr,
                "stdin": self.stdin,
                "text": True,
            }

            if sys.platform.startswith("win"):
                # Windows Strategy:
                # 1. CREATE_NEW_PROCESS_GROUP for Ctrl+Break signaling
                # 2. Job Object for hard-kill on parent death
                
                creationflags = subprocess.CREATE_NEW_PROCESS_GROUP
                if self.new_console:
                    creationflags |= subprocess.CREATE_NEW_CONSOLE
                    # If new console, and no redirection, detach handles so child uses new console
                    if self.stdout is None and self.stderr is None and self.stdin is None:
                        kwargs["close_fds"] = True
                
                kwargs["creationflags"] = creationflags
                
                # Create Job Object *before* spawning if possible, but we need PID.
                # Actually, we create Job, spawn suspended? No, Python doesn't support that easily.
                # We spawn, then assign. There is a race condition where parent dies before assignment.
                # But it's small.
                
                self.process = subprocess.Popen(self.cmd, **kwargs)
                
                if _use_job_objects:
                    self._job_handle = _create_job_object()
                    if self._job_handle:
                        if not _assign_process_to_job(self._job_handle, self.process.pid):
                            logger.warning(f"Failed to assign {self.name} to Job Object")
            else:
                # Linux Strategy:
                # 1. preexec_fn with prctl(PDEATHSIG) and setsid
                kwargs["preexec_fn"] = _linux_preexec
                self.process = subprocess.Popen(self.cmd, **kwargs)

            _register(self)
            return True
            
        except Exception as e:
            logger.error(f"Failed to start {self.name}: {e}")
            return False

    def stop(self, timeout: float = 5.0):
        if not self.process:
            return

        _unregister(self) # Prevent double cleanup
        
        try:
            if self.process.poll() is not None:
                self.process = None
                return

            # Polite termination
            if sys.platform.startswith("win"):
                # Windows: Send Ctrl+Break to group if possible, else terminate
                # Since we used CREATE_NEW_PROCESS_GROUP, we can send signal to PID
                # But Python's os.kill on Windows is limited.
                # subprocess.terminate() calls TerminateProcess.
                # We want to kill the tree.
                subprocess.run(f"taskkill /F /T /PID {self.process.pid}", 
                               shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            else:
                # Linux: Kill group
                try:
                    pgid = os.getpgid(self.process.pid)
                    os.killpg(pgid, signal.SIGTERM)
                except ProcessLookupError:
                    pass

            # Wait
            try:
                self.process.wait(timeout=timeout)
            except subprocess.TimeoutExpired:
                # Force kill
                if sys.platform.startswith("win"):
                    # Taskkill /F already did it, but just in case
                    self.process.kill()
                else:
                    try:
                        pgid = os.getpgid(self.process.pid)
                        os.killpg(pgid, signal.SIGKILL)
                    except (ProcessLookupError, OSError):
                        try:
                            self.process.kill()
                        except:
                            pass
        except Exception as e:
            logger.error(f"Error stopping {self.name}: {e}")
        finally:
            # Close Job Handle
            if self._job_handle:
                _kernel32.CloseHandle(self._job_handle)
                self._job_handle = None
            self.process = None

    def is_running(self) -> bool:
        return self.process is not None and self.process.poll() is None

    def wait(self, timeout: Optional[float] = None):
        if self.process:
            return self.process.wait(timeout)
        return None

==================================================

core\run_proxy.py
==================================================
"""
Unified CLI entrypoint for the PQC drone-GCS proxy.

Supports subcommands:
- init-identity: Create persistent GCS signing identity
- gcs: Start GCS proxy (requires secret key by default)  
- drone: Start drone proxy (requires GCS public key)

Uses persistent file-based keys by default for production security.
"""

import sys
import argparse
import signal
import os
import json
import time
import logging
import threading
from pathlib import Path
from typing import Callable, Dict, Optional

from core.config import CONFIG
from core.suites import DEFAULT_SUITE_ID, get_suite, build_suite_id
from core.logging_utils import get_logger, configure_file_logger

logger = get_logger("pqc")


def _format_duration_ns(ns: int) -> str:
    """Return human readable representation for a duration in nanoseconds."""

    if ns < 0:
        ns = 0
    if ns >= 1_000_000_000:
        seconds = ns / 1_000_000_000.0
        return f"{seconds:.3f} s"
    if ns >= 1_000_000:
        millis = ns / 1_000_000.0
        return f"{millis:.3f} ms"
    if ns >= 1_000:
        micros = ns / 1_000.0
        return f"{micros:.3f} µs"
    return f"{ns} ns"


def _ns_to_ms(value: object) -> float:
    try:
        ns = float(value)
    except (TypeError, ValueError):
        return 0.0
    if ns <= 0.0:
        return 0.0
    return round(ns / 1_000_000.0, 6)


def _flatten_part_b_metrics(handshake_metrics: Dict[str, object]) -> Dict[str, object]:
    """Derive flattened Part B primitive metrics from the handshake payload."""

    if not isinstance(handshake_metrics, dict):
        return {}

    primitives = handshake_metrics.get("primitives") or {}
    if not isinstance(primitives, dict):
        primitives = {}

    kem_metrics = primitives.get("kem") if isinstance(primitives.get("kem"), dict) else {}
    sig_metrics = primitives.get("signature") if isinstance(primitives.get("signature"), dict) else {}

    artifacts = handshake_metrics.get("artifacts") if isinstance(handshake_metrics.get("artifacts"), dict) else {}

    flat: Dict[str, object] = {}

    def _copy_float(key: str) -> None:
        value = handshake_metrics.get(key)
        if isinstance(value, (int, float)):
            flat[key] = round(float(value), 6)

    def _copy_int(key: str) -> None:
        value = handshake_metrics.get(key)
        if isinstance(value, int):
            flat[key] = value
        elif isinstance(value, float):
            flat[key] = int(value)

    timing_keys = (
        "kem_keygen_max_ms",
        "kem_keygen_avg_ms",
        "kem_encaps_max_ms",
        "kem_encaps_avg_ms",
        "kem_decaps_max_ms",
        "kem_decaps_avg_ms",
        "sig_sign_max_ms",
        "sig_sign_avg_ms",
        "sig_verify_max_ms",
        "sig_verify_avg_ms",
        "primitive_total_ms",
        "rekey_ms",
    )
    for key in timing_keys:
        _copy_float(key)

    for aead_key in ("aead_encrypt_avg_ms", "aead_decrypt_avg_ms"):
        _copy_float(aead_key)

    size_keys = (
        "pub_key_size_bytes",
        "ciphertext_size_bytes",
        "sig_size_bytes",
        "shared_secret_size_bytes",
    )
    for key in size_keys:
        value = handshake_metrics.get(key)
        if isinstance(value, int):
            flat[key] = value
        elif isinstance(value, float):
            flat[key] = int(value)
        else:
            # fall back to raw artifacts when handshake metrics missing
            if key == "pub_key_size_bytes":
                fallback = kem_metrics.get("public_key_bytes") or artifacts.get("public_key_bytes")
            elif key == "ciphertext_size_bytes":
                fallback = kem_metrics.get("ciphertext_bytes")
            elif key == "sig_size_bytes":
                fallback = sig_metrics.get("signature_bytes") or artifacts.get("signature_bytes")
            else:
                fallback = kem_metrics.get("shared_secret_bytes")
            if isinstance(fallback, int):
                flat[key] = fallback

    energy_keys = (
        "rekey_energy_mJ",
        "handshake_energy_mJ",
        "kem_keygen_mJ",
        "kem_encaps_mJ",
        "kem_decaps_mJ",
        "sig_sign_mJ",
        "sig_verify_mJ",
    )
    for key in energy_keys:
        value = handshake_metrics.get(key)
        if isinstance(value, (int, float)):
            flat[key] = round(float(value), 6)

    window_keys = (
        "handshake_energy_start_ns",
        "handshake_energy_end_ns",
        "rekey_energy_start_ns",
        "rekey_energy_end_ns",
    )
    for key in window_keys:
        value = handshake_metrics.get(key)
        if isinstance(value, int):
            flat[key] = value

    return flat


def _augment_part_b_metrics(counters: Dict[str, object]) -> None:
    """Inject flattened primitive timing/size metrics into counter payload."""

    if not isinstance(counters, dict):  # defensive guard
        return

    handshake_payload = counters.get("handshake_metrics")
    flat_metrics = _flatten_part_b_metrics(handshake_payload) if isinstance(handshake_payload, dict) else {}

    for key, value in flat_metrics.items():
        counters.setdefault(key, value)

    part_b_payload = counters.get("part_b_metrics")
    if isinstance(part_b_payload, dict):
        for key, value in part_b_payload.items():
            counters.setdefault(key, value)


def _pretty_print_counters(counters: Dict[str, object]) -> None:
    """Display counters with special handling for nested metrics."""

    scalar_items = []
    handshake_payload: Optional[Dict[str, object]] = None
    primitive_payload: Optional[Dict[str, Dict[str, object]]] = None

    for key, value in counters.items():
        if key == "handshake_metrics" and isinstance(value, dict):
            handshake_payload = value  # defer printing until after scalars
            continue
        if key == "primitive_metrics" and isinstance(value, dict):
            primitive_payload = value  # defer printing until after scalars
            continue
        scalar_items.append((key, value))

    for key, value in sorted(scalar_items, key=lambda item: item[0]):
        print(f"  {key}: {value}")

    if handshake_payload:
        print("  handshake_metrics:")
        for key, value in sorted(handshake_payload.items(), key=lambda item: item[0]):
            print(f"    {key}: {value}")

    if primitive_payload:
        print("  primitive_metrics:")
        for name, stats in sorted(primitive_payload.items(), key=lambda item: item[0]):
            if not isinstance(stats, dict):
                print(f"    {name}: {stats}")
                continue
            count = int(stats.get("count", 0) or 0)
            total_ns = int(stats.get("total_ns", 0) or 0)
            avg_ns = total_ns // count if count > 0 else 0
            min_ns = int(stats.get("min_ns", 0) or 0)
            max_ns = int(stats.get("max_ns", 0) or 0)
            total_in = int(stats.get("total_in_bytes", 0) or 0)
            total_out = int(stats.get("total_out_bytes", 0) or 0)
            print(
                "    "
                f"{name}: count={count}, avg={_format_duration_ns(avg_ns)}, "
                f"min={_format_duration_ns(min_ns)}, max={_format_duration_ns(max_ns)}, "
                f"in_bytes={total_in}, out_bytes={total_out}"
            )

def _require_signature_class():
    """Lazily import oqs Signature and provide a friendly error if missing."""

    try:
        from oqs.oqs import Signature  # type: ignore
    except ModuleNotFoundError as exc:  # pragma: no cover - exercised via CLI
        if exc.name in {"oqs", "oqs.oqs"}:
            print(
                "Error: oqs-python is required for cryptographic operations. "
                "Install it with 'pip install oqs-python' or activate the project environment."
            )
            sys.exit(1)
        raise

    return Signature


def _require_run_proxy():
    """Import run_proxy only when needed, surfacing helpful guidance on failure."""

    try:
        from core.async_proxy import run_proxy as _run_proxy  # type: ignore
    except ModuleNotFoundError as exc:  # pragma: no cover - exercised via CLI
        if exc.name in {"oqs", "oqs.oqs"}:
            print(
                "Error: oqs-python is required to start the proxy. "
                "Install it with 'pip install oqs-python' or activate the project environment."
            )
            sys.exit(1)
        raise

    return _run_proxy


def _build_matrix_secret_loader(
    *,
    suite_id: Optional[str],
    default_secret_path: Optional[Path],
    initial_secret: Optional[object],
    signature_cls,
    matrix_dir: Optional[Path] = None,
) -> Callable[[Dict[str, object]], object]:
    """Return loader that fetches per-suite signing secrets from disk.

    The loader prefers a suite-specific directory under `secrets/matrix/` and falls
    back to the primary secret path when targeting the initial suite. Results are
    cached per suite and guarded with a lock because rekeys may run in background
    threads.
    """

    lock = threading.Lock()
    cache: Dict[str, object] = {}
    if suite_id and initial_secret is not None and isinstance(initial_secret, signature_cls):
        cache[suite_id] = initial_secret

    matrix_secrets_dir = matrix_dir or Path("secrets/matrix")

    def instantiate(secret_bytes: bytes, sig_name: str):
        errors = []
        sig_obj = None
        try:
            sig_obj = signature_cls(sig_name)
        except Exception as exc:  # pragma: no cover - depends on oqs build
            errors.append(f"Signature ctor failed: {exc}")
            sig_obj = None

        if sig_obj is not None and hasattr(sig_obj, "import_secret_key"):
            try:
                sig_obj.import_secret_key(secret_bytes)
                return sig_obj
            except Exception as exc:
                errors.append(f"import_secret_key failed: {exc}")

        try:
            return signature_cls(sig_name, secret_key=secret_bytes)
        except TypeError as exc:
            errors.append(f"ctor secret_key unsupported: {exc}")
        except Exception as exc:  # pragma: no cover - defensive logging only
            errors.append(f"ctor secret_key failed: {exc}")

        detail = "; ".join(errors) if errors else "unknown error"
        raise RuntimeError(f"Unable to load signature secret: {detail}")

    def load_secret_for_suite(target_suite: Dict[str, object]):
        target_suite_id = target_suite.get("suite_id") if isinstance(target_suite, dict) else None
        if not target_suite_id:
            raise RuntimeError("Suite dictionary missing suite_id")

        with lock:
            cached = cache.get(target_suite_id)
            if cached is not None:
                return cached

        candidates = []
        if default_secret_path and suite_id and target_suite_id == suite_id:
            candidates.append(default_secret_path)
        candidates.append(matrix_secrets_dir / target_suite_id / "gcs_signing.key")

        seen: Dict[str, None] = {}
        for candidate in candidates:
            candidate_path = candidate.expanduser()
            key = str(candidate_path.resolve()) if candidate_path.exists() else str(candidate_path)
            if key in seen:
                continue
            seen[key] = None
            if not candidate_path.exists():
                continue
            try:
                secret_bytes = candidate_path.read_bytes()
            except Exception as exc:
                raise RuntimeError(f"Failed to read GCS secret key {candidate_path}: {exc}") from exc
            try:
                sig_obj = instantiate(secret_bytes, target_suite["sig_name"])  # type: ignore[index]
            except Exception as exc:
                raise RuntimeError(
                    f"Failed to load GCS secret key {candidate_path} for suite {target_suite_id}: {exc}"
                ) from exc
            with lock:
                cache[target_suite_id] = sig_obj
            return sig_obj

        raise FileNotFoundError(f"No GCS signing secret key found for suite {target_suite_id}")

    return load_secret_for_suite


def _build_matrix_public_loader(
    *,
    suite_id: Optional[str],
    default_public_path: Optional[Path],
    initial_public: Optional[bytes],
    matrix_dir: Optional[Path] = None,
) -> Callable[[Dict[str, object]], bytes]:
    """Return loader that fetches per-suite GCS signing public keys from disk."""

    lock = threading.Lock()
    cache: Dict[str, bytes] = {}
    if suite_id and initial_public is not None:
        cache[suite_id] = initial_public

    matrix_public_dir = matrix_dir or Path("secrets/matrix")

    def load_public_for_suite(target_suite: Dict[str, object]) -> bytes:
        target_suite_id = target_suite.get("suite_id") if isinstance(target_suite, dict) else None
        if not target_suite_id:
            raise RuntimeError("Suite dictionary missing suite_id")

        with lock:
            cached = cache.get(target_suite_id)
            if cached is not None:
                return cached

        candidates = []
        if default_public_path and suite_id and target_suite_id == suite_id:
            candidates.append(default_public_path)
        candidates.append(matrix_public_dir / target_suite_id / "gcs_signing.pub")

        seen: Dict[str, None] = {}
        for candidate in candidates:
            candidate_path = candidate.expanduser()
            key = str(candidate_path.resolve()) if candidate_path.exists() else str(candidate_path)
            if key in seen:
                continue
            seen[key] = None
            if not candidate_path.exists():
                continue
            try:
                public_bytes = candidate_path.read_bytes()
            except Exception as exc:
                raise RuntimeError(f"Failed to read GCS public key {candidate_path}: {exc}") from exc
            with lock:
                cache[target_suite_id] = public_bytes
            return public_bytes

        raise FileNotFoundError(f"No GCS signing public key found for suite {target_suite_id}")

    return load_public_for_suite


def signal_handler(signum, frame):
    """Handle interrupt signals gracefully."""
    print("\nReceived interrupt signal. Shutting down...")
    sys.exit(0)


def create_secrets_dir():
    """Create secrets directory if it doesn't exist."""
    secrets_dir = Path("secrets")
    secrets_dir.mkdir(exist_ok=True)
    return secrets_dir


def write_json_report(json_path: Optional[str], payload: dict, *, quiet: bool = False) -> None:
    """Persist counters payload to JSON if a path is provided."""

    if not json_path:
        return

    try:
        path = Path(json_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        if not quiet:
            print(f"Wrote JSON report to {path}")
    except Exception as exc:
        print(f"Warning: Failed to write JSON output to {json_path}: {exc}")


def _resolve_suite(args, role_label: str) -> dict:
    """Resolve suite via legacy --suite or new --kem/--aead/--sig components."""

    suite_arg = getattr(args, "suite", None)
    kem = getattr(args, "kem", None)
    sig = getattr(args, "sig", None)
    aead = getattr(args, "aead", None)

    if suite_arg and any(v is not None for v in (kem, sig, aead)):
        print("Error: --suite cannot be combined with --kem/--sig/--aead")
        sys.exit(1)

    try:
        if suite_arg:
            suite = get_suite(suite_arg)
        elif any(v is not None for v in (kem, sig, aead)):
            if not all(v is not None for v in (kem, sig, aead)):
                print("Error: --kem, --sig, and --aead must be provided together")
                sys.exit(1)
            suite_id = build_suite_id(kem, aead, sig)
            suite = get_suite(suite_id)
        else:
            print(f"Error: {role_label} requires --suite or --kem/--sig/--aead")
            sys.exit(1)
    except NotImplementedError as exc:
        print(f"Error: {exc}")
        sys.exit(1)

    # Normalize suite argument for downstream logging
    setattr(args, "suite", suite.get("suite_id", getattr(args, "suite", None)))
    return suite


def init_identity_command(args):
    """Create GCS signing identity and save to persistent files."""
    # Use custom output_dir if provided, otherwise default secrets directory
    if hasattr(args, 'output_dir') and args.output_dir:
        secrets_dir = Path(args.output_dir)
        secrets_dir.mkdir(parents=True, exist_ok=True)
    else:
        secrets_dir = create_secrets_dir()
    
    try:
        suite = get_suite(args.suite) if hasattr(args, 'suite') and args.suite else get_suite(DEFAULT_SUITE_ID)
    except KeyError as e:
        print(f"Error: Unknown suite: {args.suite if hasattr(args, 'suite') else 'default'}")
        sys.exit(1)
    
    secret_path = secrets_dir / "gcs_signing.key"
    public_path = secrets_dir / "gcs_signing.pub"
    
    if secret_path.exists() or public_path.exists():
        print("Warning: Identity files already exist. Overwriting with a new keypair.")
    
    Signature = _require_signature_class()

    try:
        sig = Signature(suite["sig_name"])
        if hasattr(sig, 'export_secret_key'):
            gcs_sig_public = sig.generate_keypair()
            gcs_sig_secret = sig.export_secret_key()
            
            # Write files with appropriate permissions
            secret_path.write_bytes(gcs_sig_secret)
            public_path.write_bytes(gcs_sig_public)
            
            # Secure the secret file
            try:
                os.chmod(secret_path, 0o600)
            except Exception:
                pass  # Best effort on Windows
                
            print(f"Created GCS signing identity (suite={suite['suite_id']}):")
            print(f"  Secret: {secret_path}")
            print(f"  Public: {public_path}")
            print(f"  Public key (hex): {gcs_sig_public.hex()}")
            return 0  # Success
            
        else:
            print("Error: oqs build lacks key import/export; use --ephemeral or upgrade oqs-python.")
            sys.exit(1)
            
    except Exception as e:
        # Resilience: attempt fallback to ML-DSA-65 if new signature unsupported
        fallback_suite_id = DEFAULT_SUITE_ID
        if suite.get("sig_name") != "ML-DSA-65":
            try:
                fallback_suite = get_suite(fallback_suite_id)
                print(f"Warning: primary suite '{suite['suite_id']}' signature unsupported ({e}); falling back to '{fallback_suite_id}'.")
                sig_fb = Signature(fallback_suite["sig_name"])
                if hasattr(sig_fb, 'export_secret_key'):
                    fb_pub = sig_fb.generate_keypair()
                    fb_sec = sig_fb.export_secret_key()
                    secret_path.write_bytes(fb_sec)
                    public_path.write_bytes(fb_pub)
                    try:
                        os.chmod(secret_path, 0o600)
                    except Exception:
                        pass
                    print(f"Created fallback GCS signing identity (suite={fallback_suite['suite_id']}):")
                    print(f"  Secret: {secret_path}")
                    print(f"  Public: {public_path}")
                    print(f"  Public key (hex): {fb_pub.hex()}")
                    return 0
            except Exception as fb_exc:
                print(f"Fallback identity creation failed: {fb_exc}")
        print(f"Error creating identity: {e}")
        sys.exit(1)


def gcs_command(args):
    """Start GCS proxy."""
    suite = _resolve_suite(args, "GCS proxy")
    suite_id = suite["suite_id"]
    
    Signature = _require_signature_class()
    proxy_runner = _require_run_proxy()

    gcs_sig_secret = None
    gcs_sig_public = None
    json_out_path = getattr(args, "json_out", None)
    quiet = getattr(args, "quiet", False)
    status_file = getattr(args, "status_file", None)
    primary_secret_path: Optional[Path] = None

    def info(msg: str) -> None:
        if not quiet:
            print(msg)
    
    if args.ephemeral:
        info("⚠️  WARNING: Using EPHEMERAL keys - not suitable for production!")
        info("⚠️  Key will be lost when process exits.")
        if not quiet:
            print()
        
        # Generate ephemeral keypair
        sig = Signature(suite["sig_name"])
        gcs_sig_public = sig.generate_keypair()
        gcs_sig_secret = sig
        info("Generated ephemeral GCS signing keypair:")
        if not quiet:
            print(f"Public key (hex): {gcs_sig_public.hex()}")
            print("Provide this to the drone via --gcs-pub-hex or --peer-pubkey-file")
            print()
        primary_secret_path = None
        
    else:
        # Load persistent key
        if args.gcs_secret_file:
            secret_path = Path(args.gcs_secret_file)
        else:
            secret_path = Path("secrets/gcs_signing.key")
            
        if not secret_path.exists():
            print(f"Error: Secret key file not found: {secret_path}")
            print("Run 'python -m core.run_proxy init-identity' to create one,")
            print("or use --ephemeral for development only.")
            sys.exit(1)
            
        secret_bytes = None
        try:
            secret_bytes = secret_path.read_bytes()
        except Exception as exc:
            print(f"Error reading secret key file: {exc}")
            sys.exit(1)

        load_errors = []
        imported_public: Optional[bytes] = None
        load_method: Optional[str] = None

        try:
            primary_sig = Signature(suite["sig_name"])
        except Exception as exc:
            load_errors.append(f"Signature ctor failed: {exc}")
            primary_sig = None  # type: ignore

        if primary_sig is not None and hasattr(primary_sig, "import_secret_key"):
            try:
                imported_public = primary_sig.import_secret_key(secret_bytes)
                gcs_sig_secret = primary_sig
                load_method = "import_secret_key"
            except Exception as exc:
                load_errors.append(f"import_secret_key failed: {exc}")

        if gcs_sig_secret is None:
            try:
                fallback_sig = Signature(suite["sig_name"], secret_key=secret_bytes)
                gcs_sig_secret = fallback_sig
                load_method = "ctor_secret_key"
            except TypeError as exc:
                load_errors.append(f"ctor secret_key unsupported: {exc}")
            except Exception as exc:
                load_errors.append(f"ctor secret_key failed: {exc}")

        if gcs_sig_secret is None:
            print("Error: oqs build lacks usable key import. Tried import_secret_key and constructor fallback without success.")
            if load_errors:
                print("Details:")
                for err in load_errors:
                    print(f"  - {err}")
            print("Consider running with --ephemeral or upgrading oqs-python/liboqs with key import support.")
            sys.exit(1)

        info("Loaded GCS signing key from file.")
        if load_method == "ctor_secret_key":
            info("Using constructor-based fallback because import/export APIs are unavailable.")

        gcs_sig_public = imported_public
        if gcs_sig_public is None:
            public_candidates = []
            if secret_path.suffix:
                public_candidates.append(secret_path.with_suffix(".pub"))
            public_candidates.append(secret_path.parent / "gcs_signing.pub")
            seen = set()
            for candidate in public_candidates:
                key = str(candidate.resolve()) if candidate.exists() else str(candidate)
                if key in seen:
                    continue
                seen.add(key)
                if candidate.exists():
                    try:
                        gcs_sig_public = candidate.read_bytes()
                        info(f"Loaded public key from {candidate}.")
                    except Exception as exc:
                        load_errors.append(f"public key read failed ({candidate}): {exc}")
                    break

        if gcs_sig_public is not None and not quiet:
            print(f"Public key (hex): {gcs_sig_public.hex()}")
        elif gcs_sig_public is None and not quiet:
            print("Warning: Could not locate public key file for display. Ensure the drone has the matching public key.")
        if not quiet:
            print()
        primary_secret_path = secret_path
    load_secret_for_suite = _build_matrix_secret_loader(
        suite_id=suite_id,
        default_secret_path=primary_secret_path,
        initial_secret=gcs_sig_secret,
        signature_cls=Signature,
    )

    try:
        log_path = configure_file_logger("gcs", logger)
        if not quiet:
            print(f"Log file: {log_path}")

        info(f"Starting GCS proxy with suite {suite_id}")
        if args.stop_seconds:
            info(f"Will auto-stop after {args.stop_seconds} seconds")
        if not quiet:
            print()
        
        counters = proxy_runner(
            role="gcs",
            suite=suite,
            cfg=CONFIG,
            gcs_sig_secret=gcs_sig_secret,
            gcs_sig_public=None,
            stop_after_seconds=args.stop_seconds,
            manual_control=getattr(args, "control_manual", False),
            quiet=quiet,
            status_file=status_file,
            load_gcs_secret=load_secret_for_suite,
        )

        _augment_part_b_metrics(counters)

        # Log final counters as JSON
        logger.info("GCS proxy shutdown", extra={"counters": counters})
        
        if not quiet:
            print("GCS proxy stopped. Final counters:")
            _pretty_print_counters(counters)

        payload = {
            "role": "gcs",
            "suite": suite_id,
            "counters": counters,
            "ts_stop_ns": time.time_ns(),
        }
        write_json_report(json_out_path, payload, quiet=quiet)
            
    except KeyboardInterrupt:
        if not quiet:
            print("\nGCS proxy stopped by user.")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


def drone_command(args):
    """Start drone proxy."""
    suite = _resolve_suite(args, "Drone proxy")
    suite_id = suite["suite_id"]
    
    proxy_runner = _require_run_proxy()

    # Get GCS public key
    gcs_sig_public = None
    json_out_path = getattr(args, "json_out", None)
    quiet = getattr(args, "quiet", False)
    status_file = getattr(args, "status_file", None)
    primary_public_path: Optional[Path] = None

    def info(msg: str) -> None:
        if not quiet:
            print(msg)
    
    try:
        if args.peer_pubkey_file:
            pub_path = Path(args.peer_pubkey_file)
            if not pub_path.exists():
                raise FileNotFoundError(f"Public key file not found: {pub_path}")
            gcs_sig_public = pub_path.read_bytes()
            primary_public_path = pub_path
        elif args.gcs_pub_hex:
            gcs_sig_public = bytes.fromhex(args.gcs_pub_hex)
        else:
            # Try default location
            default_pub = Path("secrets/gcs_signing.pub")
            if default_pub.exists():
                gcs_sig_public = default_pub.read_bytes()
                info(f"Using GCS public key from: {default_pub}")
                primary_public_path = default_pub
            else:
                raise ValueError("No GCS public key provided. Use --peer-pubkey-file, --gcs-pub-hex, or ensure secrets/gcs_signing.pub exists.")
                
    except Exception as e:
        print(f"Error loading GCS public key: {e}")
        sys.exit(1)
    
    try:
        log_path = configure_file_logger("drone", logger)
        if not quiet:
            print(f"Log file: {log_path}")

        info(f"Starting drone proxy with suite {suite_id}")
        if args.stop_seconds:
            info(f"Will auto-stop after {args.stop_seconds} seconds")
        if not quiet:
            print()

        load_public_for_suite = _build_matrix_public_loader(
            suite_id=suite_id,
            default_public_path=primary_public_path,
            initial_public=gcs_sig_public,
        )
        
        counters = proxy_runner(
            role="drone",
            suite=suite,
            cfg=CONFIG,
            gcs_sig_secret=None,
            gcs_sig_public=gcs_sig_public,
            stop_after_seconds=args.stop_seconds,
            manual_control=getattr(args, "control_manual", False),
            quiet=quiet,
            status_file=status_file,
            load_gcs_public=load_public_for_suite,
        )
        
        _augment_part_b_metrics(counters)

        # Log final counters as JSON
        logger.info("Drone proxy shutdown", extra={"counters": counters})
        
        if not quiet:
            print("Drone proxy stopped. Final counters:")
            _pretty_print_counters(counters)

        payload = {
            "role": "drone",
            "suite": suite_id,
            "counters": counters,
            "ts_stop_ns": time.time_ns(),
        }
        write_json_report(json_out_path, payload, quiet=quiet)
            
    except KeyboardInterrupt:
        if not quiet:
            print("\nDrone proxy stopped by user.")
    except Exception as e:
        print(f"Error: {e}")
        sys.exit(1)


def main():
    """Main CLI entrypoint with subcommands."""
    # Set up signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    if hasattr(signal, 'SIGTERM'):
        signal.signal(signal.SIGTERM, signal_handler)
    
    parser = argparse.ArgumentParser(description="PQC Drone-GCS Secure Proxy")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # init-identity subcommand
    init_parser = subparsers.add_parser('init-identity', 
                                       help='Create persistent GCS signing identity')
    init_parser.add_argument(
        "--suite",
        default=DEFAULT_SUITE_ID,
        help="Cryptographic suite ID (defaults to core.suites.DEFAULT_SUITE_ID)",
    )
    init_parser.add_argument("--output-dir", 
                            help="Directory for key files (default: secrets/)")
    
    # gcs subcommand
    gcs_parser = subparsers.add_parser('gcs', help='Start GCS proxy')
    gcs_parser.add_argument("--suite", help="Cryptographic suite ID")
    gcs_parser.add_argument("--kem",
                           help="KEM alias (e.g., ML-KEM-768, kyber768)")
    gcs_parser.add_argument("--aead",
                           help="AEAD alias (e.g., AES-GCM)")
    gcs_parser.add_argument("--sig",
                           help="Signature alias (e.g., ML-DSA-65, dilithium3)")
    gcs_parser.add_argument("--gcs-secret-file",
                           help="Path to GCS secret key file (default: secrets/gcs_signing.key)")
    gcs_parser.add_argument("--ephemeral", action='store_true',
                           help="Use ephemeral keys (development only - prints warning)")
    gcs_parser.add_argument("--stop-seconds", type=float,
                           help="Auto-stop after N seconds (for testing)")
    gcs_parser.add_argument("--quiet", action="store_true",
                           help="Suppress informational prints (warnings/errors still shown)")
    gcs_parser.add_argument("--json-out",
                           help="Optional path to write counters JSON on shutdown")
    gcs_parser.add_argument("--control-manual", action="store_true",
                           help="Enable interactive manual in-band rekey control thread")
    gcs_parser.add_argument("--status-file",
                           help="Path to write proxy status JSON updates (handshake/rekey)")
    
    # drone subcommand
    drone_parser = subparsers.add_parser('drone', help='Start drone proxy')
    drone_parser.add_argument("--suite", help="Cryptographic suite ID")
    drone_parser.add_argument("--kem",
                             help="KEM alias (e.g., ML-KEM-768, kyber768)")
    drone_parser.add_argument("--aead",
                             help="AEAD alias (e.g., AES-GCM)")
    drone_parser.add_argument("--sig",
                             help="Signature alias (e.g., ML-DSA-65, dilithium3)")
    drone_parser.add_argument("--peer-pubkey-file",
                             help="Path to GCS public key file (default: secrets/gcs_signing.pub)")
    drone_parser.add_argument("--gcs-pub-hex",
                             help="GCS public key as hex string")
    drone_parser.add_argument("--stop-seconds", type=float,
                             help="Auto-stop after N seconds (for testing)")
    drone_parser.add_argument("--quiet", action="store_true",
                              help="Suppress informational prints (warnings/errors still shown)")
    drone_parser.add_argument("--json-out",
                              help="Optional path to write counters JSON on shutdown")
    drone_parser.add_argument("--status-file",
                              help="Path to write proxy status JSON updates (handshake/rekey)")
    drone_parser.add_argument("--control-manual", action="store_true",
                              help="Enable interactive manual in-band rekey control thread")
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # Validate required CONFIG keys
    required_keys = [
        "TCP_HANDSHAKE_PORT", "UDP_DRONE_RX", "UDP_GCS_RX", 
        "DRONE_PLAINTEXT_TX", "DRONE_PLAINTEXT_RX",
        "GCS_PLAINTEXT_TX", "GCS_PLAINTEXT_RX", 
        "DRONE_HOST", "GCS_HOST", "REPLAY_WINDOW"
    ]
    
    missing_keys = [key for key in required_keys if key not in CONFIG]
    if missing_keys:
        print(f"Error: CONFIG missing required keys: {', '.join(missing_keys)}")
        sys.exit(1)
    
    # Route to appropriate command handler
    if args.command == 'init-identity':
        init_identity_command(args)
    elif args.command == 'gcs':
        if getattr(args, "quiet", False):
            logger.setLevel(logging.WARNING)
        gcs_command(args)
    elif args.command == 'drone':
        if getattr(args, "quiet", False):
            logger.setLevel(logging.WARNING)
        drone_command(args)


if __name__ == "__main__":
    main()
==================================================

core\suites.py
==================================================
"""PQC cryptographic suite registry and algorithm ID mapping.

Provides a composable {KEM × AEAD × SIG} registry with synonym resolution and
helpers for querying oqs availability.

NIST Security Level Reference (per liboqs / FIPS 203/204/205):
- L1: ~AES-128 equivalent security
- L3: ~AES-192 equivalent security  
- L5: ~AES-256 equivalent security

Note: ML-DSA-44 is claimed as L2 by liboqs (FIPS 204), but we map it to L1
for practical pairing with L1 KEMs (ML-KEM-512, etc.).
"""

from __future__ import annotations

from types import MappingProxyType
from typing import Dict, Iterable, Tuple
from core.logging_utils import get_logger
from core.config import CONFIG
import os

_logger = get_logger("pqc")


# Default bootstrap suite used when callers do not specify a suite.
# Keep suite IDs centralized in this module.
DEFAULT_SUITE_ID = "cs-mlkem768-aesgcm-mldsa65"


def _normalize_alias(value: str) -> str:
    """Normalize alias strings for case- and punctuation-insensitive matching."""

    return "".join(ch for ch in value.lower() if ch.isalnum())


# =============================================================================
# KEM Registry - NIST Levels per liboqs/FIPS203
# ML-KEM-512: L1, ML-KEM-768: L3, ML-KEM-1024: L5
# Classic-McEliece-348864: L1, -460896: L3, -8192128: L5
# HQC-128: L1, HQC-192: L3, HQC-256: L5
# =============================================================================
_KEM_REGISTRY = {
    "mlkem512": {
        "oqs_name": "ML-KEM-512",
        "token": "mlkem512",
        "nist_level": "L1",
        "kem_id": 1,
        "kem_param_id": 1,
        "aliases": (
            "ML-KEM-512",
            "ml-kem-512",
            "mlkem512",
            "kyber512",
            "kyber-512",
            "kyber_512",
            "Kyber512",
        ),
    },
    "mlkem768": {
        "oqs_name": "ML-KEM-768",
        "token": "mlkem768",
        "nist_level": "L3",
        "kem_id": 1,
        "kem_param_id": 2,
        "aliases": (
            "ML-KEM-768",
            "ml-kem-768",
            "mlkem768",
            "kyber768",
            "kyber-768",
            "kyber_768",
            "Kyber768",
        ),
    },
    "mlkem1024": {
        "oqs_name": "ML-KEM-1024",
        "token": "mlkem1024",
        "nist_level": "L5",
        "kem_id": 1,
        "kem_param_id": 3,
        "aliases": (
            "ML-KEM-1024",
            "ml-kem-1024",
            "mlkem1024",
            "kyber1024",
            "kyber-1024",
            "kyber_1024",
            "Kyber1024",
        ),
    },
    "classicmceliece348864": {
        "oqs_name": "Classic-McEliece-348864",
        "token": "classicmceliece348864",
        "nist_level": "L1",
        "kem_id": 3,
        "kem_param_id": 1,
        "aliases": (
            "Classic-McEliece-348864",
            "classicmceliece-348864",
            "classicmceliece348864",
            "mceliece348864",
        ),
    },
    "classicmceliece460896": {
        "oqs_name": "Classic-McEliece-460896",
        "token": "classicmceliece460896",
        "nist_level": "L3",
        "kem_id": 3,
        "kem_param_id": 2,
        "aliases": (
            "Classic-McEliece-460896",
            "classicmceliece-460896",
            "classicmceliece460896",
            "mceliece460896",
        ),
    },
    "classicmceliece8192128": {
        "oqs_name": "Classic-McEliece-8192128",
        "token": "classicmceliece8192128",
        "nist_level": "L5",
        "kem_id": 3,
        "kem_param_id": 3,
        "aliases": (
            "Classic-McEliece-8192128",
            "classicmceliece-8192128",
            "classicmceliece8192128",
            "mceliece8192128",
        ),
    },
    "hqc128": {
        "oqs_name": "HQC-128",
        "token": "hqc128",
        "nist_level": "L1",
        "kem_id": 5,
        "kem_param_id": 1,
        "aliases": (
            "HQC-128",
            "hqc-128",
            "hqc128",
        ),
    },
    "hqc192": {
        "oqs_name": "HQC-192",
        "token": "hqc192",
        "nist_level": "L3",
        "kem_id": 5,
        "kem_param_id": 2,
        "aliases": (
            "HQC-192",
            "hqc-192",
            "hqc192",
        ),
    },
    "hqc256": {
        "oqs_name": "HQC-256",
        "token": "hqc256",
        "nist_level": "L5",
        "kem_id": 5,
        "kem_param_id": 3,
        "aliases": (
            "HQC-256",
            "hqc-256",
            "hqc256",
        ),
    },
}


# =============================================================================
# Signature Registry - NIST Levels per liboqs/FIPS204/FIPS205
# ML-DSA-44: L2 (liboqs), but we use L1 for practical pairing with ML-KEM-512
# ML-DSA-65: L3, ML-DSA-87: L5
# Falcon-512: L1, Falcon-1024: L5 (no L3 variant exists in NIST standards)
# SPHINCS+-128s: L1, SPHINCS+-192s: L3, SPHINCS+-256s: L5
# =============================================================================
_SIG_REGISTRY = {
    "mldsa44": {
        "oqs_name": "ML-DSA-44",
        "token": "mldsa44",
        "nist_level": "L1",  # Practical: pairs with L1 KEMs; liboqs claims L2
        "sig_id": 1,
        "sig_param_id": 1,
        "aliases": (
            "ML-DSA-44",
            "ml-dsa-44",
            "mldsa44",
            "dilithium2",
            "dilithium-2",
            "Dilithium2",
        ),
    },
    "mldsa65": {
        "oqs_name": "ML-DSA-65",
        "token": "mldsa65",
        "nist_level": "L3",
        "sig_id": 1,
        "sig_param_id": 2,
        "aliases": (
            "ML-DSA-65",
            "ml-dsa-65",
            "mldsa65",
            "dilithium3",
            "dilithium-3",
            "Dilithium3",
        ),
    },
    "mldsa87": {
        "oqs_name": "ML-DSA-87",
        "token": "mldsa87",
        "nist_level": "L5",
        "sig_id": 1,
        "sig_param_id": 3,
        "aliases": (
            "ML-DSA-87",
            "ml-dsa-87",
            "mldsa87",
            "dilithium5",
            "dilithium-5",
            "Dilithium5",
        ),
    },
    # Falcon signatures - NTRU-lattice based, compact signatures
    # Falcon-512: L1, Falcon-1024: L5 (no L3 variant per NIST)
    "falcon512": {
        "oqs_name": "Falcon-512",
        "token": "falcon512",
        "nist_level": "L1",
        "sig_id": 2,
        "sig_param_id": 1,
        "aliases": (
            "Falcon-512",
            "falcon-512",
            "falcon512",
            "Falcon512",
        ),
    },
    "falcon1024": {
        "oqs_name": "Falcon-1024",
        "token": "falcon1024",
        "nist_level": "L5",
        "sig_id": 2,
        "sig_param_id": 2,
        "aliases": (
            "Falcon-1024",
            "falcon-1024",
            "falcon1024",
            "Falcon1024",
        ),
    },
    # SPHINCS+ hash-based signatures (stateless)
    "sphincs128s": {
        "oqs_name": "SPHINCS+-SHA2-128s-simple",
        "token": "sphincs128s",
        "nist_level": "L1",
        "sig_id": 3,
        "sig_param_id": 1,
        "aliases": (
            "SLH-DSA-SHA2-128s",
            "SPHINCS+-SHA2-128s-simple",
            "sphincs+-sha2-128s-simple",
            "sphincs128s",
            "sphincs128s_sha2",
            # Fast variant aliases (f vs s - both map to our s variant)
            "sphincs128f",
            "sphincs128fsha2",
            "sphincs128f_sha2",
            "SPHINCS+128s",
        ),
    },
    "sphincs192s": {
        "oqs_name": "SPHINCS+-SHA2-192s-simple",
        "token": "sphincs192s",
        "nist_level": "L3",
        "sig_id": 3,
        "sig_param_id": 2,
        "aliases": (
            "SLH-DSA-SHA2-192s",
            "SPHINCS+-SHA2-192s-simple",
            "sphincs+-sha2-192s-simple",
            "sphincs192s",
            "sphincs192s_sha2",
            "sphincs192f",
            "sphincs192fsha2",
            "sphincs192f_sha2",
            "SPHINCS+192s",
        ),
    },
    "sphincs256s": {
        "oqs_name": "SPHINCS+-SHA2-256s-simple",
        "token": "sphincs256s",
        "nist_level": "L5",
        "sig_id": 3,
        "sig_param_id": 3,
        "aliases": (
            "SLH-DSA-SHA2-256s",
            "SPHINCS+-SHA2-256s-simple",
            "sphincs+-sha2-256s-simple",
            "sphincs256s",
            "sphincs256s_sha2",
            # Fast variant aliases
            "sphincs256f",
            "sphincs256fsha2",
            "sphincs256f_sha2",
            "SPHINCS+256s",
        ),
    },
}


_AEAD_REGISTRY = {
    "aesgcm": {
        "display_name": "AES-256-GCM",
        "token": "aesgcm",
        "kdf": "HKDF-SHA256",
        "aliases": (
            "AES-256-GCM",
            "aes-256-gcm",
            "aesgcm",
            "aes256gcm",
            "aes-gcm",
            "AESGCM",
        ),
    },
    "chacha20poly1305": {
        "display_name": "ChaCha20-Poly1305",
        "token": "chacha20poly1305",
        "kdf": "HKDF-SHA256",
        "aliases": (
            "ChaCha20-Poly1305",
            "chacha20poly1305",
            "chacha20-poly1305",
            "chacha20",
            "ChaCha20Poly1305",
        ),
    },
    "ascon128a": {
        "display_name": "Ascon-128a",
        "token": "ascon128a",
        "kdf": "HKDF-SHA256",
        "aliases": (
            "Ascon-128a",
            "ascon128a",
            "ascon-128a",
            "ascona",
            "Ascon128a",
        ),
    },
}


def _probe_aead_support() -> Tuple[Tuple[str, ...], Dict[str, str]]:
    """Detect AEAD algorithm support available in the current runtime.

    Note: the suite registry always includes Ascon-128a; this probe reports whether the
    current runtime can actually instantiate that AEAD.

    Ascon-128a may be provided either by the compiled `core._ascon_native` module or by
    the pure-Python `pyascon` fallback.
    Returns (available_tokens, missing_reason_map).
    """

    available: list[str] = ["aesgcm"]
    missing: Dict[str, str] = {}

    # ChaCha20-Poly1305 is optional
    try:  # pragma: no cover - build dependent
        from cryptography.hazmat.primitives.ciphers.aead import ChaCha20Poly1305  # type: ignore
        if ChaCha20Poly1305 is None:  # type: ignore[truthy-bool]
            raise ImportError("ChaCha20Poly1305 unavailable in cryptography")
    except Exception as exc:  # pragma: no cover
        missing["chacha20poly1305"] = str(exc)
    else:
        available.append("chacha20poly1305")

    # Ascon-128a: may be disabled via config; native preferred; pure-python fallback acceptable.
    if not bool(CONFIG.get("ENABLE_ASCON", True)) or not bool(CONFIG.get("ENABLE_ASCON128A", True)):
        missing["ascon128a"] = "disabled_by_config"
        return tuple(available), missing

    ascon_available = False
    native_reason: str | None = None
    py_reason: str | None = None

    try:  # pragma: no cover - build/runtime dependent
        from core import _ascon_native as _ascon  # type: ignore
    except Exception as exc:  # pragma: no cover
        native_reason = f"core._ascon_native unavailable: {exc}"
    else:
        if not hasattr(_ascon, "encrypt") or not hasattr(_ascon, "decrypt"):
            native_reason = "core._ascon_native missing encrypt/decrypt exports"
        else:
            ascon_available = True

    if not ascon_available:
        try:  # pragma: no cover - optional dependency
            import pyascon as _pyascon  # type: ignore
        except Exception as exc:  # pragma: no cover
            py_reason = f"pyascon unavailable: {exc}"
        else:
            if hasattr(_pyascon, "ascon_encrypt") and hasattr(_pyascon, "ascon_decrypt"):
                ascon_available = True
            else:
                py_reason = "pyascon missing ascon_encrypt/ascon_decrypt"

    if ascon_available:
        available.append("ascon128a")
    else:
        missing["ascon128a"] = "; ".join(
            [reason for reason in (native_reason, py_reason) if reason]
        ) or "ascon backend unavailable"

    return tuple(available), missing


def available_aead_tokens() -> Tuple[str, ...]:
    """Return the AEAD tokens supported by this runtime."""

    supported, _ = _probe_aead_support()
    return supported


def unavailable_aead_reasons() -> Dict[str, str]:
    """Return descriptive reasons for AEAD algorithms that are unavailable."""

    _, missing = _probe_aead_support()
    return dict(missing)


def _build_alias_map(registry: Dict[str, Dict]) -> Dict[str, str]:
    alias_map: Dict[str, str] = {}
    for key, entry in registry.items():
        for alias in entry["aliases"]:
            normalized = _normalize_alias(alias)
            alias_map[normalized] = key
        alias_map[_normalize_alias(entry["oqs_name"]) if "oqs_name" in entry else _normalize_alias(entry["display_name"])] = key
        alias_map[_normalize_alias(entry["token"])] = key
    return alias_map


_KEM_ALIASES = _build_alias_map(_KEM_REGISTRY)
_SIG_ALIASES = _build_alias_map(_SIG_REGISTRY)
_AEAD_ALIASES = _build_alias_map(_AEAD_REGISTRY)


def _resolve_kem_key(name: str) -> str:
    lookup = _KEM_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise ValueError(f"unknown KEM: {name}")
    return lookup


def _resolve_sig_key(name: str) -> str:
    lookup = _SIG_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise ValueError(f"unknown signature: {name}")
    return lookup


def _resolve_aead_key(name: str) -> str:
    lookup = _AEAD_ALIASES.get(_normalize_alias(name))
    if lookup is None:
        raise ValueError(f"unknown AEAD: {name}")
    return lookup


def build_suite_id(kem: str, aead: str, sig: str) -> str:
    """Build canonical suite identifier from component aliases."""

    kem_key = _resolve_kem_key(kem)
    aead_key = _resolve_aead_key(aead)
    sig_key = _resolve_sig_key(sig)

    kem_entry = _KEM_REGISTRY[kem_key]
    aead_entry = _AEAD_REGISTRY[aead_key]
    sig_entry = _SIG_REGISTRY[sig_key]

    return f"cs-{kem_entry['token']}-{aead_entry['token']}-{sig_entry['token']}"


_SUITE_ALIASES = {
    # Kyber -> ML-KEM aliases with Dilithium -> ML-DSA
    "cs-kyber512-aesgcm-dilithium2": "cs-mlkem512-aesgcm-mldsa44",
    "cs-kyber768-aesgcm-dilithium3": "cs-mlkem768-aesgcm-mldsa65",
    "cs-kyber1024-aesgcm-dilithium5": "cs-mlkem1024-aesgcm-mldsa87",
    # Kyber + Falcon suites
    "cs-kyber512-aesgcm-falcon512": "cs-mlkem512-aesgcm-falcon512",
    "cs-kyber768-aesgcm-falcon512": "cs-mlkem768-aesgcm-falcon512",
    "cs-kyber1024-aesgcm-falcon1024": "cs-mlkem1024-aesgcm-falcon1024",
    # SPHINCS+ f/s variant aliases (fast -> small)
    "cs-kyber512-aesgcm-sphincs128f_sha2": "cs-mlkem512-aesgcm-sphincs128s",
    "cs-kyber512-aesgcm-sphincs128fsha2": "cs-mlkem512-aesgcm-sphincs128s",
    "cs-kyber1024-aesgcm-sphincs256f_sha2": "cs-mlkem1024-aesgcm-sphincs256s",
    "cs-kyber1024-aesgcm-sphincs256fsha2": "cs-mlkem1024-aesgcm-sphincs256s",
    # Classic-McEliece + SPHINCS+ f variant
    "cs-classicmceliece348864-aesgcm-sphincs128fsha2": "cs-classicmceliece348864-aesgcm-sphincs128s",
    "cs-classicmceliece348864-chacha20poly1305-sphincs128fsha2": "cs-classicmceliece348864-chacha20poly1305-sphincs128s",
    "cs-classicmceliece8192128-aesgcm-sphincs256fsha2": "cs-classicmceliece8192128-aesgcm-sphincs256s",
    "cs-classicmceliece8192128-chacha20poly1305-sphincs256fsha2": "cs-classicmceliece8192128-chacha20poly1305-sphincs256s",
    # HQC + SPHINCS+ f variant
    "cs-hqc128-aesgcm-sphincs128fsha2": "cs-hqc128-aesgcm-sphincs128s",
    "cs-hqc128-chacha20poly1305-sphincs128fsha2": "cs-hqc128-chacha20poly1305-sphincs128s",
    "cs-hqc256-aesgcm-sphincs256fsha2": "cs-hqc256-aesgcm-sphincs256s",
    "cs-hqc256-chacha20poly1305-sphincs256fsha2": "cs-hqc256-chacha20poly1305-sphincs256s",
}


def _compose_suite(kem_key: str, aead_key: str, sig_key: str) -> Dict[str, object]:
    kem_entry = _KEM_REGISTRY[kem_key]
    aead_entry = _AEAD_REGISTRY[aead_key]
    sig_entry = _SIG_REGISTRY[sig_key]

    if kem_entry["nist_level"] != sig_entry["nist_level"]:
        raise NotImplementedError(
            f"NIST level mismatch for {kem_entry['oqs_name']} / {sig_entry['oqs_name']}"
        )

    suite_id = f"cs-{kem_entry['token']}-{aead_entry['token']}-{sig_entry['token']}"

    return {
        "suite_id": suite_id,
        "kem_name": kem_entry["oqs_name"],
        "kem_id": kem_entry["kem_id"],
        "kem_param_id": kem_entry["kem_param_id"],
        "sig_name": sig_entry["oqs_name"],
        "sig_id": sig_entry["sig_id"],
        "sig_param_id": sig_entry["sig_param_id"],
        "nist_level": kem_entry["nist_level"],
        "aead": aead_entry["display_name"],
        "kdf": aead_entry["kdf"],
        "aead_token": aead_entry["token"],
    }

def _generate_level_consistent_matrix() -> Tuple[Tuple[str, str], ...]:
    """Generate matrix of (kem_key, sig_key) pairs sharing identical NIST level.

    This expands prior static matrix to all level-aligned combinations while
    preserving backward compatibility (legacy combos remain valid subset).
    """
    # Allow runtime ignore lists for KEMs/AEADs: keep registry entries,
    # but avoid generating suites that include ignored primitives.
    _DEFAULT_IGNORED_KEMS = ()

    # Environment overrides (comma-separated keys matching registry keys)
    ignored_kems_env = os.getenv("SUITES_IGNORE_KEMS", "").strip()
    ignored_aeads_env = os.getenv("SUITES_IGNORE_AEADS", "").strip()

    ignored_kems = set(_DEFAULT_IGNORED_KEMS)
    ignored_aeads = set()
    if ignored_kems_env:
        ignored_kems.update(k.strip() for k in ignored_kems_env.split(",") if k.strip())
    if ignored_aeads_env:
        ignored_aeads.update(a.strip() for a in ignored_aeads_env.split(",") if a.strip())

    pairs: list[Tuple[str, str]] = []
    for kem_key, kem_entry in _KEM_REGISTRY.items():
        if kem_key in ignored_kems:
            # skip composing suites with ignored KEMs
            continue
        kem_level = kem_entry.get("nist_level")
        for sig_key, sig_entry in _SIG_REGISTRY.items():
            if sig_entry.get("nist_level") == kem_level:
                pairs.append((kem_key, sig_key))
    # Deterministic order: sort by kem token then signature token
    pairs.sort(key=lambda t: (t[0], t[1]))
    return tuple(pairs)

_SUITE_MATRIX: Tuple[Tuple[str, str], ...] = _generate_level_consistent_matrix()

_AEAD_ORDER: Tuple[str, ...] = ("aesgcm", "chacha20poly1305", "ascon128a")

def valid_nist_levels() -> Tuple[str, ...]:
    """Return distinct NIST security levels present in the registry."""
    levels = {entry["nist_level"] for entry in _KEM_REGISTRY.values()} | {entry["nist_level"] for entry in _SIG_REGISTRY.values()}
    ordered = sorted(levels)
    return tuple(ordered)

def list_suites_for_level(level: str) -> Dict[str, Dict]:
    """List suites restricted to a single NIST level.

    Raises ValueError if level is not present. Returns mapping of suite_id->suite dict copy.
    """
    if level not in {e["nist_level"] for e in _KEM_REGISTRY.values()}:
        raise ValueError(f"unknown NIST level: {level}")
    result: Dict[str, Dict] = {}
    for sid, cfg in SUITES.items():
        if cfg.get("nist_level") == level:
            result[sid] = dict(cfg)
    return result

def filter_suites_by_levels(levels: Iterable[str]) -> Tuple[str, ...]:
    """Return tuple of suite_ids whose nist_level is in provided iterable.

    Invalid levels raise ValueError.
    """
    level_set = set(levels)
    known = {e["nist_level"] for e in _KEM_REGISTRY.values()}
    if not level_set.issubset(known):
        unknown = level_set - known
        raise ValueError(f"unknown NIST levels requested: {sorted(unknown)}")
    return tuple(sid for sid, cfg in SUITES.items() if cfg.get("nist_level") in level_set)


def _canonicalize_suite_id(suite_id: str) -> str:
    if not suite_id:
        raise ValueError("suite_id cannot be empty")

    candidate = suite_id.strip()
    if candidate in _SUITE_ALIASES:
        return _SUITE_ALIASES[candidate]

    if not candidate.startswith("cs-"):
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    parts = candidate[3:].split("-")
    if len(parts) < 3:
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    kem_part = parts[0]
    aead_part = parts[1]
    sig_part = "-".join(parts[2:])

    try:
        return build_suite_id(kem_part, aead_part, sig_part)
    except ValueError as exc:
        raise ValueError(f"unknown suite_id: {suite_id}") from exc


def _generate_suite_registry() -> MappingProxyType:
    suites: Dict[str, MappingProxyType] = {}
    for kem_key, sig_key in _SUITE_MATRIX:
        if kem_key not in _KEM_REGISTRY:
            raise ValueError(f"unknown KEM in suite matrix: {kem_key}")
        if sig_key not in _SIG_REGISTRY:
            raise ValueError(f"unknown signature in suite matrix: {sig_key}")
        # Skip suites that use ignored AEAD tokens
        # Resolve ignored AEADs from environment if provided (see _generate_level_consistent_matrix)
        ignored_aeads_env = os.getenv("SUITES_IGNORE_AEADS", "").strip()
        # Ignore list keeps optional AEADs out unless enabled.
        ignored_aeads: set[str] = set()
        if ignored_aeads_env:
            ignored_aeads.update(a.strip() for a in ignored_aeads_env.split(",") if a.strip())

        for aead_key in _AEAD_ORDER:
            if aead_key in ignored_aeads:
                continue
            suite_dict = _compose_suite(kem_key, aead_key, sig_key)
            suites[suite_dict["suite_id"]] = MappingProxyType(suite_dict)
    return MappingProxyType(suites)


SUITES = _generate_suite_registry()


def list_suites() -> Dict[str, Dict]:
    """Return all available suites as immutable mapping."""

    return {suite_id: dict(config) for suite_id, config in SUITES.items()}


def get_suite(suite_id: str) -> Dict:
    """Get suite configuration by ID, resolving legacy aliases and synonyms."""

    canonical_id = _canonicalize_suite_id(suite_id)

    if canonical_id not in SUITES:
        raise NotImplementedError(f"unknown suite_id: {suite_id}")

    suite = SUITES[canonical_id]

    required_fields = {"kem_name", "sig_name", "aead", "kdf", "nist_level"}
    missing_fields = required_fields - set(suite.keys())
    if missing_fields:
        raise ValueError(f"malformed suite {suite_id}: missing fields {missing_fields}")

    return dict(suite)


def _safe_get_enabled_kem_mechanisms() -> Iterable[str]:
    try:
        from oqs.oqs import get_enabled_KEM_mechanisms as kem_loader  # type: ignore[attr-defined]
    except ImportError:
        from oqs.oqs import get_enabled_kem_mechanisms as kem_loader  # type: ignore[attr-defined]
    except AttributeError:
        from oqs.oqs import get_enabled_kem_mechanisms as kem_loader  # type: ignore[attr-defined]

    return kem_loader()


def _safe_get_enabled_sig_mechanisms() -> Iterable[str]:
    try:
        from oqs.oqs import get_enabled_sig_mechanisms as sig_loader  # type: ignore[attr-defined]
    except ImportError:
        from oqs.oqs import get_enabled_sig_mechanisms as sig_loader  # type: ignore[attr-defined]
    except AttributeError:
        from oqs.oqs import get_enabled_sig_mechanisms as sig_loader  # type: ignore[attr-defined]

    return sig_loader()


def enabled_kems() -> Tuple[str, ...]:
    """Return tuple of oqs KEM mechanism names supported by the runtime."""

    mechanisms = {_normalize_alias(name) for name in _safe_get_enabled_kem_mechanisms()}
    result = [
        entry["oqs_name"]
        for entry in _KEM_REGISTRY.values()
        if _normalize_alias(entry["oqs_name"]) in mechanisms
    ]
    return tuple(result)


def enabled_sigs() -> Tuple[str, ...]:
    """Return tuple of oqs signature mechanism names supported by the runtime."""

    mechanisms = {_normalize_alias(name) for name in _safe_get_enabled_sig_mechanisms()}
    result = [
        entry["oqs_name"]
        for entry in _SIG_REGISTRY.values()
        if _normalize_alias(entry["oqs_name"]) in mechanisms
    ]
    return tuple(result)


def _prune_suites_for_runtime() -> None:
    """Filter suites by signature availability, preserving registry immutability."""

    global SUITES
    try:
        available = set(enabled_sigs())
    except Exception:
        return
    if not available:
        return

    filtered: Dict[str, MappingProxyType] = {}
    removed: list[str] = []
    for suite_id, config in SUITES.items():
        if config.get("sig_name") in available:
            filtered[suite_id] = config
        else:
            removed.append(suite_id)

    if not removed:
        return

    _logger.warning(
        "Pruning suites with unsupported signature algorithms",
        extra={"removed_suites": removed},
    )
    from types import MappingProxyType as _MP

    SUITES = _MP(filtered)  # type: ignore[assignment]


def header_ids_for_suite(suite: Dict) -> Tuple[int, int, int, int]:
    """Return embedded header ID bytes for provided suite dict copy."""

    try:
        return (
            suite["kem_id"],
            suite["kem_param_id"],
            suite["sig_id"],
            suite["sig_param_id"],
        )
    except KeyError as e:
        raise ValueError(f"suite missing embedded id field: {e}")


def header_ids_from_names(kem_name: str, sig_name: str) -> Tuple[int, int, int, int]:
    """Return header IDs from algorithm names.
    
    Used by async_proxy.py for runtime header ID resolution when
    kem_name and sig_name are returned from the handshake.
    """
    kem_key = _resolve_kem_key(kem_name)
    sig_key = _resolve_sig_key(sig_name)
    kem_entry = _KEM_REGISTRY[kem_key]
    sig_entry = _SIG_REGISTRY[sig_key]
    return (
        kem_entry["kem_id"],
        kem_entry["kem_param_id"],
        sig_entry["sig_id"],
        sig_entry["sig_param_id"],
    )


def suite_bytes_for_hkdf(suite: Dict) -> bytes:
    """Generate deterministic bytes from suite for HKDF info parameter."""

    if "suite_id" in suite:
        return suite["suite_id"].encode("utf-8")

    try:
        suite_id = build_suite_id(suite["kem_name"], suite["aead"], suite["sig_name"])
    except (KeyError, ValueError) as exc:
        raise ValueError("Suite configuration not found in registry") from exc

    return suite_id.encode("utf-8")


_prune_suites_for_runtime()
==================================================

core\__init__.py
==================================================
"""
PQC Drone-GCS Secure Proxy Core Package.

Provides post-quantum cryptography secure communication components.
"""
==================================================

scheduler\sdrone.py
==================================================
#!/usr/bin/env python3
"""Simplified Drone Scheduler - Runs drone proxy with UDP echo for all suites.

This scheduler:
1. Starts the drone proxy for each suite
2. Runs a UDP echo server to reflect traffic back to GCS
3. Accepts rekey commands from GCS scheduler via TCP control
4. Iterates through all available suites on command

No benchmarking, no power monitoring - just robust PQC tunnel operation.
"""

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# Keep this block near the top for quick edits.
# --------------------
LOCAL_CONTROL_HOST = None  # e.g. "127.0.0.1" or None to use config/env
LOCAL_CONTROL_PORT = None  # e.g. 48080 or None to use config


from __future__ import annotations

import sys
from pathlib import Path


def _ensure_core_importable() -> Path:
    """Guarantee the repository root is on sys.path before importing core."""
    root = Path(__file__).resolve().parents[1]
    root_str = str(root)
    if root_str not in sys.path:
        sys.path.insert(0, root_str)
    try:
        __import__("core")
    except ModuleNotFoundError as exc:
        raise RuntimeError(
            f"Unable to import 'core'; repo root {root} missing from sys.path."
        ) from exc
    return root


ROOT = _ensure_core_importable()

import argparse
import json
import os
import signal
import socket
import struct
import subprocess
import threading
import time
from typing import Dict, IO, List, Optional, Tuple

from core.config import CONFIG
from core import suites as suites_mod
from core.suites import DEFAULT_SUITE_ID, get_suite, list_suites

# ---------------------------------------------------------------------------
# Configuration from core.config (same as drone_follower.py)
# ---------------------------------------------------------------------------

_CONTROL_HOST_FALLBACK = CONFIG.get("DRONE_HOST", "127.0.0.1")
CONTROL_HOST = str(
    CONFIG.get("DRONE_CONTROL_HOST")
    or os.getenv("DRONE_CONTROL_HOST")
    or _CONTROL_HOST_FALLBACK
).strip() or str(_CONTROL_HOST_FALLBACK)
CONTROL_PORT = int(CONFIG.get("DRONE_CONTROL_PORT", 48080))

APP_BIND_HOST = CONFIG.get("DRONE_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
APP_SEND_HOST = CONFIG.get("DRONE_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("DRONE_PLAINTEXT_TX", 47003))

DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

SECRETS_DIR = ROOT / "secrets/matrix"
LOGS_DIR = ROOT / "logs/scheduler/drone"

# ---------------------------------------------------------------------------
# Utility Functions
# ---------------------------------------------------------------------------

def ts() -> str:
    """Return ISO timestamp."""
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def log(msg: str) -> None:
    """Print timestamped log message."""
    print(f"[{ts()}] [sdrone] {msg}", flush=True)


def get_available_suites() -> List[str]:
    """Return list of suite IDs that have keys in secrets/matrix/."""
    available = []
    all_suites = list_suites()
    
    for suite_id in sorted(all_suites.keys()):
        suite_dir = SECRETS_DIR / suite_id
        pub_file = suite_dir / "gcs_signing.pub"
        if pub_file.exists():
            available.append(suite_id)
    
    return available


def popen(cmd: List[str], **kwargs) -> subprocess.Popen:
    """Launch subprocess with proper flags."""
    if sys.platform == "win32":
        kwargs.setdefault("creationflags", subprocess.CREATE_NEW_PROCESS_GROUP)
    return subprocess.Popen(cmd, **kwargs)


def killtree(proc: subprocess.Popen) -> None:
    """Terminate process tree."""
    try:
        if sys.platform == "win32":
            subprocess.run(
                ["taskkill", "/F", "/T", "/PID", str(proc.pid)],
                capture_output=True,
                timeout=5,
            )
        else:
            import os as _os
            _os.killpg(_os.getpgid(proc.pid), signal.SIGKILL)
    except Exception:
        try:
            proc.kill()
        except Exception:
            pass


# ---------------------------------------------------------------------------
# UDP Echo Server
# ---------------------------------------------------------------------------

class UdpEchoServer(threading.Thread):
    """Simple UDP echo server - reflects all received packets back."""
    
    def __init__(
        self,
        bind_host: str,
        recv_port: int,
        send_host: str,
        send_port: int,
        stop_event: threading.Event,
    ):
        super().__init__(daemon=True)
        self.bind_host = bind_host
        self.recv_port = recv_port
        self.send_host = send_host
        self.send_port = send_port
        self.stop_event = stop_event
        self.rx_count = 0
        self.tx_count = 0
        self.rx_bytes = 0
        self.tx_bytes = 0
        self._lock = threading.Lock()
    
    def run(self) -> None:
        rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        try:
            rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        except Exception:
            pass
        rx_sock.bind((self.bind_host, self.recv_port))
        rx_sock.settimeout(0.5)
        
        tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        try:
            tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)
        except Exception:
            pass
        
        log(f"Echo server listening on {self.bind_host}:{self.recv_port}")
        
        while not self.stop_event.is_set():
            try:
                data, addr = rx_sock.recvfrom(65535)
                with self._lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
                
                tx_sock.sendto(data, (self.send_host, self.send_port))
                with self._lock:
                    self.tx_count += 1
                    self.tx_bytes += len(data)
                    
            except socket.timeout:
                continue
            except Exception as e:
                if not self.stop_event.is_set():
                    log(f"Echo error: {e}")
        
        rx_sock.close()
        tx_sock.close()
        log("Echo server stopped")
    
    def get_stats(self) -> Dict[str, int]:
        with self._lock:
            return {
                "rx_count": self.rx_count,
                "tx_count": self.tx_count,
                "rx_bytes": self.rx_bytes,
                "tx_bytes": self.tx_bytes,
            }
    
    def reset_stats(self) -> None:
        with self._lock:
            self.rx_count = 0
            self.tx_count = 0
            self.rx_bytes = 0
            self.tx_bytes = 0


# ---------------------------------------------------------------------------
# TCP Control Server (accepts commands from GCS scheduler)
# ---------------------------------------------------------------------------

class ControlServer(threading.Thread):
    """TCP control server - accepts rekey commands from GCS scheduler."""
    
    def __init__(
        self,
        host: str,
        port: int,
        state: Dict,
        stop_event: threading.Event,
    ):
        super().__init__(daemon=True)
        self.host = host
        self.port = port
        self.state = state
        self.stop_event = stop_event
    
    def run(self) -> None:
        server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        server.settimeout(1.0)
        server.bind((self.host, self.port))
        server.listen(5)
        
        log(f"Control server listening on {self.host}:{self.port}")
        
        while not self.stop_event.is_set():
            try:
                conn, addr = server.accept()
                conn.settimeout(10.0)
                threading.Thread(
                    target=self._handle_client,
                    args=(conn, addr),
                    daemon=True
                ).start()
            except socket.timeout:
                continue
            except Exception as e:
                if not self.stop_event.is_set():
                    log(f"Control server error: {e}")
        
        server.close()
        log("Control server stopped")
    
    def _handle_client(self, conn: socket.socket, addr: Tuple) -> None:
        try:
            data = conn.recv(8192)
            if not data:
                return
            
            try:
                request = json.loads(data.decode("utf-8"))
            except json.JSONDecodeError:
                conn.sendall(json.dumps({"error": "invalid_json"}).encode())
                return
            
            cmd = request.get("cmd", "")
            response = self._handle_command(cmd, request)
            conn.sendall(json.dumps(response).encode())
            
        except Exception as e:
            log(f"Client handler error: {e}")
        finally:
            conn.close()
    
    def _handle_command(self, cmd: str, request: Dict) -> Dict:
        if cmd == "ping":
            return {"status": "ok", "ts": ts()}
        
        elif cmd == "status":
            return {
                "status": "ok",
                "suite": self.state.get("suite"),
                "proxy_running": self.state.get("proxy") is not None and self.state["proxy"].poll() is None,
                "echo_stats": self.state.get("echo").get_stats() if self.state.get("echo") else {},
            }
        
        elif cmd == "start":
            # Start proxy for the first time (GCS already listening)
            suite = request.get("suite") or self.state.get("pending_suite")
            if not suite:
                return {"status": "error", "error": "missing_suite"}
            
            # If proxy already running, stop it first
            old_proxy = self.state.get("proxy")
            if old_proxy and old_proxy.poll() is None:
                log("Stopping current proxy for new start...")
                try:
                    old_proxy.terminate()
                    old_proxy.wait(timeout=5)
                except Exception:
                    killtree(old_proxy)
                
                old_log = self.state.get("log_handle")
                if old_log:
                    try:
                        old_log.close()
                    except Exception:
                        pass
                
                self.state["proxy"] = None
                self.state["log_handle"] = None
                self.state["suite"] = None
                time.sleep(1)
            
            try:
                result = self._do_start(suite)
                return result
            except Exception as e:
                return {"status": "error", "error": str(e)}
        
        elif cmd == "prepare_rekey":
            # Stop current proxy in preparation for rekey (GCS will start first)
            log("Prepare rekey: stopping proxy...")
            old_proxy = self.state.get("proxy")
            if old_proxy and old_proxy.poll() is None:
                try:
                    old_proxy.terminate()
                    old_proxy.wait(timeout=5)
                except Exception:
                    killtree(old_proxy)
            
            old_log = self.state.get("log_handle")
            if old_log:
                try:
                    old_log.close()
                except Exception:
                    pass
            
            self.state["proxy"] = None
            self.state["log_handle"] = None
            self.state["suite"] = None
            
            # Reset echo stats
            echo = self.state.get("echo")
            if echo:
                echo.reset_stats()
            
            return {"status": "ok", "message": "ready_for_rekey"}
        
        elif cmd == "rekey":
            new_suite = request.get("suite")
            if not new_suite:
                return {"status": "error", "error": "missing_suite"}
            
            try:
                result = self._do_rekey(new_suite)
                return result
            except Exception as e:
                return {"status": "error", "error": str(e)}
        
        elif cmd == "stop":
            self.stop_event.set()
            return {"status": "ok", "message": "stopping"}
        
        elif cmd == "get_suites":
            return {"status": "ok", "suites": get_available_suites()}
        
        else:
            return {"status": "error", "error": f"unknown_cmd: {cmd}"}
    
    def _do_start(self, suite: str) -> Dict:
        """Start proxy for the first time (GCS must already be listening)."""
        log(f"Start requested for suite: {suite}")
        
        # Validate suite exists
        suite_dir = SECRETS_DIR / suite
        pub_file = suite_dir / "gcs_signing.pub"
        if not pub_file.exists():
            return {"status": "error", "error": f"no_keys_for_suite: {suite}"}
        
        # Check if already running
        old_proxy = self.state.get("proxy")
        if old_proxy and old_proxy.poll() is None:
            return {"status": "error", "error": "proxy_already_running"}
        
        # Reset echo stats
        echo = self.state.get("echo")
        if echo:
            echo.reset_stats()
        
        # Start new proxy
        log(f"Starting proxy with suite {suite}")
        try:
            proc, log_handle = start_drone_proxy(suite)
            self.state["proxy"] = proc
            self.state["log_handle"] = log_handle
            self.state["suite"] = suite
            
            # Wait for handshake
            time.sleep(3)
            
            if proc.poll() is not None:
                return {"status": "error", "error": "proxy_exited_early"}
            
            return {"status": "ok", "suite": suite}
            
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def _do_rekey(self, new_suite: str) -> Dict:
        """Stop current proxy, start new one with different suite."""
        log(f"Rekey requested: {self.state.get('suite')} -> {new_suite}")
        
        # Validate suite exists
        suite_dir = SECRETS_DIR / new_suite
        pub_file = suite_dir / "gcs_signing.pub"
        if not pub_file.exists():
            return {"status": "error", "error": f"no_keys_for_suite: {new_suite}"}
        
        # Stop current proxy
        old_proxy = self.state.get("proxy")
        if old_proxy and old_proxy.poll() is None:
            log("Stopping current proxy...")
            try:
                old_proxy.terminate()
                old_proxy.wait(timeout=5)
            except Exception:
                killtree(old_proxy)
        
        # Close old log handle
        old_log = self.state.get("log_handle")
        if old_log:
            try:
                old_log.close()
            except Exception:
                pass
        
        # Wait for ports to clear
        time.sleep(1)
        
        # Reset echo stats
        echo = self.state.get("echo")
        if echo:
            echo.reset_stats()
        
        # Start new proxy
        log(f"Starting proxy with suite {new_suite}")
        try:
            proc, log_handle = start_drone_proxy(new_suite)
            self.state["proxy"] = proc
            self.state["log_handle"] = log_handle
            self.state["suite"] = new_suite
            
            # Wait for handshake
            time.sleep(3)
            
            if proc.poll() is not None:
                return {"status": "error", "error": "proxy_exited_early"}
            
            return {"status": "ok", "suite": new_suite}
            
        except Exception as e:
            return {"status": "error", "error": str(e)}


# ---------------------------------------------------------------------------
# Proxy Management
# ---------------------------------------------------------------------------

def start_drone_proxy(suite: str) -> Tuple[subprocess.Popen, IO[str]]:
    """Start drone proxy for the given suite."""
    suite_dir = SECRETS_DIR / suite
    pub_file = suite_dir / "gcs_signing.pub"
    
    if not pub_file.exists():
        raise FileNotFoundError(f"Public key not found: {pub_file}")
    
    # Set environment
    env = os.environ.copy()
    env["DRONE_HOST"] = DRONE_HOST
    env["GCS_HOST"] = GCS_HOST
    env["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    env["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"
    
    root_str = str(ROOT)
    existing_py_path = env.get("PYTHONPATH")
    if existing_py_path:
        if root_str not in existing_py_path.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing_py_path
    else:
        env["PYTHONPATH"] = root_str
    
    # Create log directory
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    log_path = LOGS_DIR / f"drone_{suite}_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_handle = open(log_path, "w", encoding="utf-8")
    
    cmd = [
        sys.executable,
        "-m", "core.run_proxy",
        "drone",
        "--suite", suite,
        "--peer-pubkey-file", str(pub_file),
        "--quiet",
    ]
    
    log(f"Launching: {' '.join(cmd)}")
    
    proc = popen(
        cmd,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        text=True,
        env=env,
        cwd=str(ROOT),
    )
    
    return proc, log_handle


# ---------------------------------------------------------------------------
# Main Entry Point
# ---------------------------------------------------------------------------

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Simplified Drone Scheduler - Run PQC tunnel with all suites"
    )
    parser.add_argument(
        "--suite",
        default=None,
        help="Initial suite (default: first available)",
    )
    parser.add_argument(
        "--control-host",
        default=CONTROL_HOST,
        help=f"Control server bind host (default: {CONTROL_HOST})",
    )
    parser.add_argument(
        "--control-port",
        type=int,
        default=CONTROL_PORT,
        help=f"Control server port (default: {CONTROL_PORT})",
    )
    return parser.parse_args()


def main() -> int:
    args = parse_args()
    
    print("=" * 60)
    print("Simplified Drone Scheduler (sdrone)")
    print("=" * 60)
    log(f"DRONE_HOST={DRONE_HOST}, GCS_HOST={GCS_HOST}")
    log(f"Control: {args.control_host}:{args.control_port}")
    
    # Get available suites
    available_suites = get_available_suites()
    if not available_suites:
        log("ERROR: No suites with keys found in secrets/matrix/")
        return 1
    
    log(f"Available suites: {len(available_suites)}")
    
    # Determine initial suite
    initial_suite = args.suite
    if initial_suite and initial_suite not in available_suites:
        log(f"WARNING: Suite {initial_suite} not available, using first available")
        initial_suite = None
    if not initial_suite:
        initial_suite = available_suites[0]
    
    log(f"Initial suite: {initial_suite}")
    
    # Setup
    stop_event = threading.Event()
    
    def signal_handler(signum, frame):
        log("Received shutdown signal")
        stop_event.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Start echo server
    echo = UdpEchoServer(
        APP_BIND_HOST, APP_RECV_PORT,
        APP_SEND_HOST, APP_SEND_PORT,
        stop_event
    )
    echo.start()
    
    # State dict shared with control server
    # NOTE: Proxy is started on first rekey command from GCS (GCS must start first)
    state = {
        "proxy": None,
        "log_handle": None,
        "suite": None,
        "pending_suite": initial_suite,  # Will start this on first 'start' or 'rekey' command
        "echo": echo,
        "stop_event": stop_event,
    }

    # If LOCAL control overrides are set, update the control server bind values
    if LOCAL_CONTROL_HOST:
        args.control_host = LOCAL_CONTROL_HOST
    if LOCAL_CONTROL_PORT:
        args.control_port = int(LOCAL_CONTROL_PORT)
    
    # Start control server
    control = ControlServer(
        args.control_host,
        args.control_port,
        state,
        stop_event
    )
    control.start()
    
    # Main loop
    log("Drone scheduler running. Waiting for 'start' command from GCS...")
    log("(GCS must start first, then send 'start' or 'rekey' command)")
    
    proxy = None
    try:
        while not stop_event.is_set():
            # Check if proxy is running
            proxy = state.get("proxy")
            if proxy and proxy.poll() is not None:
                log(f"Proxy exited with code {proxy.returncode}")
                state["proxy"] = None
                state["suite"] = None
            
            # Print periodic status
            stats = echo.get_stats()
            if stats["rx_count"] > 0:
                log(f"Echo stats: RX={stats['rx_count']}, TX={stats['tx_count']}, "
                    f"RX_bytes={stats['rx_bytes']:,}, TX_bytes={stats['tx_bytes']:,}")
            
            time.sleep(5)
            
    except KeyboardInterrupt:
        log("Interrupted")
        stop_event.set()
    
    # Cleanup
    log("Shutting down...")
    
    proxy = state.get("proxy")
    if proxy and proxy.poll() is None:
        try:
            proxy.terminate()
            proxy.wait(timeout=5)
        except Exception:
            killtree(proxy)
    
    log_handle = state.get("log_handle")
    if log_handle:
        try:
            log_handle.close()
        except Exception:
            pass
    
    log("Drone scheduler stopped")
    return 0


if __name__ == "__main__":
    sys.exit(main())

==================================================

scheduler\sgcs.py
==================================================
#!/usr/bin/env python3
"""Simplified GCS Scheduler - Runs all PQC suites with high-throughput traffic.

This scheduler:
1. Starts GCS proxy for each suite sequentially
2. Generates 110 Mbps UDP traffic for 10 seconds per suite
3. Sends rekey commands to drone scheduler for suite transitions
4. Iterates through all available suites automatically

No benchmarking, no power monitoring - just robust PQC tunnel operation.
"""

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# Keep this block within the first ~10 lines of the file for easy editing.
# --------------------
# Set to None to use defaults or auto-detection.
LOCAL_BANDWIDTH_MBPS = 110.0
LOCAL_DURATION_S = 10.0
LOCAL_PAYLOAD_BYTES = 1200
LOCAL_SUITES = None  # e.g. ['cs-mlkem768-aesgcm-mldsa65'] or None to run all
LOCAL_MAX_SUITES = None  # e.g. 2 to limit number of suites


from __future__ import annotations

import sys
from pathlib import Path


def _ensure_core_importable() -> Path:
    """Guarantee the repository root is on sys.path before importing core."""
    root = Path(__file__).resolve().parents[1]
    root_str = str(root)
    if root_str not in sys.path:
        sys.path.insert(0, root_str)
    try:
        __import__("core")
    except ModuleNotFoundError as exc:
        raise RuntimeError(
            f"Unable to import 'core'; repo root {root} missing from sys.path."
        ) from exc
    return root


ROOT = _ensure_core_importable()

import argparse
import json
import os
import signal
import socket
import struct
import subprocess
import threading
import time
from dataclasses import dataclass
from typing import Dict, IO, List, Optional, Tuple

from core.config import CONFIG
from core import suites as suites_mod
from core.suites import DEFAULT_SUITE_ID, get_suite, list_suites

# ---------------------------------------------------------------------------
# Configuration from core.config (same as gcs_scheduler.py)
# ---------------------------------------------------------------------------

DRONE_HOST = CONFIG["DRONE_HOST"]
GCS_HOST = CONFIG["GCS_HOST"]

CONTROL_PORT = int(CONFIG.get("DRONE_CONTROL_PORT", 48080))

APP_SEND_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_SEND_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))
APP_RECV_HOST = CONFIG.get("GCS_PLAINTEXT_HOST", "127.0.0.1")
APP_RECV_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))

SECRETS_DIR = ROOT / "secrets/matrix"
LOGS_DIR = ROOT / "logs/scheduler/gcs"

# Traffic defaults
DEFAULT_BANDWIDTH_MBPS = 110.0
DEFAULT_DURATION_S = 10.0
DEFAULT_PAYLOAD_BYTES = 1200  # Near MTU for efficiency

# ---------------------------------------------------------------------------
# Utility Functions
# ---------------------------------------------------------------------------

def ts() -> str:
    """Return ISO timestamp."""
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())


def log(msg: str) -> None:
    """Print timestamped log message."""
    print(f"[{ts()}] [sgcs] {msg}", flush=True)


def get_available_suites() -> List[str]:
    """Return list of suite IDs that have keys in secrets/matrix/."""
    available = []
    all_suites = list_suites()
    
    for suite_id in sorted(all_suites.keys()):
        suite_dir = SECRETS_DIR / suite_id
        key_file = suite_dir / "gcs_signing.key"
        if key_file.exists():
            available.append(suite_id)
    
    return available


def popen(cmd: List[str], **kwargs) -> subprocess.Popen:
    """Launch subprocess with proper flags."""
    if sys.platform == "win32":
        kwargs.setdefault("creationflags", subprocess.CREATE_NEW_PROCESS_GROUP)
    return subprocess.Popen(cmd, **kwargs)


def killtree(proc: subprocess.Popen) -> None:
    """Terminate process tree."""
    try:
        if sys.platform == "win32":
            subprocess.run(
                ["taskkill", "/F", "/T", "/PID", str(proc.pid)],
                capture_output=True,
                timeout=5,
            )
        else:
            import os as _os
            _os.killpg(_os.getpgid(proc.pid), signal.SIGKILL)
    except Exception:
        try:
            proc.kill()
        except Exception:
            pass


# ---------------------------------------------------------------------------
# Drone Control Client
# ---------------------------------------------------------------------------

def send_control_command(host: str, port: int, cmd: Dict, timeout: float = 30.0) -> Dict:
    """Send command to drone control server and return response."""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(timeout)
    try:
        sock.connect((host, port))
        sock.sendall(json.dumps(cmd).encode("utf-8"))
        response = sock.recv(65535)
        return json.loads(response.decode("utf-8"))
    finally:
        sock.close()


def ping_drone(host: str, port: int) -> bool:
    """Check if drone control server is responding."""
    try:
        response = send_control_command(host, port, {"cmd": "ping"}, timeout=5.0)
        return response.get("status") == "ok"
    except Exception:
        return False


def start_drone_proxy(host: str, port: int, suite: str, timeout: float = 45.0) -> Dict:
    """Send start command to drone to initiate its proxy."""
    return send_control_command(host, port, {"cmd": "start", "suite": suite}, timeout=timeout)


def rekey_drone(host: str, port: int, suite: str, timeout: float = 45.0) -> Dict:
    """Send rekey command to drone."""
    return send_control_command(host, port, {"cmd": "rekey", "suite": suite}, timeout=timeout)


def get_drone_status(host: str, port: int) -> Dict:
    """Get current status from drone."""
    return send_control_command(host, port, {"cmd": "status"}, timeout=10.0)


def stop_drone(host: str, port: int) -> Dict:
    """Send stop command to drone."""
    return send_control_command(host, port, {"cmd": "stop"}, timeout=5.0)


# ---------------------------------------------------------------------------
# Traffic Generator
# ---------------------------------------------------------------------------

@dataclass
class TrafficStats:
    """Traffic statistics."""
    tx_packets: int = 0
    rx_packets: int = 0
    tx_bytes: int = 0
    rx_bytes: int = 0
    start_time: float = 0.0
    end_time: float = 0.0
    
    @property
    def duration_s(self) -> float:
        return self.end_time - self.start_time if self.end_time > self.start_time else 0.0
    
    @property
    def delivery_ratio(self) -> float:
        return self.rx_packets / self.tx_packets if self.tx_packets > 0 else 0.0
    
    @property
    def tx_mbps(self) -> float:
        return (self.tx_bytes * 8 / 1_000_000) / self.duration_s if self.duration_s > 0 else 0.0
    
    @property
    def rx_mbps(self) -> float:
        return (self.rx_bytes * 8 / 1_000_000) / self.duration_s if self.duration_s > 0 else 0.0


class TrafficGenerator:
    """High-throughput UDP traffic generator with receiver."""
    
    def __init__(
        self,
        tx_host: str,
        tx_port: int,
        rx_host: str,
        rx_port: int,
        payload_bytes: int = 1200,
        target_mbps: float = 110.0,
    ):
        self.tx_host = tx_host
        self.tx_port = tx_port
        self.rx_host = rx_host
        self.rx_port = rx_port
        self.payload_bytes = payload_bytes
        self.target_mbps = target_mbps
        
        # Calculate packets per second for target bandwidth
        # bandwidth = pps * packet_size * 8
        # pps = bandwidth / (packet_size * 8)
        self.target_pps = int((target_mbps * 1_000_000) / (payload_bytes * 8))
        
        self._stop_event = threading.Event()
        self._stats = TrafficStats()
        self._stats_lock = threading.Lock()
    
    def run(self, duration_s: float) -> TrafficStats:
        """Run traffic for specified duration and return stats."""
        self._stop_event.clear()
        self._stats = TrafficStats()
        
        # Start receiver thread
        rx_thread = threading.Thread(target=self._receiver_loop, daemon=True)
        rx_thread.start()
        
        # Small delay for receiver to bind
        time.sleep(0.1)
        
        # Run transmitter
        self._transmitter_loop(duration_s)
        
        # Wait a bit for final packets
        time.sleep(0.5)
        self._stop_event.set()
        rx_thread.join(timeout=2.0)
        
        with self._stats_lock:
            return TrafficStats(
                tx_packets=self._stats.tx_packets,
                rx_packets=self._stats.rx_packets,
                tx_bytes=self._stats.tx_bytes,
                rx_bytes=self._stats.rx_bytes,
                start_time=self._stats.start_time,
                end_time=self._stats.end_time,
            )
    
    def _transmitter_loop(self, duration_s: float) -> None:
        """Send packets at target rate."""
        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        try:
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)
        except Exception:
            pass
        
        # Pre-generate payload template
        payload_template = bytearray(self.payload_bytes)
        
        interval = 1.0 / self.target_pps if self.target_pps > 0 else 0.001
        batch_size = max(1, self.target_pps // 100)  # Send in batches for efficiency
        batch_interval = interval * batch_size
        
        start_time = time.perf_counter()
        with self._stats_lock:
            self._stats.start_time = time.time()
        
        seq = 0
        next_batch_time = start_time
        
        while True:
            now = time.perf_counter()
            elapsed = now - start_time
            
            if elapsed >= duration_s:
                break
            
            # Send batch
            for _ in range(batch_size):
                # Embed sequence number in payload
                struct.pack_into(">I", payload_template, 0, seq)
                struct.pack_into(">d", payload_template, 4, time.time())
                
                try:
                    sock.sendto(bytes(payload_template), (self.tx_host, self.tx_port))
                    with self._stats_lock:
                        self._stats.tx_packets += 1
                        self._stats.tx_bytes += len(payload_template)
                except Exception:
                    pass
                
                seq += 1
            
            # Rate limiting
            next_batch_time += batch_interval
            sleep_time = next_batch_time - time.perf_counter()
            if sleep_time > 0:
                time.sleep(sleep_time)
        
        with self._stats_lock:
            self._stats.end_time = time.time()
        
        sock.close()
    
    def _receiver_loop(self) -> None:
        """Receive and count packets."""
        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        try:
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        except Exception:
            pass
        sock.bind((self.rx_host, self.rx_port))
        sock.settimeout(0.1)
        
        while not self._stop_event.is_set():
            try:
                data, addr = sock.recvfrom(65535)
                with self._stats_lock:
                    self._stats.rx_packets += 1
                    self._stats.rx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception:
                if not self._stop_event.is_set():
                    pass
        
        sock.close()


# ---------------------------------------------------------------------------
# Proxy Management
# ---------------------------------------------------------------------------

def start_gcs_proxy(suite: str) -> Tuple[subprocess.Popen, IO[str]]:
    """Start GCS proxy for the given suite."""
    suite_dir = SECRETS_DIR / suite
    key_file = suite_dir / "gcs_signing.key"
    
    if not key_file.exists():
        raise FileNotFoundError(f"Secret key not found: {key_file}")
    
    # Set environment
    env = os.environ.copy()
    env["DRONE_HOST"] = DRONE_HOST
    env["GCS_HOST"] = GCS_HOST
    env["ENABLE_PACKET_TYPE"] = "1" if CONFIG.get("ENABLE_PACKET_TYPE", True) else "0"
    env["STRICT_UDP_PEER_MATCH"] = "1" if CONFIG.get("STRICT_UDP_PEER_MATCH", True) else "0"
    
    root_str = str(ROOT)
    existing_py_path = env.get("PYTHONPATH")
    if existing_py_path:
        if root_str not in existing_py_path.split(os.pathsep):
            env["PYTHONPATH"] = root_str + os.pathsep + existing_py_path
    else:
        env["PYTHONPATH"] = root_str
    
    # Create log directory
    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    log_path = LOGS_DIR / f"gcs_{suite}_{time.strftime('%Y%m%d-%H%M%S')}.log"
    log_handle = open(log_path, "w", encoding="utf-8")
    
    cmd = [
        sys.executable,
        "-m", "core.run_proxy",
        "gcs",
        "--suite", suite,
        "--gcs-secret-file", str(key_file),
        "--quiet",
    ]
    
    log(f"Launching: {' '.join(cmd)}")
    
    proc = popen(
        cmd,
        stdout=log_handle,
        stderr=subprocess.STDOUT,
        text=True,
        env=env,
        cwd=str(ROOT),
    )
    
    return proc, log_handle


# ---------------------------------------------------------------------------
# Suite Runner
# ---------------------------------------------------------------------------

@dataclass
class SuiteResult:
    """Result from running a single suite."""
    suite_id: str
    success: bool
    stats: Optional[TrafficStats]
    error: Optional[str]
    handshake_time_s: float
    traffic_time_s: float


def run_suite(
    suite_id: str,
    drone_host: str,
    drone_port: int,
    bandwidth_mbps: float,
    duration_s: float,
    payload_bytes: int,
    is_first: bool = False,
) -> SuiteResult:
    """Run a single suite: start proxy, handshake, run traffic."""
    log(f"{'='*60}")
    log(f"Running suite: {suite_id}")
    log(f"{'='*60}")
    
    gcs_proc = None
    gcs_log = None
    handshake_time = 0.0
    traffic_time = 0.0
    
    try:
        # For first suite: start GCS proxy first, then tell drone to start
        # For subsequent suites: stop current, rekey drone, then start new GCS
        
        if is_first:
            # Start GCS proxy first (it listens for handshake)
            log(f"Starting GCS proxy (first suite)...")
            proxy_start = time.time()
            gcs_proc, gcs_log = start_gcs_proxy(suite_id)
            
            # Wait for GCS to be ready
            time.sleep(2)
            
            if gcs_proc.poll() is not None:
                return SuiteResult(suite_id, False, None, f"GCS proxy exited with {gcs_proc.returncode}", 0, 0)
            
            # Now tell drone to start its proxy
            log(f"Sending start command to drone...")
            try:
                response = start_drone_proxy(drone_host, drone_port, suite_id)
                if response.get("status") != "ok":
                    error = response.get("error", "unknown")
                    return SuiteResult(suite_id, False, None, f"Drone start failed: {error}", 0, 0)
            except Exception as e:
                return SuiteResult(suite_id, False, None, f"Drone start error: {e}", 0, 0)
            
            log(f"Drone started")
        else:
            # Rekey sequence:
            # 1. Tell drone to prepare for rekey (stop its proxy)
            # 2. Start new GCS proxy
            # 3. Tell drone to start with new suite
            
            log(f"Preparing for rekey...")
            
            # Tell drone to stop its proxy first
            try:
                response = send_control_command(drone_host, drone_port, {"cmd": "prepare_rekey"}, timeout=15.0)
                # prepare_rekey may not exist, fall back to regular rekey behavior
            except Exception:
                pass
            
            # Start GCS proxy first
            log(f"Starting GCS proxy...")
            proxy_start = time.time()
            gcs_proc, gcs_log = start_gcs_proxy(suite_id)
            
            # Wait for GCS to be ready
            time.sleep(2)
            
            if gcs_proc.poll() is not None:
                return SuiteResult(suite_id, False, None, f"GCS proxy exited with {gcs_proc.returncode}", 0, 0)
            
            # Now tell drone to start with new suite
            log(f"Sending start command to drone for rekey...")
            try:
                response = start_drone_proxy(drone_host, drone_port, suite_id)
                if response.get("status") != "ok":
                    error = response.get("error", "unknown")
                    # If already running, try rekey instead
                    if "already_running" in str(error):
                        response = rekey_drone(drone_host, drone_port, suite_id)
                        if response.get("status") != "ok":
                            return SuiteResult(suite_id, False, None, f"Drone rekey failed: {response.get('error', 'unknown')}", 0, 0)
                    else:
                        return SuiteResult(suite_id, False, None, f"Drone start failed: {error}", 0, 0)
            except Exception as e:
                return SuiteResult(suite_id, False, None, f"Drone start error: {e}", 0, 0)
            
            log(f"Drone started for rekey")
        
        # Wait for handshake
        log(f"Waiting for handshake...")
        handshake_timeout = 30.0
        handshake_start = time.time()
        
        while time.time() - handshake_start < handshake_timeout:
            if gcs_proc.poll() is not None:
                return SuiteResult(suite_id, False, None, f"GCS proxy exited with {gcs_proc.returncode}", 0, 0)
            
            # Check if drone has the same suite
            try:
                status = get_drone_status(drone_host, drone_port)
                if status.get("suite") == suite_id and status.get("proxy_running"):
                    break
            except Exception:
                pass
            
            time.sleep(0.5)
        else:
            return SuiteResult(suite_id, False, None, "Handshake timeout", 0, 0)
        
        handshake_time = time.time() - handshake_start
        log(f"Handshake complete in {handshake_time:.2f}s")
        
        # Small settle time
        time.sleep(1.0)
        
        # Run traffic
        log(f"Starting traffic: {bandwidth_mbps:.1f} Mbps for {duration_s:.1f}s")
        traffic_start = time.time()
        
        generator = TrafficGenerator(
            APP_SEND_HOST, APP_SEND_PORT,
            APP_RECV_HOST, APP_RECV_PORT,
            payload_bytes=payload_bytes,
            target_mbps=bandwidth_mbps,
        )
        
        stats = generator.run(duration_s)
        traffic_time = time.time() - traffic_start
        
        log(f"Traffic complete:")
        log(f"  TX: {stats.tx_packets:,} packets, {stats.tx_bytes:,} bytes, {stats.tx_mbps:.1f} Mbps")
        log(f"  RX: {stats.rx_packets:,} packets, {stats.rx_bytes:,} bytes, {stats.rx_mbps:.1f} Mbps")
        log(f"  Delivery: {stats.delivery_ratio*100:.1f}%")
        
        # Stop GCS proxy
        if gcs_proc and gcs_proc.poll() is None:
            gcs_proc.terminate()
            try:
                gcs_proc.wait(timeout=5)
            except Exception:
                killtree(gcs_proc)
        
        if gcs_log:
            try:
                gcs_log.close()
            except Exception:
                pass
        
        success = stats.delivery_ratio > 0.5  # At least 50% delivery
        
        return SuiteResult(suite_id, success, stats, None, handshake_time, traffic_time)
        
    except Exception as e:
        log(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
        
        # Cleanup
        if gcs_proc and gcs_proc.poll() is None:
            try:
                gcs_proc.terminate()
                gcs_proc.wait(timeout=3)
            except Exception:
                killtree(gcs_proc)
        
        if gcs_log:
            try:
                gcs_log.close()
            except Exception:
                pass
        
        return SuiteResult(suite_id, False, None, str(e), handshake_time, traffic_time)


# ---------------------------------------------------------------------------
# Main Entry Point
# ---------------------------------------------------------------------------

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Simplified GCS Scheduler - Run all PQC suites with traffic"
    )
    parser.add_argument(
        "--drone-host",
        default=DRONE_HOST,
        help=f"Drone control host (default: {DRONE_HOST})",
    )
    parser.add_argument(
        "--drone-port",
        type=int,
        default=CONTROL_PORT,
        help=f"Drone control port (default: {CONTROL_PORT})",
    )
    parser.add_argument(
        "--bandwidth",
        type=float,
        default=DEFAULT_BANDWIDTH_MBPS,
        help=f"Target bandwidth in Mbps (default: {DEFAULT_BANDWIDTH_MBPS})",
    )
    parser.add_argument(
        "--duration",
        type=float,
        default=DEFAULT_DURATION_S,
        help=f"Duration per suite in seconds (default: {DEFAULT_DURATION_S})",
    )
    parser.add_argument(
        "--payload",
        type=int,
        default=DEFAULT_PAYLOAD_BYTES,
        help=f"UDP payload size in bytes (default: {DEFAULT_PAYLOAD_BYTES})",
    )
    parser.add_argument(
        "--suite",
        default=None,
        help="Run only this suite (default: run all)",
    )
    parser.add_argument(
        "--suites",
        default=None,
        help="Comma-separated list of suites to run",
    )
    parser.add_argument(
        "--nist-level",
        default=None,
        help="Filter suites by NIST level (L1, L3, L5)",
    )
    parser.add_argument(
        "--inter-gap",
        type=float,
        default=2.0,
        help="Gap between suites in seconds (default: 2.0)",
    )
    return parser.parse_args()


def main() -> int:
    args = parse_args()
    
    print("=" * 60)
    print("Simplified GCS Scheduler (sgcs)")
    print("=" * 60)
    log(f"DRONE_HOST={DRONE_HOST}, GCS_HOST={GCS_HOST}")
    log(f"Drone control: {args.drone_host}:{args.drone_port}")
    log(f"Traffic: {args.bandwidth:.1f} Mbps, {args.duration:.1f}s per suite, {args.payload} byte payload")
    
    # Get available suites
    available_suites = get_available_suites()
    if not available_suites:
        log("ERROR: No suites with keys found in secrets/matrix/")
        return 1

    # Apply local in-file configuration overrides (if set)
    if LOCAL_BANDWIDTH_MBPS is not None:
        args.bandwidth = float(LOCAL_BANDWIDTH_MBPS)
    if LOCAL_DURATION_S is not None:
        args.duration = float(LOCAL_DURATION_S)
    if LOCAL_PAYLOAD_BYTES is not None:
        args.payload = int(LOCAL_PAYLOAD_BYTES)

    # If LOCAL_SUITES provided, use it. Otherwise, optionally cap with LOCAL_MAX_SUITES
    if LOCAL_SUITES:
        suites_to_run = [s for s in LOCAL_SUITES if s in available_suites]
        if not suites_to_run:
            log("ERROR: None of the LOCAL_SUITES are available")
            return 1
    else:
        suites_to_run = available_suites
        if LOCAL_MAX_SUITES:
            suites_to_run = suites_to_run[: int(LOCAL_MAX_SUITES)]
    
    log(f"Available suites: {len(available_suites)}")
    
    # NIST-level filtering (CLI still supported)
    if args.nist_level:
        level = args.nist_level.upper()
        if not level.startswith("L"):
            level = f"L{level}"

        filtered = []
        all_suites_info = list_suites()
        for suite_id in suites_to_run:
            suite_info = all_suites_info.get(suite_id, {})
            if suite_info.get("nist_level") == level:
                filtered.append(suite_id)

        if not filtered:
            log(f"ERROR: No suites match NIST level {level}")
            return 1
        suites_to_run = filtered
    
    log(f"Suites to run: {len(suites_to_run)}")
    
    # Wait for drone to be ready
    log("Waiting for drone scheduler...")
    max_wait = 60
    start_wait = time.time()
    
    while time.time() - start_wait < max_wait:
        if ping_drone(args.drone_host, args.drone_port):
            log("Drone scheduler is ready")
            break
        time.sleep(1)
    else:
        log("ERROR: Drone scheduler not responding")
        return 1
    
    # Run suites
    results: List[SuiteResult] = []
    
    for idx, suite_id in enumerate(suites_to_run):
        is_first = (idx == 0)
        
        result = run_suite(
            suite_id,
            args.drone_host,
            args.drone_port,
            args.bandwidth,
            args.duration,
            args.payload,
            is_first=is_first,
        )
        results.append(result)
        
        # Inter-suite gap
        if idx < len(suites_to_run) - 1:
            log(f"Waiting {args.inter_gap:.1f}s before next suite...")
            time.sleep(args.inter_gap)
    
    # Print summary
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)
    
    passed = 0
    failed = 0
    
    for result in results:
        status = "PASS" if result.success else "FAIL"
        if result.success:
            passed += 1
            if result.stats:
                print(f"  [{status}] {result.suite_id}: {result.stats.delivery_ratio*100:.1f}% delivery, "
                      f"{result.stats.tx_mbps:.1f}/{result.stats.rx_mbps:.1f} Mbps TX/RX")
            else:
                print(f"  [{status}] {result.suite_id}")
        else:
            failed += 1
            print(f"  [{status}] {result.suite_id}: {result.error}")
    
    print("-" * 60)
    print(f"Total: {len(results)}, Passed: {passed}, Failed: {failed}")
    print("=" * 60)
    
    # Stop drone
    log("Sending stop command to drone...")
    try:
        stop_drone(args.drone_host, args.drone_port)
    except Exception:
        pass
    
    log("GCS scheduler complete")
    return 0 if failed == 0 else 1


if __name__ == "__main__":
    sys.exit(main())

==================================================

scheduler\__init__.py
==================================================
# Simplified PQC Scheduler Package

==================================================

scripts\regenerate_matrix_keys.py
==================================================
import sys
import os
from pathlib import Path
import subprocess

# Add project root to path
sys.path.insert(0, os.getcwd())

from core.suites import SUITES

def main():
    matrix_dir = Path("secrets/matrix")
    matrix_dir.mkdir(parents=True, exist_ok=True)

    print(f"Regenerating keys for {len(SUITES)} suites...")

    for suite_id, suite in SUITES.items():
        print(f"Processing {suite_id}...")
        suite_dir = matrix_dir / suite_id
        suite_dir.mkdir(exist_ok=True)
        
        # Run init-identity
        cmd = [
            sys.executable, "-m", "core.run_proxy", "init-identity",
            "--suite", suite_id,
            "--output-dir", str(suite_dir)
        ]
        
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            print(f"  [OK] {suite_id}")
        except subprocess.CalledProcessError as e:
            print(f"  [FAIL] {suite_id}")
            print(e.stderr.decode())

if __name__ == "__main__":
    main()

==================================================

scripts\run_gcs_metrics.py
==================================================
#!/usr/bin/env python3
"""
Minimal runner for GCS Metrics Collector.
Usage: python scripts/run_gcs_metrics.py
"""

import sys
import time
import logging
from pathlib import Path

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from sscheduler.gcs_metrics import GcsMetricsCollector

def main():
    logging.basicConfig(level=logging.INFO)
    
    # Use the same port as configured in sgcs.py
    SNIFF_PORT = 14552
    
    print(f"Starting GCS Metrics Collector on port {SNIFF_PORT}...")
    print("Note: This script only collects metrics if MAVLink traffic is present on this port.")
    print("      (e.g. if sscheduler.sgcs is running and MAVProxy is forwarding to it)")
    
    collector = GcsMetricsCollector(
        mavlink_host="127.0.0.1",
        mavlink_port=SNIFF_PORT
    )
    
    try:
        collector.start()
        
        # Run for 30 seconds
        for i in range(30):
            time.sleep(1.0)
            if i % 5 == 0:
                print(f"Running... {30-i}s remaining")
                
    except KeyboardInterrupt:
        print("Interrupted")
    finally:
        print("Stopping collector...")
        collector.stop()
        print("Done.")

if __name__ == "__main__":
    main()

==================================================

scripts\run_gcs_telemetry_v1.py
==================================================
#!/usr/bin/env python3
"""
Validation script for GCS Telemetry v1.
Starts the collector and a synthetic traffic generator.
"""

import sys
import time
import json
import socket
import threading
import logging
from pathlib import Path

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from sscheduler.gcs_metrics import GcsMetricsCollector

SNIFF_PORT = 14552

def traffic_generator(running_event):
    """Generates synthetic UDP traffic to sniff port"""
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    seq = 0
    print(f"Traffic generator started, targeting 127.0.0.1:{SNIFF_PORT}")
    
    while running_event.is_set():
        # Simulate 10Hz traffic
        msg = f"synthetic_packet_{seq}".encode()
        try:
            sock.sendto(msg, ("127.0.0.1", SNIFF_PORT))
        except Exception:
            pass
        seq += 1
        time.sleep(0.1)

def main():
    logging.basicConfig(level=logging.INFO)
    
    print("Initializing GCS Metrics Collector v1...")
    collector = GcsMetricsCollector(
        mavlink_host="127.0.0.1",
        mavlink_port=SNIFF_PORT
    )
    
    running_event = threading.Event()
    running_event.set()
    
    # Start traffic generator
    gen_thread = threading.Thread(target=traffic_generator, args=(running_event,), daemon=True)
    gen_thread.start()
    
    try:
        collector.start()
        print("Collector started. Running for 10 seconds...")
        
        for i in range(10):
            time.sleep(1.0)
            snapshot = collector.get_snapshot()
            
            # Print key metrics
            sniff = snapshot['metrics']['sniff']
            sys_stats = snapshot['metrics']['sys']
            print(f"[{i+1}s] PPS={sniff['rx_pps']} GapMax={sniff['gap_max_ms']}ms CPU={sys_stats['cpu_pct']}%")
            
            # Verify schema
            if snapshot.get("schema") != "uav.pqc.telemetry.v1":
                print("ERROR: Schema mismatch!")
                
    except KeyboardInterrupt:
        print("Interrupted")
    finally:
        print("Stopping...")
        running_event.clear()
        collector.stop()
        gen_thread.join(timeout=1.0)
        print("Done.")

if __name__ == "__main__":
    main()

==================================================

sscheduler\gcs_metrics.py
==================================================
"""
GCS Telemetry Metrics Collector (Schema v1)
sscheduler/gcs_metrics.py

Collects real-time receiver-side metrics for the GCS, matching schema uav.pqc.telemetry.v1.
Non-blocking, bounded memory, best-effort.
"""

import os
import time
import json
import threading
import socket
import logging
from pathlib import Path
from collections import deque, defaultdict
from datetime import datetime, timezone

try:
    import psutil
except ImportError:
    psutil = None

try:
    from pymavlink import mavutil
except ImportError:
    mavutil = None

# Constants
SCHEMA_NAME = "uav.pqc.telemetry.v1"
SCHEMA_VER = 1
WINDOW_S = 5.0
BURST_GAP_THRESHOLD_MS = 200.0
MAX_PACKETS_PER_LOOP = 100

class GcsMetricsCollector:
    def __init__(self, mavlink_host, mavlink_port, proxy_manager=None, mavproxy_proc=None, log_dir=None):
        self.mavlink_host = mavlink_host
        self.mavlink_port = mavlink_port
        self.proxy_manager = proxy_manager
        self.mavproxy_proc = mavproxy_proc
        
        if log_dir:
            self.log_dir = Path(log_dir)
        else:
            self.log_dir = Path(__file__).parent.parent / "logs"
        
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.log_file = self.log_dir / "gcs_telemetry_v1.jsonl"
        
        self.running = False
        self.thread = None
        self.mav_conn = None
        self.sock = None
        self.lock = threading.Lock()
        
        # Identity
        self.pid = os.getpid()
        self.boot_id = int(time.time())
        
        # Metrics State (Sliding Window)
        self.arrival_times = deque() # (mono_s, size_bytes)
        self.gaps = deque()          # (mono_s, gap_ms)
        self.burst_gaps = 0          # Count of gaps > threshold in window
        
        # MAVLink State
        self.mav_state = {
            "heartbeat": None,
            "sys_status": None,
            "radio_status": None,
            "failsafe": {"flags": 0, "last_statustext": None},
            "decode_stats": {"ok": 0, "parse_errors": 0, "reason": None}
        }
        self.msg_rates = defaultdict(int) # msg_id -> count in window
        self.msg_timestamps = deque()     # (mono_s, msg_id)
        
        # Tunnel/Events
        self.events = [] # List of event dicts
        
        # Loop health
        self.last_tick_mono = 0.0
        self.loop_lag_ms = 0.0

    def start(self):
        if self.running:
            return
        self.running = True
        self.thread = threading.Thread(target=self._run_loop, daemon=True)
        self.thread.start()
        logging.info(f"GCS Metrics Collector v1 started on {self.mavlink_host}:{self.mavlink_port}")

    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.mav_conn:
            self.mav_conn.close()
        if self.sock:
            self.sock.close()

    def _connect(self):
        # Use udpin to bind and listen for packets from MAVProxy
        conn_str = f"udpin:{self.mavlink_host}:{self.mavlink_port}"
        if mavutil:
            try:
                self.mav_conn = mavutil.mavlink_connection(conn_str, source_system=255)
                return True
            except Exception as e:
                logging.error(f"MAVLink connect failed: {e}")
                pass
        
        try:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            self.sock.bind((self.mavlink_host, self.mavlink_port))
            self.sock.settimeout(0.1)
            return True
        except Exception as e:
            logging.error(f"Socket bind failed: {e}")
            return False

    def _prune_windows(self, now_mono):
        cutoff = now_mono - WINDOW_S
        
        while self.arrival_times and self.arrival_times[0][0] < cutoff:
            self.arrival_times.popleft()
            
        while self.gaps and self.gaps[0][0] < cutoff:
            self.gaps.popleft()
            
        while self.msg_timestamps and self.msg_timestamps[0][0] < cutoff:
            _, msg_id = self.msg_timestamps.popleft()
            self.msg_rates[msg_id] = max(0, self.msg_rates[msg_id] - 1)

    def _process_mavlink(self, msg, now_mono):
        msg_type = msg.get_type()
        msg_id = msg.get_msgId()
        
        # Rate tracking
        self.msg_rates[msg_id] += 1
        self.msg_timestamps.append((now_mono, msg_id))
        
        if msg_type == 'HEARTBEAT':
            self.mav_state['heartbeat'] = {
                "age_ms": 0, # Updated at snapshot time
                "last_mono": now_mono,
                "armed": bool(msg.base_mode & mavutil.mavlink.MAV_MODE_FLAG_SAFETY_ARMED) if mavutil else False,
                "mode": msg.custom_mode,
                "sysid": msg.get_srcSystem(),
                "compid": msg.get_srcComponent()
            }
        elif msg_type == 'SYS_STATUS':
            self.mav_state['sys_status'] = {
                "battery_remaining_pct": msg.battery_remaining,
                "voltage_battery_mv": msg.voltage_battery,
                "drop_rate_comm": msg.drop_rate_comm,
                "errors_count": [msg.errors_count1, msg.errors_count2, msg.errors_count3, msg.errors_count4],
                "load_pct": msg.load / 10.0 # usually 1000 = 100%
            }
        elif msg_type == 'RADIO_STATUS':
            self.mav_state['radio_status'] = {
                "rssi": msg.rssi,
                "remrssi": msg.remrssi,
                "noise": msg.noise,
                "remnoise": msg.remnoise,
                "rxerrors": msg.rxerrors,
                "fixed": msg.fixed
            }
        elif msg_type == 'STATUSTEXT':
            txt = msg.text
            if isinstance(txt, bytes):
                txt = txt.decode('utf-8', errors='ignore')
            self.mav_state['failsafe']['last_statustext'] = txt

    def _read_packets(self):
        count = 0
        now_mono = time.monotonic()
        
        while count < MAX_PACKETS_PER_LOOP:
            ts_mono = None
            size = 0
            msg = None
            
            if self.mav_conn:
                try:
                    msg = self.mav_conn.recv_match(blocking=False)
                    if msg:
                        ts_mono = time.monotonic()
                        # Estimate size
                        size = 20 # Header
                        if hasattr(msg, 'get_payload'):
                             payload = msg.get_payload()
                             if payload:
                                 size += len(payload)
                        self.mav_state['decode_stats']['ok'] += 1
                        self._process_mavlink(msg, ts_mono)
                except Exception as e:
                    self.mav_state['decode_stats']['parse_errors'] += 1
                    self.mav_state['decode_stats']['reason'] = str(e)
            elif self.sock:
                try:
                    data, _ = self.sock.recvfrom(65535)
                    ts_mono = time.monotonic()
                    size = len(data)
                except socket.timeout:
                    pass
                except Exception:
                    pass
            
            if ts_mono:
                with self.lock:
                    # Gap detection
                    if self.arrival_times:
                        last_mono = self.arrival_times[-1][0]
                        gap_ms = (ts_mono - last_mono) * 1000.0
                        self.gaps.append((ts_mono, gap_ms))
                        if gap_ms > BURST_GAP_THRESHOLD_MS:
                            self.burst_gaps += 1
                    
                    self.arrival_times.append((ts_mono, size))
                count += 1
            else:
                break
        
        with self.lock:
            self._prune_windows(now_mono)
            # Prune burst gaps count (recalculate from window)
            self.burst_gaps = sum(1 for _, g in self.gaps if g > BURST_GAP_THRESHOLD_MS)

    def get_snapshot(self):
        now_mono = time.monotonic()
        now_wall = time.time()
        
        with self.lock:
            # Metrics Calculation
            count = len(self.arrival_times)
            total_bytes = sum(s for _, s in self.arrival_times)
            
            rx_pps = count / WINDOW_S
            rx_bps = total_bytes / WINDOW_S
            
            silence_ms = 0.0
            if self.arrival_times:
                silence_ms = (now_mono - self.arrival_times[-1][0]) * 1000.0
            
            gap_max_ms = 0.0
            gap_p95_ms = 0.0
            jitter_ms = 0.0
            
            if self.gaps:
                gap_values = [g for _, g in self.gaps]
                gap_max_ms = max(gap_values)
                gap_values.sort()
                idx = int(len(gap_values) * 0.95)
                gap_p95_ms = gap_values[idx] if gap_values else 0.0
                
                if len(gap_values) > 1:
                    mean_gap = sum(gap_values) / len(gap_values)
                    jitter_ms = sum(abs(g - mean_gap) for g in gap_values) / len(gap_values)

            # System Stats
            cpu_pct = psutil.cpu_percent(interval=None) if psutil else 0.0
            mem_pct = psutil.virtual_memory().percent if psutil else 0.0
            cpu_freq = 0.0
            if psutil and hasattr(psutil, 'cpu_freq'):
                f = psutil.cpu_freq()
                if f: cpu_freq = f.current
            
            # Process Health
            mavproxy_alive = False
            mavproxy_pid = 0
            if self.mavproxy_proc:
                mavproxy_alive = self.mavproxy_proc.is_running()
                mavproxy_pid = self.mavproxy_proc.process.pid if self.mavproxy_proc.process else 0
            
            # Proxy Status
            proxy_alive = False
            proxy_pid = 0
            active_suite = None
            if self.proxy_manager:
                proxy_alive = self.proxy_manager.is_running()
                active_suite = self.proxy_manager.current_suite
                # If ManagedProcess exposed PID, we'd use it. Assuming it does via .process
                if self.proxy_manager.managed_proc and self.proxy_manager.managed_proc.process:
                    proxy_pid = self.proxy_manager.managed_proc.process.pid

            # MAVLink Rates
            msg_rate_total = len(self.msg_timestamps) / WINDOW_S
            # Critical: HEARTBEAT(0), SYS_STATUS(1), STATUSTEXT(253)
            msg_rate_critical = (self.msg_rates[0] + self.msg_rates[1] + self.msg_rates[253]) / WINDOW_S
            # High: ATTITUDE(30), VFR_HUD(74) - examples
            msg_rate_high = (self.msg_rates[30] + self.msg_rates[74]) / WINDOW_S

            # Heartbeat Age
            hb = self.mav_state['heartbeat']
            if hb:
                hb['age_ms'] = (now_mono - hb['last_mono']) * 1000.0
                # Remove internal field before export
                hb_export = hb.copy()
                del hb_export['last_mono']
            else:
                hb_export = None

            snapshot = {
                "schema": SCHEMA_NAME,
                "schema_ver": SCHEMA_VER,
                "sender": {
                    "role": "gcs",
                    "node_id": socket.gethostname(),
                    "pid": self.pid
                },
                "t": {
                    "wall_ms": now_wall * 1000.0,
                    "mono_ms": now_mono * 1000.0,
                    "boot_id": self.boot_id
                },
                "caps": {
                    "pymavlink": mavutil is not None,
                    "psutil": psutil is not None,
                    "proxy_status_file": False # Deprecated in favor of process check
                },
                "state": {
                    "gcs": {
                        "mavproxy_alive": mavproxy_alive,
                        "mavproxy_pid": mavproxy_pid,
                        "qgc_alive": False, # Placeholder
                        "collector_alive": True,
                        "collector_last_tick_mono_ms": self.last_tick_mono * 1000.0,
                        "collector_loop_lag_ms": self.loop_lag_ms
                    },
                    "suite": {
                        "active_suite": active_suite,
                        "suite_epoch": 0, # TODO: wire if available
                        "pending_suite": None
                    }
                },
                "metrics": {
                    "sniff": {
                        "bind": f"{self.mavlink_host}:{self.mavlink_port}",
                        "window_s": WINDOW_S,
                        "sample_count": count,
                        "rx_pps": round(rx_pps, 1),
                        "rx_bps": round(rx_bps, 1),
                        "silence_ms": round(silence_ms, 1),
                        "gap_max_ms": round(gap_max_ms, 1),
                        "gap_p95_ms": round(gap_p95_ms, 1),
                        "jitter_ms": round(jitter_ms, 1),
                        "burst_gap_count": self.burst_gaps,
                        "burst_gap_threshold_ms": BURST_GAP_THRESHOLD_MS
                    },
                    "sys": {
                        "cpu_pct": cpu_pct,
                        "mem_pct": mem_pct,
                        "cpu_freq_mhz": cpu_freq,
                        "temp_c": 0.0 # Requires platform specific
                    }
                },
                "mav": {
                    "decode": self.mav_state['decode_stats'],
                    "heartbeat": hb_export,
                    "sys_status": self.mav_state['sys_status'],
                    "radio_status": self.mav_state['radio_status'],
                    "rates": {
                        "window_s": WINDOW_S,
                        "msg_rate_total": round(msg_rate_total, 1),
                        "msg_rate_critical": round(msg_rate_critical, 1),
                        "msg_rate_high": round(msg_rate_high, 1)
                    },
                    "failsafe": self.mav_state['failsafe']
                },
                "tunnel": {
                    "proxy_alive": proxy_alive,
                    "proxy_pid": proxy_pid,
                    "status_file_age_ms": 0,
                    "counters": None
                },
                "events": list(self.events) # Copy
            }
            
            # Clear one-shot events
            self.events.clear()
            
            return snapshot

    def _run_loop(self):
        self._connect()
        last_log_time = time.monotonic()
        
        while self.running:
            loop_start = time.monotonic()
            
            # Reconnect
            if not self.mav_conn and not self.sock:
                if not self._connect():
                    time.sleep(1.0)
                    continue

            self._read_packets()
            
            # Logging (1Hz)
            if loop_start - last_log_time >= 1.0:
                self._write_log()
                last_log_time = loop_start
            
            # Loop health
            self.last_tick_mono = loop_start
            elapsed = time.monotonic() - loop_start
            self.loop_lag_ms = elapsed * 1000.0
            
            sleep_time = max(0.01, 0.1 - elapsed) # Aim for 10Hz loop
            time.sleep(sleep_time)

    def _write_log(self):
        try:
            snapshot = self.get_snapshot()
            # JSON serialization helper for non-serializable types
            def default(o):
                if isinstance(o, (datetime,)):
                    return o.isoformat()
                return str(o)
                
            with open(self.log_file, "a") as f:
                f.write(json.dumps(snapshot, default=default) + "\n")
        except Exception:
            pass

    def add_event(self, event_type, **kwargs):
        with self.lock:
            evt = {
                "type": event_type,
                "t_mono_ms": time.monotonic() * 1000.0,
                **kwargs
            }
            self.events.append(evt)

==================================================

sscheduler\policy.py
==================================================
import random
import time


class SchedulingPolicy:
    """Base class for all scheduling logic."""
    def __init__(self, suites):
        self.suites = list(suites)
        self.current_index = -1

    def next_suite(self):
        """Returns the next suite name to run."""
        raise NotImplementedError("Must implement next_suite")

    def get_duration(self):
        """Returns duration in seconds for the current run."""
        return 10.0  # Default


class LinearLoopPolicy(SchedulingPolicy):
    """Cycles through suites 0 to N, then restarts."""
    def next_suite(self):
        self.current_index = (self.current_index + 1) % len(self.suites)
        return self.suites[self.current_index]


class RandomPolicy(SchedulingPolicy):
    """Picks a random suite every time."""
    def next_suite(self):
        return random.choice(self.suites)


class ManualOverridePolicy(SchedulingPolicy):
    """Runs a specific index repeatedly."""
    def __init__(self, suites, fixed_index=0):
        super().__init__(suites)
        self.fixed_index = fixed_index

    def next_suite(self):
        safe_index = self.fixed_index % len(self.suites)
        return self.suites[safe_index]


==================================================

sscheduler\sdrone copy.py
==================================================
#!/usr/bin/env python3
"""
Simplified Drone Scheduler (CONTROLLER) - sscheduler/sdrone.py

REVERSED CONTROL: Drone is the controller, GCS follows.
- Drone decides suite order, timing, rekey
- Drone sends commands to GCS
- Drone runs echo server (receives traffic from GCS)
- Drone starts its proxy first, then tells GCS to start

Usage:
    python -m sscheduler.sdrone [options]

Environment:
    DRONE_HOST          Drone IP (default: from config)
    GCS_HOST            GCS IP (default: from config)
    GCS_CONTROL_HOST    GCS control server IP (default: GCS_HOST)
"""

import os
import sys
import time
import json
import socket
import signal
import argparse
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timezone

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import CONFIG
from core.suites import get_suite, list_suites

# Extract config values (use CONFIG as single source of truth)
DRONE_HOST = str(CONFIG.get("DRONE_HOST"))
GCS_HOST = str(CONFIG.get("GCS_HOST"))
DRONE_PLAIN_RX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
DRONE_PLAIN_TX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_TX", 47003))

# Control endpoint for GCS: use configured GCS_HOST and GCS_CONTROL_PORT
GCS_CONTROL_HOST = str(CONFIG.get("GCS_HOST"))
GCS_CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", 48080))

# Derived internal proxy control port to avoid collisions
PROXY_INTERNAL_CONTROL_PORT = GCS_CONTROL_PORT + 100

DEFAULT_SUITE = "cs-mlkem768-aesgcm-mldsa65"
SECRETS_DIR = Path(__file__).parent.parent / "secrets" / "matrix"

# Traffic settings (for telling GCS how long to run)
DEFAULT_DURATION = 10.0  # seconds per suite
DEFAULT_RATE_MBPS = 110.0
PAYLOAD_SIZE = 1200

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# --------------------
LOCAL_DURATION = None  # override DEFAULT_DURATION if set, e.g. 10.0
LOCAL_RATE_MBPS = None  # override DEFAULT_RATE_MBPS if set, e.g. 110.0
LOCAL_MAX_SUITES = None  # limit suites run, e.g. 2
LOCAL_SUITES = None  # list of suite names to run, or None

# Get all suites (list_suites returns dict, convert to list of dicts)
_suites_dict = list_suites()
SUITES = [{"name": k, **v} for k, v in _suites_dict.items()]

ROOT = Path(__file__).resolve().parents[1]
LOGS_DIR = ROOT / "logs" / "sscheduler" / "drone"
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# Logging
# ============================================================

def log(msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [sdrone-ctrl] {msg}", flush=True)

# ============================================================
# UDP Echo Server (drone receives traffic from GCS)
# ============================================================

class UdpEchoServer:
    """Echoes UDP packets: receives on DRONE_PLAIN_RX, sends back on DRONE_PLAIN_TX"""
    
    def __init__(self):
        self.rx_sock = None
        self.tx_sock = None
        self.running = False
        self.thread = None
        self.rx_count = 0
        self.tx_count = 0
        self.rx_bytes = 0
        self.tx_bytes = 0
        self.lock = threading.Lock()
    
    def start(self):
        if self.running:
            return
        
        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        self.rx_sock.bind((DRONE_HOST, DRONE_PLAIN_RX_PORT))
        self.rx_sock.settimeout(1.0)
        
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)
        
        self.running = True
        self.thread = threading.Thread(target=self._echo_loop, daemon=True)
        self.thread.start()
        
        log(f"Echo server listening on {DRONE_HOST}:{DRONE_PLAIN_RX_PORT}")
    
    def _echo_loop(self):
        while self.running:
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                with self.lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
                
                self.tx_sock.sendto(data, (DRONE_HOST, DRONE_PLAIN_TX_PORT))
                with self.lock:
                    self.tx_count += 1
                    self.tx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Echo error: {e}")
    
    def get_stats(self):
        with self.lock:
            return {
                "rx_count": self.rx_count,
                "tx_count": self.tx_count,
                "rx_bytes": self.rx_bytes,
                "tx_bytes": self.tx_bytes,
            }
    
    def reset_stats(self):
        with self.lock:
            self.rx_count = 0
            self.tx_count = 0
            self.rx_bytes = 0
            self.tx_bytes = 0
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.rx_sock:
            self.rx_sock.close()
        if self.tx_sock:
            self.tx_sock.close()

# ============================================================
# GCS Control Client (drone sends commands to GCS)
# ============================================================

def send_gcs_command(cmd: str, **params) -> dict:
    """Send command to GCS control server"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(30.0)
        sock.connect((GCS_CONTROL_HOST, GCS_CONTROL_PORT))
        
        request = {"cmd": cmd, **params}
        sock.sendall(json.dumps(request).encode() + b"\n")
        
        response = b""
        while True:
            chunk = sock.recv(4096)
            if not chunk:
                break
            response += chunk
            if b"\n" in response:
                break
        
        sock.close()
        return json.loads(response.decode().strip())
    except Exception as e:
        return {"status": "error", "message": str(e)}

def wait_for_gcs(timeout: float = 30.0) -> bool:
    """Wait for GCS control server to be ready"""
    start = time.time()
    while time.time() - start < timeout:
        result = send_gcs_command("ping")
        if result.get("status") == "ok":
            return True
        time.sleep(0.5)
    return False

# ============================================================
# Drone Proxy Management
# ============================================================

class DroneProxyManager:
    """Manages drone proxy subprocess"""
    
    def __init__(self):
        self.process = None
        self.current_suite = None
    
    def start(self, suite_name: str) -> bool:
        """Start drone proxy with given suite"""
        if self.process and self.process.poll() is None:
            self.stop()
        
        suite = get_suite(suite_name)
        if not suite:
            log(f"Unknown suite: {suite_name}")
            return False
        
        secret_dir = SECRETS_DIR / suite_name
        peer_pubkey = secret_dir / "gcs_signing.pub"
        
        if not peer_pubkey.exists():
            log(f"Missing key: {peer_pubkey}")
            return False
        
        cmd = [
            sys.executable, "-m", "core.run_proxy", "drone",
            "--suite", suite_name,
            "--peer-pubkey-file", str(peer_pubkey),
            "--quiet"
        ]

        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_path = LOGS_DIR / f"drone_{suite_name}_{timestamp}.log"
        log(f"Launching: {' '.join(cmd)} (log: {log_path})")
        log_handle = open(log_path, "w", encoding="utf-8")
        self.process = subprocess.Popen(
            cmd,
            stdout=log_handle,
            stderr=subprocess.STDOUT,
            bufsize=1,
            universal_newlines=True,
        )
        self._last_log = log_path
        self.current_suite = suite_name
        
        # Wait for proxy to initialize
        time.sleep(3.0)

        if self.process.poll() is not None:
            log(f"Proxy exited early with code {self.process.returncode}")
            # Print tail of log for diagnosis
            try:
                with open(self._last_log, "r", encoding="utf-8") as fh:
                    lines = fh.read().splitlines()[-30:]
                    log("--- proxy log tail ---")
                    for l in lines:
                        log(l)
                    log("--- end log tail ---")
            except Exception:
                pass
            return False
        
        return True
    
    def stop(self):
        """Stop drone proxy"""
        if self.process:
            self.process.terminate()
            try:
                self.process.wait(timeout=5.0)
            except subprocess.TimeoutExpired:
                self.process.kill()
            self.process = None
            self.current_suite = None
            try:
                # close last log handle if exists by leaving file closed (we opened in start)
                pass
            except Exception:
                pass
    
    def is_running(self) -> bool:
        return self.process is not None and self.process.poll() is None

# ============================================================
# Suite Runner
# ============================================================

def run_suite(proxy: DroneProxyManager, echo: UdpEchoServer, 
              suite_name: str, duration: float, is_first: bool = False) -> dict:
    """Run a single suite test - drone controls the flow.
    
    NOTE: Even though drone is the controller, GCS proxy must start first
    because the TCP handshake requires GCS to listen and drone to connect.
    Drone controls WHEN to start, but GCS proxy goes up first.
    """
    
    result = {
        "suite": suite_name,
        "status": "unknown",
        "echo_rx": 0,
        "echo_tx": 0,
    }
    
    # Reset echo stats
    echo.reset_stats()
    
    if not is_first:
        # Rekey: tell GCS to prepare (stop its proxy)
        log("Preparing GCS for rekey...")
        resp = send_gcs_command("prepare_rekey")
        if resp.get("status") != "ok":
            log(f"GCS prepare_rekey failed: {resp}")
            result["status"] = "gcs_prepare_failed"
            return result
        
        # Stop our proxy too
        proxy.stop()
        time.sleep(0.5)
    
    # Tell GCS to start its proxy first (GCS listens, drone connects)
    log(f"Telling GCS to start proxy for {suite_name}...")
    resp = send_gcs_command("start_proxy", suite=suite_name)
    log(f"GCS start_proxy response: {resp}")
    if resp.get("status") != "ok":
        log(f"GCS start_proxy failed: {resp}")
        result["status"] = "gcs_start_failed"
        return result

    # Wait for GCS proxy to be ready by polling status
    log("Waiting for GCS proxy to report ready...")
    start_wait = time.time()
    ready = False
    while time.time() - start_wait < 20.0:
        time.sleep(0.5)
        try:
            st = send_gcs_command("status")
            if st.get("proxy_running"):
                ready = True
                break
        except Exception:
            pass

    if not ready:
        log("GCS proxy did not become ready in time")
        result["status"] = "gcs_not_ready"
        return result
    
    # Now start drone proxy (it will connect to GCS)
    log(f"Starting drone proxy for {suite_name}...")
    if not proxy.start(suite_name):
        result["status"] = "proxy_start_failed"
        # include last log path if available
        try:
            tail = getattr(proxy, "_last_log", None)
            if tail:
                result["log"] = str(tail)
        except Exception:
            pass
        return result
    
    # Wait for handshake
    time.sleep(1.0)
    
    # Tell GCS to start traffic
    log("Telling GCS to start traffic...")
    resp = send_gcs_command("start_traffic", duration=duration)
    if resp.get("status") != "ok":
        log(f"GCS start_traffic failed: {resp}")
        result["status"] = "gcs_traffic_failed"
        return result
    
    log("Traffic started, waiting for completion...")
    
    # Wait for GCS to finish traffic generation
    # Poll GCS status
    traffic_done = False
    start_time = time.time()
    max_wait = duration + 30  # Extra buffer
    
    while time.time() - start_time < max_wait:
        time.sleep(2.0)
        
        # Log echo stats periodically
        stats = echo.get_stats()
        log(f"Echo stats: RX={stats['rx_count']}, TX={stats['tx_count']}, "
            f"RX_bytes={stats['rx_bytes']:,}, TX_bytes={stats['tx_bytes']:,}")
        
        # Check GCS status
        status = send_gcs_command("status")
        if status.get("traffic_complete"):
            traffic_done = True
            break
        
        # Check if proxy died
        if not proxy.is_running():
            log("Proxy exited unexpectedly")
            result["status"] = "proxy_exited"
            return result
    
    if not traffic_done:
        log("Traffic did not complete in time")
        result["status"] = "timeout"
        return result
    
    # Get final stats
    stats = echo.get_stats()
    result["echo_rx"] = stats["rx_count"]
    result["echo_tx"] = stats["tx_count"]
    result["status"] = "pass"
    
    return result

# ============================================================
# Main
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="Drone Scheduler (Controller)")
    parser.add_argument("--suite", default=None, help="Single suite to run")
    parser.add_argument("--nist-level", choices=["L1", "L3", "L5"], help="Run suites for NIST level")
    parser.add_argument("--all", action="store_true", help="Run all suites")
    parser.add_argument("--duration", type=float, default=DEFAULT_DURATION, help="Seconds per suite")
    parser.add_argument("--rate", type=float, default=DEFAULT_RATE_MBPS, help="Traffic rate Mbps")
    parser.add_argument("--max-suites", type=int, default=None, help="Max suites to run")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Simplified Drone Scheduler (CONTROLLER) - sscheduler")
    print("=" * 60)
    # Configuration dump for debugging
    cfg = {
        "DRONE_HOST": DRONE_HOST,
        "GCS_HOST": GCS_HOST,
        "GCS_CONTROL": f"{GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}",
        "PROXY_INTERNAL_CONTROL_PORT": PROXY_INTERNAL_CONTROL_PORT,
        "DRONE_PLAINTEXT_RX": DRONE_PLAIN_RX_PORT,
        "DRONE_PLAINTEXT_TX": DRONE_PLAIN_TX_PORT,
    }
    log("Configuration Dump:")
    for k, v in cfg.items():
        log(f"  {k}: {v}")
    log(f"Duration: {args.duration}s per suite, Rate: {args.rate} Mbps")
    
    # Determine suites to run
    if args.suite:
        suites_to_run = [args.suite]
    elif args.nist_level:
        suites_to_run = [s["name"] for s in SUITES if s.get("nist_level") == args.nist_level]
    elif args.all:
        suites_to_run = [s["name"] for s in SUITES]
    else:
        # Default: run all available suites
        suites_to_run = [s["name"] for s in SUITES]
    
    if args.max_suites:
        suites_to_run = suites_to_run[:args.max_suites]

    # Apply local in-file overrides
    if LOCAL_RATE_MBPS is not None:
        args.rate = float(LOCAL_RATE_MBPS)
    if LOCAL_DURATION is not None:
        args.duration = float(LOCAL_DURATION)
    if LOCAL_SUITES:
        suites_to_run = [s for s in LOCAL_SUITES if s in [x["name"] for x in SUITES]]
    if LOCAL_MAX_SUITES:
        suites_to_run = suites_to_run[: int(LOCAL_MAX_SUITES)]
    
    log(f"Suites to run: {len(suites_to_run)}")
    
    # Initialize components
    echo = UdpEchoServer()
    echo.start()
    
    proxy = DroneProxyManager()
    
    # Wait for GCS
    log("Waiting for GCS scheduler...")
    if not wait_for_gcs(timeout=60.0):
        log("ERROR: GCS scheduler not responding")
        echo.stop()
        return 1
    log("GCS scheduler is ready")
    
    # Tell GCS the test parameters
    send_gcs_command("configure", rate_mbps=args.rate, duration=args.duration)
    
    # Run suites
    results = []
    
    def signal_handler(sig, frame):
        log("Interrupted, cleaning up...")
        proxy.stop()
        echo.stop()
        send_gcs_command("stop")
        sys.exit(1)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        for i, suite_name in enumerate(suites_to_run):
            log("=" * 60)
            log(f"Running suite {i+1}/{len(suites_to_run)}: {suite_name}")
            log("=" * 60)
            
            result = run_suite(proxy, echo, suite_name, args.duration, is_first=(i == 0))
            results.append(result)
            
            if result["status"] == "pass":
                log(f"Suite PASSED: echo RX={result['echo_rx']}, TX={result['echo_tx']}")
            else:
                log(f"Suite FAILED: {result['status']}")
            
            # Wait between suites
            if i < len(suites_to_run) - 1:
                log("Waiting 2s before next suite...")
                time.sleep(2.0)
    
    finally:
        proxy.stop()
        echo.stop()
        send_gcs_command("stop")
    
    # Summary
    print()
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for r in results if r["status"] == "pass")
    failed = len(results) - passed
    
    for r in results:
        status = "[PASS]" if r["status"] == "pass" else "[FAIL]"
        print(f"  {status} {r['suite']}: {r['status']}, echo={r['echo_rx']}/{r['echo_tx']}")
    
    print("-" * 60)
    print(f"Total: {len(results)}, Passed: {passed}, Failed: {failed}")
    print("=" * 60)
    
    return 0 if failed == 0 else 1

if __name__ == "__main__":
    sys.exit(main())

==================================================

sscheduler\sdrone.py
==================================================
#!/usr/bin/env python3
"""
Simplified Drone Scheduler (CONTROLLER) - sscheduler/sdrone.py

REVERSED CONTROL: Drone is the controller, GCS follows.
- Drone decides suite order, timing, rekey
- Drone sends commands to GCS
- Drone runs echo server (receives traffic from GCS)
- Drone starts its proxy first, then tells GCS to start

Usage:
    python -m sscheduler.sdrone [options]

Environment:
    DRONE_HOST          Drone IP (default: from config)
    GCS_HOST            GCS IP (default: from config)
    GCS_CONTROL_HOST    GCS control server IP (default: GCS_HOST)
"""

import os
import sys
import time
import json
import socket
import signal
import argparse
import threading
import subprocess
import logging
import atexit
from pathlib import Path
from datetime import datetime, timezone

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import CONFIG
from core.suites import get_suite, list_suites
from core.process import ManagedProcess
from tools.mavproxy_manager import MavProxyManager
from sscheduler.policy import LinearLoopPolicy, RandomPolicy, ManualOverridePolicy

# Extract config values (use CONFIG as single source of truth)
DRONE_HOST = str(CONFIG.get("DRONE_HOST"))
GCS_HOST = str(CONFIG.get("GCS_HOST"))
DRONE_PLAIN_RX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
DRONE_PLAIN_TX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_TX", 47003))
GCS_TELEMETRY_PORT = int(CONFIG.get("GCS_TELEMETRY_PORT", 52080))

# Control endpoint for GCS: use configured GCS_HOST and GCS_CONTROL_PORT
GCS_CONTROL_HOST = str(CONFIG.get("GCS_HOST"))
GCS_CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", 48080))

# Derived internal proxy control port to avoid collisions
PROXY_INTERNAL_CONTROL_PORT = GCS_CONTROL_PORT + 100

DEFAULT_SUITE = "cs-mlkem768-aesgcm-mldsa65"
SECRETS_DIR = Path(__file__).parent.parent / "secrets" / "matrix"

# Traffic settings (for telling GCS how long to run)
DEFAULT_DURATION = 10.0  # seconds per suite
DEFAULT_RATE_MBPS = 110.0
PAYLOAD_SIZE = 1200

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# --------------------
LOCAL_DURATION = None  # override DEFAULT_DURATION if set, e.g. 10.0
LOCAL_RATE_MBPS = None  # override DEFAULT_RATE_MBPS if set, e.g. 110.0
LOCAL_MAX_SUITES = None  # limit suites run, e.g. 2
LOCAL_SUITES = None  # list of suite names to run, or None

# Get all suites (list_suites returns dict, convert to list of dicts)
_suites_dict = list_suites()
SUITES = [{"name": k, **v} for k, v in _suites_dict.items()]

ROOT = Path(__file__).resolve().parents[1]
LOGS_DIR = ROOT / "logs" / "sscheduler" / "drone"
LOGS_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# Logging
# ============================================================

def log(msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [sdrone-ctrl] {msg}", flush=True)

# ============================================================
# Telemetry Listener & Decision Context
# ============================================================

class TelemetryListener:
    """Receives telemetry updates from GCS via UDP"""
    def __init__(self, port: int):
        self.port = port
        self.sock = None
        self.running = False
        self.thread = None
        self.latest_data = {}
        self.last_update = 0
        self.lock = threading.Lock()

    def start(self):
        if self.running:
            return
            
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.sock.bind(("0.0.0.0", self.port))
        self.sock.settimeout(1.0)
        
        self.running = True
        self.thread = threading.Thread(target=self._listen_loop, daemon=True)
        self.thread.start()
        log(f"Telemetry listener started on port {self.port}")

    def _listen_loop(self):
        while self.running:
            try:
                data, addr = self.sock.recvfrom(65535)
                try:
                    packet = json.loads(data.decode('utf-8'))
                    with self.lock:
                        # Support v1 schema (flat) or legacy (nested data)
                        if "schema" in packet:
                            self.latest_data = packet
                        else:
                            self.latest_data = packet.get("data", {})
                        self.last_update = time.time()
                except json.JSONDecodeError:
                    pass
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Telemetry error: {e}")

    def get_latest(self):
        with self.lock:
            return self.latest_data, self.last_update

    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.sock:
            self.sock.close()

class DecisionContext:
    """Aggregates system state for policy decisions"""
    def __init__(self, telemetry: TelemetryListener):
        self.telemetry = telemetry

    def get_gcs_status(self):
        data, ts = self.telemetry.get_latest()
        age = time.time() - ts
        return data, age

# ============================================================
# UDP Echo Server (drone receives traffic from GCS)
# ============================================================

class UdpEchoServer:
    """Echoes UDP packets: receives on DRONE_PLAIN_RX, sends back on DRONE_PLAIN_TX"""
    
    def __init__(self):
        self.rx_sock = None
        self.tx_sock = None
        self.running = False
        self.thread = None
        self.rx_count = 0
        self.tx_count = 0
        self.rx_bytes = 0
        self.tx_bytes = 0
        self.lock = threading.Lock()
    
    def start(self):
        if self.running:
            return
        
        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        self.rx_sock.bind((DRONE_HOST, DRONE_PLAIN_RX_PORT))
        self.rx_sock.settimeout(1.0)
        
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)
        
        self.running = True
        self.thread = threading.Thread(target=self._echo_loop, daemon=True)
        self.thread.start()
        
        log(f"Echo server listening on {DRONE_HOST}:{DRONE_PLAIN_RX_PORT}")
    
    def _echo_loop(self):
        while self.running:
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                with self.lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
                
                self.tx_sock.sendto(data, (DRONE_HOST, DRONE_PLAIN_TX_PORT))
                with self.lock:
                    self.tx_count += 1
                    self.tx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Echo error: {e}")
    
    def get_stats(self):
        with self.lock:
            return {
                "rx_count": self.rx_count,
                "tx_count": self.tx_count,
                "rx_bytes": self.rx_bytes,
                "tx_bytes": self.tx_bytes,
            }
    
    def reset_stats(self):
        with self.lock:
            self.rx_count = 0
            self.tx_count = 0
            self.rx_bytes = 0
            self.tx_bytes = 0
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.rx_sock:
            self.rx_sock.close()
        if self.tx_sock:
            self.tx_sock.close()


# MavProxyManager imported from tools.mavproxy_manager

# ============================================================
# GCS Control Client (drone sends commands to GCS)
# ============================================================

def send_gcs_command(cmd: str, **params) -> dict:
    """Send command to GCS control server"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(30.0)
        sock.connect((GCS_CONTROL_HOST, GCS_CONTROL_PORT))
        
        request = {"cmd": cmd, **params}
        sock.sendall(json.dumps(request).encode() + b"\n")
        
        response = b""
        while True:
            chunk = sock.recv(4096)
            if not chunk:
                break
            response += chunk
            if b"\n" in response:
                break
        
        sock.close()
        return json.loads(response.decode().strip())
    except Exception as e:
        return {"status": "error", "message": str(e)}

def wait_for_gcs(timeout: float = 30.0) -> bool:
    """Wait for GCS control server to be ready"""
    start = time.time()
    while time.time() - start < timeout:
        result = send_gcs_command("ping")
        if result.get("status") == "ok":
            return True
        time.sleep(0.5)
    return False

# ============================================================
# Drone Proxy Management
# ============================================================

class DroneProxyManager:
    """Manages drone proxy subprocess"""
    
    def __init__(self):
        self.managed_proc = None
        self.current_suite = None
    
    def start(self, suite_name: str) -> bool:
        """Start drone proxy with given suite"""
        if self.managed_proc and self.managed_proc.is_running():
            self.stop()
        
        suite = get_suite(suite_name)
        if not suite:
            log(f"Unknown suite: {suite_name}")
            return False
        
        secret_dir = SECRETS_DIR / suite_name
        peer_pubkey = secret_dir / "gcs_signing.pub"
        
        if not peer_pubkey.exists():
            log(f"Missing key: {peer_pubkey}")
            return False
        
        cmd = [
            sys.executable, "-m", "core.run_proxy", "drone",
            "--suite", suite_name,
            "--peer-pubkey-file", str(peer_pubkey),
            "--quiet",
            "--status-file", str(LOGS_DIR / "drone_status.json")
        ]

        timestamp = time.strftime("%Y%m%d-%H%M%S")
        log_path = LOGS_DIR / f"drone_{suite_name}_{timestamp}.log"
        log(f"Launching: {' '.join(cmd)} (log: {log_path})")
        log_handle = open(log_path, "w", encoding="utf-8")
        
        self.managed_proc = ManagedProcess(
            cmd=cmd,
            name=f"proxy-{suite_name}",
            stdout=log_handle,
            stderr=subprocess.STDOUT
        )
        
        if self.managed_proc.start():
            self._last_log = log_path
            self.current_suite = suite_name
            time.sleep(3.0)
            if not self.managed_proc.is_running():
                log(f"Proxy exited early")
                return False
            return True
        return False
    
    def stop(self):
        """Stop drone proxy"""
        if self.managed_proc:
            self.managed_proc.stop()
            self.managed_proc = None
            self.current_suite = None
    
    def is_running(self) -> bool:
        return self.managed_proc is not None and self.managed_proc.is_running()

# ============================================================
# Suite Runner
# ============================================================

def run_suite(proxy: DroneProxyManager, mavproxy, 
              suite_name: str, duration: float, is_first: bool = False) -> dict:
    """Run a single suite test - drone controls the flow.
    
    NOTE: Even though drone is the controller, GCS proxy must start first
    because the TCP handshake requires GCS to listen and drone to connect.
    Drone controls WHEN to start, but GCS proxy goes up first.
    """
    
    result = {
        "suite": suite_name,
        "status": "unknown",
        "echo_rx": 0,
        "echo_tx": 0,
    }
    
    # Ensure mavproxy (application-layer relay) is available
    mav_running = False
    try:
        # Support either the manager object with is_running(), or a subprocess.Popen
        if hasattr(mavproxy, "is_running"):
            mav_running = bool(mavproxy.is_running())
        else:
            # treat mavproxy as subprocess-like
            mav_running = mavproxy is not None and getattr(mavproxy, "poll", lambda: None)() is None
    except Exception:
        mav_running = False
    
    if not is_first:
        # Rekey: tell GCS to prepare (stop its proxy)
        log("Preparing GCS for rekey...")
        resp = send_gcs_command("prepare_rekey")
        if resp.get("status") != "ok":
            log(f"GCS prepare_rekey failed: {resp}")
            result["status"] = "gcs_prepare_failed"
            return result
        
        # Stop our proxy too
        proxy.stop()
        time.sleep(0.5)
    
    # Tell GCS to start its proxy first (GCS listens, drone connects)
    log(f"Telling GCS to start proxy for {suite_name}...")
    resp = send_gcs_command("start_proxy", suite=suite_name)
    log(f"GCS start_proxy response: {resp}")
    if resp.get("status") != "ok":
        log(f"GCS start_proxy failed: {resp}")
        result["status"] = "gcs_start_failed"
        return result

    # Wait for GCS proxy to be ready by polling status
    log("Waiting for GCS proxy to report ready...")
    start_wait = time.time()
    ready = False
    while time.time() - start_wait < 20.0:
        time.sleep(0.5)
        try:
            st = send_gcs_command("status")
            if st.get("proxy_running"):
                ready = True
                break
        except Exception:
            pass

    if not ready:
        log("GCS proxy did not become ready in time")
        result["status"] = "gcs_not_ready"
        return result
    
    # Now start drone proxy (it will connect to GCS)
    log(f"Starting drone proxy for {suite_name}...")
    if not proxy.start(suite_name):
        result["status"] = "proxy_start_failed"
        # include last log path if available
        try:
            tail = getattr(proxy, "_last_log", None)
            if tail:
                result["log"] = str(tail)
        except Exception:
            pass
        return result
    
    # Wait for handshake
    time.sleep(1.0)
    
    # Tell GCS to start traffic
    log("Telling GCS to start traffic...")
    resp = send_gcs_command("start_traffic", duration=duration)
    if resp.get("status") != "ok":
        log(f"GCS start_traffic failed: {resp}")
        result["status"] = "gcs_traffic_failed"
        return result
    
    log("Traffic started, waiting for completion... (mavproxy relaying MAVLink)")
    
    # Wait for GCS to finish traffic generation
    # Poll GCS status
    traffic_done = False
    start_time = time.time()
    max_wait = duration + 30  # Extra buffer
    
    while time.time() - start_time < max_wait:
        time.sleep(2.0)
        
        # Log mavproxy status periodically
        try:
            log(f"mavproxy running: {mavproxy.is_running()}")
        except Exception:
            pass
        
        # Check GCS status
        status = send_gcs_command("status")
        if status.get("traffic_complete"):
            traffic_done = True
            break
        
        # Check if proxy died
        if not proxy.is_running():
            log("Proxy exited unexpectedly")
            result["status"] = "proxy_exited"
            return result
    
    if not traffic_done:
        log("Traffic did not complete in time")
        result["status"] = "timeout"
        return result
    
    # Indicate mavproxy and proxy status in result
    try:
        if hasattr(mavproxy, "is_running"):
            result["mavproxy_running"] = bool(mavproxy.is_running())
        else:
            result["mavproxy_running"] = mavproxy is not None and getattr(mavproxy, "poll", lambda: 1)() is None
    except Exception:
        result["mavproxy_running"] = False
    result["proxy_running"] = bool(proxy.is_running())
    result["status"] = "pass"
    
    return result


# ============================================================
# Scheduler Class
# ============================================================


class DroneScheduler:
    """Manages persistent MAVProxy and per-suite crypto tunnels."""

    def __init__(self, args, suites):
        self.args = args
        self.suites = suites
        self.policy = LinearLoopPolicy(self.suites)
        self.proxy = DroneProxyManager()
        self.mavproxy_proc = None
        self.current_proxy_proc = None
        
        # Telemetry & Decision Context
        self.telemetry = TelemetryListener(GCS_TELEMETRY_PORT)
        self.context = DecisionContext(self.telemetry)
        
        # Simple GCS client wrapper exposing send_command
        class _GcsClient:
            def send_command(self, cmd, params=None):
                params = params or {}
                try:
                    return send_gcs_command(cmd, **params)
                except Exception as e:
                    return {"status": "error", "message": str(e)}

        self.gcs_client = _GcsClient()

    def wait_for_handshake_completion(self, timeout: float = 10.0) -> bool:
        """Poll for the handshake completion status file."""
        status_file = Path(__file__).resolve().parents[1] / "logs" / "drone_status.json"
        start_time = time.time()
        while time.time() - start_time < timeout:
            if status_file.exists():
                try:
                    with open(status_file, "r") as f:
                        data = json.load(f)
                        if data.get("status") == "handshake_ok":
                            return True
                except Exception:
                    pass
            time.sleep(0.1)
        return False

    def start_persistent_mavproxy(self) -> bool:
        """Start MAVProxy once for the scheduler and keep handle."""
        try:
            python_exe = sys.executable
            master = self.args.mav_master
            out_arg = f"udp:127.0.0.1:{DRONE_PLAIN_TX_PORT}"

            # Interactive mode requested
            # [FIX] Added --daemon to prevent prompt_toolkit crash on Windows/Headless environments
            cmd = [
                python_exe,
                "-m",
                "MAVProxy.mavproxy",
                f"--master={master}",
                f"--out={out_arg}",
                "--nowait",
                "--daemon",
            ]

            ts = time.strftime("%Y%m%d-%H%M%S")
            log_dir = LOGS_DIR
            log_dir.mkdir(parents=True, exist_ok=True)
            log_path = log_dir / f"mavproxy_drone_{ts}.log"
            try:
                fh = open(log_path, "w", encoding="utf-8")
            except Exception:
                fh = subprocess.DEVNULL

            log(f"Starting persistent mavproxy (drone): {' '.join(cmd)} (log: {log_path})")
            
            self.mavproxy_proc = ManagedProcess(
                cmd=cmd,
                name="mavproxy-drone",
                stdout=fh,
                stderr=subprocess.STDOUT,
                new_console=False # Headless for stability
            )
            
            if self.mavproxy_proc.start():
                time.sleep(1.0)
                return self.mavproxy_proc.is_running()
            return False
        except Exception as e:
            log(f"start_persistent_mavproxy exception: {e}")
            return False

    def start_tunnel_for_suite(self, suite_name: str) -> bool:
        return self.proxy.start(suite_name)

    def stop_current_tunnel(self):
        try:
            # stop crypto proxy
            if self.proxy and self.proxy.is_running():
                logging.info("Stopping crypto proxy")
                self.proxy.stop()
        except Exception:
            pass

    def cleanup(self):
        logging.info("--- DroneScheduler CLEANUP START ---")
        try:
            if self.telemetry:
                self.telemetry.stop()
        except Exception:
            pass

        try:
            self.stop_current_tunnel()
        except Exception:
            pass

        try:
            if self.mavproxy_proc:
                logging.info(f"Terminating MAVProxy")
                self.mavproxy_proc.stop()
        except Exception:
            pass

        logging.info("--- DroneScheduler CLEANUP COMPLETE ---")

    def run_scheduler(self):
        def _sigint(sig, frame):
            log("Interrupted; cleaning up and exiting")
            self.cleanup()
            sys.exit(0)

        signal.signal(signal.SIGINT, _sigint)

        # Start Telemetry Listener
        self.telemetry.start()

        # Start MAVProxy once
        ok = self.start_persistent_mavproxy()
        if not ok:
            log("Warning: persistent MAVProxy failed to start; continuing")

        count = 0
        try:
            while True:
                suite_name = self.policy.next_suite()
                duration = self.policy.get_duration()
                log(f"=== Activating Suite: {suite_name} (duration={duration}) ===")

                # Coordinate with GCS: request GCS to start its proxy BEFORE starting local proxy
                try:
                    log(f"Telling GCS to start proxy for {suite_name}...")
                    resp = self.gcs_client.send_command("start_proxy", {"suite": suite_name})
                    if resp.get("status") != "ok":
                        logging.error(f"GCS rejected start_proxy: {resp}")
                        # skip this suite and continue
                        time.sleep(1.0)
                        continue
                    else:
                        log(f"GCS acknowledged start_proxy for {suite_name}")
                except Exception as e:
                    logging.error(f"Failed to command GCS: {e}")
                    time.sleep(1.0)
                    continue

                # Now start local crypto tunnel (drone proxy)
                self.start_tunnel_for_suite(suite_name)
                
                # Wait for handshake to complete before counting duration
                if self.wait_for_handshake_completion(timeout=10.0):
                    log(f"Handshake complete for {suite_name}")
                else:
                    log(f"Warning: Handshake timed out for {suite_name}")

                time.sleep(duration)
                self.stop_current_tunnel()

                count += 1
                if self.args.max_suites and count >= int(self.args.max_suites):
                    break

                time.sleep(2.0)
        except Exception as e:
            logging.error(f"Scheduler crash: {e}")
        finally:
            self.cleanup()


# ============================================================
# Main
# ============================================================

def cleanup_environment():
    """Force kill any stale instances of our components (Linux/Posix)."""
    log("Cleaning up stale processes...")
    patterns = ["mavproxy.py", "core.run_proxy"]
    for p in patterns:
        try:
            # -f matches full command line, ignore exit code (1 if not found)
            subprocess.run(["pkill", "-f", p], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        except Exception as e:
            log(f"Cleanup warning: {e}")
    # Give OS time to reclaim resources
    time.sleep(1.0)

def main():
    parser = argparse.ArgumentParser(description="Drone Scheduler (Controller)")
    parser.add_argument("--mav-master", default=str(CONFIG.get("MAV_MASTER", "/dev/ttyACM0")), help="Primary MAVLink master (e.g. /dev/ttyACM0 or tcp:host:port)")
    parser.add_argument("--suite", default=None, help="Single suite to run")
    parser.add_argument("--nist-level", choices=["L1", "L3", "L5"], help="Run suites for NIST level")
    parser.add_argument("--all", action="store_true", help="Run all suites")
    parser.add_argument("--duration", type=float, default=DEFAULT_DURATION, help="Seconds per suite")
    parser.add_argument("--rate", type=float, default=DEFAULT_RATE_MBPS, help="Traffic rate Mbps")
    parser.add_argument("--max-suites", type=int, default=None, help="Max suites to run")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Simplified Drone Scheduler (CONTROLLER) - sscheduler")
    print("=" * 60)
    # Configuration dump for debugging
    cfg = {
        "DRONE_HOST": DRONE_HOST,
        "GCS_HOST": GCS_HOST,
        "GCS_CONTROL": f"{GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}",
        "PROXY_INTERNAL_CONTROL_PORT": PROXY_INTERNAL_CONTROL_PORT,
        "DRONE_PLAINTEXT_RX": DRONE_PLAIN_RX_PORT,
        "DRONE_PLAINTEXT_TX": DRONE_PLAIN_TX_PORT,
    }
    log("Configuration Dump:")
    for k, v in cfg.items():
        log(f"  {k}: {v}")
    log(f"Duration: {args.duration}s per suite, Rate: {args.rate} Mbps")
    
    # Determine suites to run
    if args.suite:
        suites_to_run = [args.suite]
    elif args.nist_level:
        suites_to_run = [s["name"] for s in SUITES if s.get("nist_level") == args.nist_level]
    elif args.all:
        suites_to_run = [s["name"] for s in SUITES]
    else:
        # Default: run all available suites
        suites_to_run = [s["name"] for s in SUITES]

    if args.max_suites:
        suites_to_run = suites_to_run[:args.max_suites]

    # Register cleanup on exit
    atexit.register(cleanup_environment)

    # Apply local in-file overrides
    if LOCAL_RATE_MBPS is not None:
        args.rate = float(LOCAL_RATE_MBPS)
    if LOCAL_DURATION is not None:
        args.duration = float(LOCAL_DURATION)
    if LOCAL_SUITES:
        suites_to_run = [s for s in LOCAL_SUITES if s in [x["name"] for x in SUITES]]
    if LOCAL_MAX_SUITES:
        suites_to_run = suites_to_run[: int(LOCAL_MAX_SUITES)]

    log(f"Suites to run: {len(suites_to_run)}")

    # Cleanup environment before starting
    cleanup_environment()

    # Initialize components
    scheduler = DroneScheduler(args, suites_to_run)
    # configure logging
    logging.basicConfig(level=logging.INFO)
    scheduler.run_scheduler()

    return 0

if __name__ == "__main__":
    sys.exit(main())

==================================================

sscheduler\sgcs copy 2.py
==================================================
#!/usr/bin/env python3
"""
Simplified GCS Scheduler (FOLLOWER) - sscheduler/sgcs.py

REVERSED CONTROL: GCS follows drone commands.
- GCS has control server, waits for drone commands
- GCS starts its proxy when drone says "start"
- GCS runs traffic generator when commanded
- Drone controls suite order, timing, rekey

Usage:
    python -m sscheduler.sgcs [options]

Environment:
    DRONE_HOST          Drone IP (default: from config)
    GCS_HOST            GCS IP (default: from config)
    GCS_CONTROL_HOST    GCS control server bind IP (default: GCS_HOST)
"""

import os
import sys
import time
import json
import socket
import signal
import argparse
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timezone

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import CONFIG
from core.suites import get_suite, list_suites

# Extract config values (single source of truth)
DRONE_HOST = str(CONFIG.get("DRONE_HOST"))
GCS_HOST = str(CONFIG.get("GCS_HOST"))
GCS_PLAIN_TX_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))
GCS_PLAIN_RX_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))
DRONE_PLAIN_RX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
TCP_CTRL_PORT = CONFIG.get("TCP_HANDSHAKE_PORT")

# ============================================================
# Configuration (derived from CONFIG)
# ============================================================

# Bind control server to 0.0.0.0 so Drone can connect in diverse networks
GCS_CONTROL_HOST = str(CONFIG.get("GCS_CONTROL_BIND_HOST", "0.0.0.0"))
# Use configured GCS control port (default 48080)
GCS_CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", 48080))

# Derived internal proxy control port to avoid collision when ports change
PROXY_INTERNAL_CONTROL_PORT = GCS_CONTROL_PORT + 100

SECRETS_DIR = Path(__file__).parent.parent / "secrets" / "matrix"

# Default traffic settings (can be overridden by drone)
DEFAULT_RATE_MBPS = 110.0
DEFAULT_DURATION = 10.0
PAYLOAD_SIZE = 1200

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# --------------------
LOCAL_RATE_MBPS = None  # e.g. 110.0
LOCAL_DURATION = None  # e.g. 10.0
LOCAL_MAX_SUITES = None
LOCAL_SUITES = None

# Get all suites (list_suites returns dict, convert to list of dicts)
_suites_dict = list_suites()
SUITES = [{"name": k, **v} for k, v in _suites_dict.items()]

# ============================================================
# Logging
# ============================================================

def log(msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [sgcs-follow] {msg}", flush=True)

# ============================================================
# Traffic Generator
# ============================================================

class TrafficGenerator:
    """Generates UDP traffic from GCS to drone"""
    
    def __init__(self, rate_mbps: float = DEFAULT_RATE_MBPS):
        self.rate_mbps = rate_mbps
        self.tx_sock = None
        self.rx_sock = None
        self.running = False
        self.tx_count = 0
        self.rx_count = 0
        self.tx_bytes = 0
        self.rx_bytes = 0
        self.lock = threading.Lock()
        self.complete = False
    
    def start(self, duration: float):
        """Start traffic generation in background thread"""
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)

        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        # Bind receive socket on the GCS plaintext RX port so echoes return here
        self.rx_sock.bind((GCS_HOST, GCS_PLAIN_RX_PORT))
        self.rx_sock.settimeout(1.0)
        
        self.running = True
        self.complete = False
        self.tx_count = 0
        self.rx_count = 0
        self.tx_bytes = 0
        self.rx_bytes = 0
        
        # Start receiver thread
        self.rx_thread = threading.Thread(target=self._receive_loop, daemon=True)
        self.rx_thread.start()
        
        # Start sender thread
        self.tx_thread = threading.Thread(target=self._send_loop, args=(duration,), daemon=True)
        self.tx_thread.start()
        
        log(f"Traffic started: {self.rate_mbps} Mbps for {duration}s")
    
    def _send_loop(self, duration: float):
        """Send packets at target rate"""
        payload = b"X" * PAYLOAD_SIZE
        packets_per_sec = (self.rate_mbps * 1_000_000) / (8 * PAYLOAD_SIZE)
        interval = 1.0 / packets_per_sec
        batch_size = max(1, int(packets_per_sec / 100))  # Send in batches
        batch_interval = interval * batch_size
        
        start_time = time.time()
        end_time = start_time + duration
        
        while time.time() < end_time and self.running:
            batch_start = time.time()
            
            for _ in range(batch_size):
                try:
                    # Send traffic to the Drone's plaintext receive port
                    self.tx_sock.sendto(payload, (DRONE_HOST, DRONE_PLAIN_RX_PORT))
                    with self.lock:
                        self.tx_count += 1
                        self.tx_bytes += len(payload)
                except Exception:
                    pass
            
            # Rate limiting
            elapsed = time.time() - batch_start
            if elapsed < batch_interval:
                time.sleep(batch_interval - elapsed)
        
        self.complete = True
        log(f"Traffic complete: TX={self.tx_count}, RX={self.rx_count}")
    
    def _receive_loop(self):
        """Receive echo responses"""
        while self.running:
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                with self.lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception:
                if self.running:
                    pass
    
    def get_stats(self):
        with self.lock:
            return {
                "tx_count": self.tx_count,
                "rx_count": self.rx_count,
                "tx_bytes": self.tx_bytes,
                "rx_bytes": self.rx_bytes,
                "complete": self.complete,
            }
    
    def stop(self):
        self.running = False
        if hasattr(self, 'tx_thread'):
            self.tx_thread.join(timeout=2.0)
        if hasattr(self, 'rx_thread'):
            self.rx_thread.join(timeout=2.0)
        if self.tx_sock:
            self.tx_sock.close()
        if self.rx_sock:
            self.rx_sock.close()
    
    def is_complete(self):
        return self.complete

# ============================================================
# GCS Proxy Management
# ============================================================

class GcsProxyManager:
    """Manages GCS proxy subprocess"""
    
    def __init__(self):
        self.process = None
        self.current_suite = None
    
    def start(self, suite_name: str) -> bool:
        """Start GCS proxy with given suite"""
        if self.process and self.process.poll() is None:
            self.stop()
        
        suite = get_suite(suite_name)
        if not suite:
            log(f"Unknown suite: {suite_name}")
            return False
        
        secret_dir = SECRETS_DIR / suite_name
        gcs_key = secret_dir / "gcs_signing.key"
        
        if not gcs_key.exists():
            log(f"Missing key: {gcs_key}")
            return False
        
        cmd = [
            sys.executable, "-m", "core.run_proxy", "gcs",
            "--suite", suite_name,
            "--gcs-secret-file", str(gcs_key),
            "--quiet"
        ]
        
        log(f"Launching: {' '.join(cmd)}")
        self.process = subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        self.current_suite = suite_name
        
        # Wait for proxy to initialize
        time.sleep(2.0)
        
        if self.process.poll() is not None:
            log(f"Proxy exited early with code {self.process.returncode}")
            return False
        
        return True
    
    def stop(self):
        """Stop GCS proxy"""
        if self.process:
            self.process.terminate()
            try:
                self.process.wait(timeout=5.0)
            except subprocess.TimeoutExpired:
                self.process.kill()
            self.process = None
            self.current_suite = None
    
    def is_running(self) -> bool:
        return self.process is not None and self.process.poll() is None

# ============================================================
# Control Server (GCS listens for drone commands)
# ============================================================

class ControlServer:
    """TCP control server - GCS listens for commands from drone"""
    
    def __init__(self, proxy: GcsProxyManager):
        self.proxy = proxy
        self.traffic = None
        self.server_sock = None
        self.running = False
        self.thread = None
        self.rate_mbps = DEFAULT_RATE_MBPS
        self.duration = DEFAULT_DURATION
    
    def start(self):
        self.server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.server_sock.bind((GCS_CONTROL_HOST, GCS_CONTROL_PORT))
        self.server_sock.listen(5)
        self.server_sock.settimeout(1.0)
        
        self.running = True
        self.thread = threading.Thread(target=self._server_loop, daemon=True)
        self.thread.start()
        
        log(f"Control server listening on {GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}")
    
    def _server_loop(self):
        while self.running:
            try:
                client, addr = self.server_sock.accept()
                threading.Thread(target=self._handle_client, args=(client, addr), daemon=True).start()
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Server error: {e}")
    
    def _handle_client(self, client: socket.socket, addr):
        try:
            client.settimeout(30.0)
            data = b""
            while b"\n" not in data:
                chunk = client.recv(4096)
                if not chunk:
                    break
                data += chunk
            
            if data:
                request = json.loads(data.decode().strip())
                response = self._handle_command(request)
                client.sendall(json.dumps(response).encode() + b"\n")
        except Exception as e:
            log(f"Client error: {e}")
        finally:
            client.close()
    
    def _handle_command(self, request: dict) -> dict:
        cmd = request.get("cmd", "")
        
        if cmd == "ping":
            return {"status": "ok", "message": "pong", "role": "gcs_follower"}
        
        elif cmd == "status":
            traffic_stats = self.traffic.get_stats() if self.traffic else {}
            return {
                "status": "ok",
                "proxy_running": self.proxy.is_running(),
                "current_suite": self.proxy.current_suite,
                "traffic_complete": traffic_stats.get("complete", False),
                "traffic_stats": traffic_stats,
            }
        
        elif cmd == "configure":
            # Drone tells GCS the traffic parameters
            self.rate_mbps = request.get("rate_mbps", DEFAULT_RATE_MBPS)
            self.duration = request.get("duration", DEFAULT_DURATION)
            log(f"Configured: rate={self.rate_mbps} Mbps, duration={self.duration}s")
            return {"status": "ok", "message": "configured"}
        
        elif cmd == "start":
            # Drone tells GCS to start proxy and begin traffic (combined)
            suite = request.get("suite")
            duration = request.get("duration", self.duration)
            
            if not suite:
                return {"status": "error", "message": "missing suite"}
            
            log(f"Start requested for suite: {suite}")
            
            # Start GCS proxy
            if not self.proxy.start(suite):
                return {"status": "error", "message": "proxy_start_failed"}
            
            # Wait a moment for handshake
            time.sleep(1.0)
            
            # Start traffic generation
            if self.traffic:
                self.traffic.stop()
            
            self.traffic = TrafficGenerator(self.rate_mbps)
            self.traffic.start(duration)
            
            return {"status": "ok", "message": "started"}
        
        elif cmd == "start_proxy":
            # Drone tells GCS to start proxy only (no traffic yet)
            suite = request.get("suite")
            
            if not suite:
                return {"status": "error", "message": "missing suite"}
            
            log(f"Start proxy requested for suite: {suite}")
            
            # Start GCS proxy
            if not self.proxy.start(suite):
                return {"status": "error", "message": "proxy_start_failed"}
            
            return {"status": "ok", "message": "proxy_started"}
        
        elif cmd == "start_traffic":
            # Drone tells GCS to start traffic (proxy already running)
            duration = request.get("duration", self.duration)
            
            if not self.proxy.is_running():
                return {"status": "error", "message": "proxy_not_running"}
            
            log(f"Starting traffic: {self.rate_mbps} Mbps for {duration}s")
            
            # Start traffic generation
            if self.traffic:
                self.traffic.stop()
            
            self.traffic = TrafficGenerator(self.rate_mbps)
            self.traffic.start(duration)
            
            return {"status": "ok", "message": "traffic_started"}
        
        elif cmd == "prepare_rekey":
            # Drone tells GCS to prepare for rekey (stop proxy)
            log("Prepare rekey: stopping proxy...")
            self.proxy.stop()
            
            if self.traffic:
                self.traffic.stop()
                self.traffic = None
            
            return {"status": "ok", "message": "ready_for_rekey"}
        
        elif cmd == "stop":
            log("Stop command received")
            self.proxy.stop()
            
            if self.traffic:
                self.traffic.stop()
                self.traffic = None
            
            return {"status": "ok", "message": "stopped"}
        
        elif cmd == "get_suites":
            return {
                "status": "ok",
                "suites": [s["name"] for s in SUITES],
                "count": len(SUITES),
            }
        
        else:
            return {"status": "error", "message": f"unknown command: {cmd}"}
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.server_sock:
            self.server_sock.close()
        if self.traffic:
            self.traffic.stop()

# ============================================================
# Main
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="GCS Scheduler (Follower)")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Simplified GCS Scheduler (FOLLOWER) - sscheduler")
    print("=" * 60)
    # Configuration dump for debugging
    cfg = {
        "DRONE_HOST": DRONE_HOST,
        "GCS_HOST": GCS_HOST,
        "GCS_CONTROL_BIND": f"{GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}",
        "PROXY_INTERNAL_CONTROL_PORT": PROXY_INTERNAL_CONTROL_PORT,
        "GCS_PLAINTEXT_RX": GCS_PLAIN_RX_PORT,
        "GCS_PLAINTEXT_TX": GCS_PLAIN_TX_PORT,
        "DRONE_PLAINTEXT_RX": DRONE_PLAIN_RX_PORT,
    }
    log("Configuration Dump:")
    for k, v in cfg.items():
        log(f"  {k}: {v}")
    log("GCS scheduler running. Waiting for commands from drone...")
    log("(Drone will send 'start', 'rekey', 'stop' commands)")
    
    # Initialize components
    proxy = GcsProxyManager()
    control = ControlServer(proxy)
    control.start()

    # Apply local in-file overrides for rate/duration if set
    if LOCAL_RATE_MBPS is not None:
        control.rate_mbps = float(LOCAL_RATE_MBPS)
    if LOCAL_DURATION is not None:
        control.duration = float(LOCAL_DURATION)
    
    # Wait for shutdown
    shutdown = threading.Event()
    
    def signal_handler(sig, frame):
        log("Shutdown signal received")
        shutdown.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        while not shutdown.is_set():
            shutdown.wait(timeout=1.0)
    finally:
        log("Shutting down...")
        control.stop()
        proxy.stop()
    
    log("GCS scheduler stopped")
    return 0

if __name__ == "__main__":
    sys.exit(main())

==================================================

sscheduler\sgcs copy.py
==================================================
#!/usr/bin/env python3
"""
Simplified GCS Scheduler (FOLLOWER) - sscheduler/sgcs.py

REVERSED CONTROL: GCS follows drone commands.
- GCS has control server, waits for drone commands
- GCS starts its proxy when drone says "start"
- GCS runs traffic generator when commanded
- Drone controls suite order, timing, rekey

Usage:
    python -m sscheduler.sgcs [options]

Environment:
    DRONE_HOST          Drone IP (default: from config)
    GCS_HOST            GCS IP (default: from config)
    GCS_CONTROL_HOST    GCS control server bind IP (default: GCS_HOST)
"""

import os
import sys
import time
import json
import socket
import signal
import argparse
import threading
import subprocess
from pathlib import Path
from datetime import datetime, timezone

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import CONFIG
from core.suites import get_suite, list_suites

# Extract config values (single source of truth)
DRONE_HOST = str(CONFIG.get("DRONE_HOST"))
GCS_HOST = str(CONFIG.get("GCS_HOST"))
GCS_PLAIN_TX_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))
GCS_PLAIN_RX_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))
DRONE_PLAIN_RX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
TCP_CTRL_PORT = CONFIG.get("TCP_HANDSHAKE_PORT")

# ============================================================
# Configuration (derived from CONFIG)
# ============================================================

# Bind control server to 0.0.0.0 so Drone can connect in diverse networks
GCS_CONTROL_HOST = str(CONFIG.get("GCS_CONTROL_BIND_HOST", "0.0.0.0"))
# Use configured GCS control port (default 48080)
GCS_CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", 48080))

# Derived internal proxy control port to avoid collision when ports change
PROXY_INTERNAL_CONTROL_PORT = GCS_CONTROL_PORT + 100

SECRETS_DIR = Path(__file__).parent.parent / "secrets" / "matrix"

# Default traffic settings (can be overridden by drone)
DEFAULT_RATE_MBPS = 110.0
DEFAULT_DURATION = 10.0
PAYLOAD_SIZE = 1200

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# --------------------
LOCAL_RATE_MBPS = None  # e.g. 110.0
LOCAL_DURATION = None  # e.g. 10.0
LOCAL_MAX_SUITES = None
LOCAL_SUITES = None

# Get all suites (list_suites returns dict, convert to list of dicts)
_suites_dict = list_suites()
SUITES = [{"name": k, **v} for k, v in _suites_dict.items()]

# ============================================================
# Logging
# ============================================================

def log(msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [sgcs-follow] {msg}", flush=True)

# ============================================================
# Traffic Generator
# ============================================================

class TrafficGenerator:
    """Generates UDP traffic from GCS to drone"""
    
    def __init__(self, rate_mbps: float = DEFAULT_RATE_MBPS):
        self.rate_mbps = rate_mbps
        self.tx_sock = None
        self.rx_sock = None
        self.running = False
        self.tx_count = 0
        self.rx_count = 0
        self.tx_bytes = 0
        self.rx_bytes = 0
        self.lock = threading.Lock()
        self.complete = False
    
    def start(self, duration: float):
        """Start traffic generation in background thread"""
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)

        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        # Bind receive socket on the GCS plaintext RX port so echoes return here
        self.rx_sock.bind((GCS_HOST, GCS_PLAIN_RX_PORT))
        self.rx_sock.settimeout(1.0)
        
        self.running = True
        self.complete = False
        self.tx_count = 0
        self.rx_count = 0
        self.tx_bytes = 0
        self.rx_bytes = 0
        
        # Start receiver thread
        self.rx_thread = threading.Thread(target=self._receive_loop, daemon=True)
        self.rx_thread.start()
        
        # Start sender thread
        self.tx_thread = threading.Thread(target=self._send_loop, args=(duration,), daemon=True)
        self.tx_thread.start()
        
        log(f"Traffic started: {self.rate_mbps} Mbps for {duration}s")
    
    def _send_loop(self, duration: float):
        """Send packets at target rate"""
        payload = b"X" * PAYLOAD_SIZE
        packets_per_sec = (self.rate_mbps * 1_000_000) / (8 * PAYLOAD_SIZE)
        interval = 1.0 / packets_per_sec
        batch_size = max(1, int(packets_per_sec / 100))  # Send in batches
        batch_interval = interval * batch_size
        
        start_time = time.time()
        end_time = start_time + duration
        
        while time.time() < end_time and self.running:
            batch_start = time.time()
            
            for _ in range(batch_size):
                try:
                    # Send traffic to the Drone's plaintext receive port
                    self.tx_sock.sendto(payload, (DRONE_HOST, DRONE_PLAIN_RX_PORT))
                    with self.lock:
                        self.tx_count += 1
                        self.tx_bytes += len(payload)
                except Exception:
                    pass
            
            # Rate limiting
            elapsed = time.time() - batch_start
            if elapsed < batch_interval:
                time.sleep(batch_interval - elapsed)
        
        self.complete = True
        log(f"Traffic complete: TX={self.tx_count}, RX={self.rx_count}")
    
    def _receive_loop(self):
        """Receive echo responses"""
        while self.running:
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                with self.lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception:
                if self.running:
                    pass
    
    def get_stats(self):
        with self.lock:
            return {
                "tx_count": self.tx_count,
                "rx_count": self.rx_count,
                "tx_bytes": self.tx_bytes,
                "rx_bytes": self.rx_bytes,
                "complete": self.complete,
            }
    
    def stop(self):
        self.running = False
        if hasattr(self, 'tx_thread'):
            self.tx_thread.join(timeout=2.0)
        if hasattr(self, 'rx_thread'):
            self.rx_thread.join(timeout=2.0)
        if self.tx_sock:
            self.tx_sock.close()
        if self.rx_sock:
            self.rx_sock.close()
    
    def is_complete(self):
        return self.complete

# ============================================================
# GCS Proxy Management
# ============================================================

class GcsProxyManager:
    """Manages GCS proxy subprocess"""
    
    def __init__(self):
        self.process = None
        self.current_suite = None
    
    def start(self, suite_name: str) -> bool:
        """Start GCS proxy with given suite"""
        if self.process and self.process.poll() is None:
            self.stop()
        
        suite = get_suite(suite_name)
        if not suite:
            log(f"Unknown suite: {suite_name}")
            return False
        
        secret_dir = SECRETS_DIR / suite_name
        gcs_key = secret_dir / "gcs_signing.key"
        
        if not gcs_key.exists():
            log(f"Missing key: {gcs_key}")
            return False
        
        cmd = [
            sys.executable, "-m", "core.run_proxy", "gcs",
            "--suite", suite_name,
            "--gcs-secret-file", str(gcs_key),
            "--quiet"
        ]
        
        log(f"Launching: {' '.join(cmd)}")
        self.process = subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        self.current_suite = suite_name
        
        # Wait for proxy to initialize
        time.sleep(2.0)
        
        if self.process.poll() is not None:
            log(f"Proxy exited early with code {self.process.returncode}")
            return False
        
        return True
    
    def stop(self):
        """Stop GCS proxy"""
        if self.process:
            self.process.terminate()
            try:
                self.process.wait(timeout=5.0)
            except subprocess.TimeoutExpired:
                self.process.kill()
            self.process = None
            self.current_suite = None
    
    def is_running(self) -> bool:
        return self.process is not None and self.process.poll() is None

# ============================================================
# Control Server (GCS listens for drone commands)
# ============================================================

class ControlServer:
    """TCP control server - GCS listens for commands from drone"""
    
    def __init__(self, proxy: GcsProxyManager):
        self.proxy = proxy
        self.traffic = None
        self.server_sock = None
        self.running = False
        self.thread = None
        self.rate_mbps = DEFAULT_RATE_MBPS
        self.duration = DEFAULT_DURATION
    
    def start(self):
        self.server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.server_sock.bind((GCS_CONTROL_HOST, GCS_CONTROL_PORT))
        self.server_sock.listen(5)
        self.server_sock.settimeout(1.0)
        
        self.running = True
        self.thread = threading.Thread(target=self._server_loop, daemon=True)
        self.thread.start()
        
        log(f"Control server listening on {GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}")
    
    def _server_loop(self):
        while self.running:
            try:
                client, addr = self.server_sock.accept()
                threading.Thread(target=self._handle_client, args=(client, addr), daemon=True).start()
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Server error: {e}")
    
    def _handle_client(self, client: socket.socket, addr):
        try:
            client.settimeout(30.0)
            data = b""
            while b"\n" not in data:
                chunk = client.recv(4096)
                if not chunk:
                    break
                data += chunk
            
            if data:
                request = json.loads(data.decode().strip())
                response = self._handle_command(request)
                client.sendall(json.dumps(response).encode() + b"\n")
        except Exception as e:
            log(f"Client error: {e}")
        finally:
            client.close()
    
    def _handle_command(self, request: dict) -> dict:
        cmd = request.get("cmd", "")
        
        if cmd == "ping":
            return {"status": "ok", "message": "pong", "role": "gcs_follower"}
        
        elif cmd == "status":
            traffic_stats = self.traffic.get_stats() if self.traffic else {}
            return {
                "status": "ok",
                "proxy_running": self.proxy.is_running(),
                "current_suite": self.proxy.current_suite,
                "traffic_complete": traffic_stats.get("complete", False),
                "traffic_stats": traffic_stats,
            }
        
        elif cmd == "configure":
            # Drone tells GCS the traffic parameters
            self.rate_mbps = request.get("rate_mbps", DEFAULT_RATE_MBPS)
            self.duration = request.get("duration", DEFAULT_DURATION)
            log(f"Configured: rate={self.rate_mbps} Mbps, duration={self.duration}s")
            return {"status": "ok", "message": "configured"}
        
        elif cmd == "start":
            # Drone tells GCS to start proxy and begin traffic (combined)
            suite = request.get("suite")
            duration = request.get("duration", self.duration)
            
            if not suite:
                return {"status": "error", "message": "missing suite"}
            
            log(f"Start requested for suite: {suite}")
            
            # Start GCS proxy
            if not self.proxy.start(suite):
                return {"status": "error", "message": "proxy_start_failed"}
            
            # Wait a moment for handshake
            time.sleep(1.0)
            
            # Start traffic generation
            if self.traffic:
                self.traffic.stop()
            
            self.traffic = TrafficGenerator(self.rate_mbps)
            self.traffic.start(duration)
            
            return {"status": "ok", "message": "started"}
        
        elif cmd == "start_proxy":
            # Drone tells GCS to start proxy only (no traffic yet)
            suite = request.get("suite")
            
            if not suite:
                return {"status": "error", "message": "missing suite"}
            
            log(f"Start proxy requested for suite: {suite}")
            
            # Start GCS proxy
            if not self.proxy.start(suite):
                return {"status": "error", "message": "proxy_start_failed"}
            
            return {"status": "ok", "message": "proxy_started"}
        
        elif cmd == "start_traffic":
            # Drone tells GCS to start traffic (proxy already running)
            duration = request.get("duration", self.duration)
            
            if not self.proxy.is_running():
                return {"status": "error", "message": "proxy_not_running"}
            
            log(f"Starting traffic: {self.rate_mbps} Mbps for {duration}s")
            
            # Start traffic generation
            if self.traffic:
                self.traffic.stop()
            
            self.traffic = TrafficGenerator(self.rate_mbps)
            self.traffic.start(duration)
            
            return {"status": "ok", "message": "traffic_started"}
        
        elif cmd == "prepare_rekey":
            # Drone tells GCS to prepare for rekey (stop proxy)
            log("Prepare rekey: stopping proxy...")
            self.proxy.stop()
            
            if self.traffic:
                self.traffic.stop()
                self.traffic = None
            
            return {"status": "ok", "message": "ready_for_rekey"}
        
        elif cmd == "stop":
            log("Stop command received")
            self.proxy.stop()
            
            if self.traffic:
                self.traffic.stop()
                self.traffic = None
            
            return {"status": "ok", "message": "stopped"}
        
        elif cmd == "get_suites":
            return {
                "status": "ok",
                "suites": [s["name"] for s in SUITES],
                "count": len(SUITES),
            }
        
        else:
            return {"status": "error", "message": f"unknown command: {cmd}"}
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.server_sock:
            self.server_sock.close()
        if self.traffic:
            self.traffic.stop()

# ============================================================
# Main
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="GCS Scheduler (Follower)")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Simplified GCS Scheduler (FOLLOWER) - sscheduler")
    print("=" * 60)
    # Configuration dump for debugging
    cfg = {
        "DRONE_HOST": DRONE_HOST,
        "GCS_HOST": GCS_HOST,
        "GCS_CONTROL_BIND": f"{GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}",
        "PROXY_INTERNAL_CONTROL_PORT": PROXY_INTERNAL_CONTROL_PORT,
        "GCS_PLAINTEXT_RX": GCS_PLAIN_RX_PORT,
        "GCS_PLAINTEXT_TX": GCS_PLAIN_TX_PORT,
        "DRONE_PLAINTEXT_RX": DRONE_PLAIN_RX_PORT,
    }
    log("Configuration Dump:")
    for k, v in cfg.items():
        log(f"  {k}: {v}")
    log("GCS scheduler running. Waiting for commands from drone...")
    log("(Drone will send 'start', 'rekey', 'stop' commands)")
    
    # Initialize components
    proxy = GcsProxyManager()
    control = ControlServer(proxy)
    control.start()

    # Apply local in-file overrides for rate/duration if set
    if LOCAL_RATE_MBPS is not None:
        control.rate_mbps = float(LOCAL_RATE_MBPS)
    if LOCAL_DURATION is not None:
        control.duration = float(LOCAL_DURATION)
    
    # Wait for shutdown
    shutdown = threading.Event()
    
    def signal_handler(sig, frame):
        log("Shutdown signal received")
        shutdown.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        while not shutdown.is_set():
            shutdown.wait(timeout=1.0)
    finally:
        log("Shutting down...")
        control.stop()
        proxy.stop()
    
    log("GCS scheduler stopped")
    return 0

if __name__ == "__main__":
    sys.exit(main())

==================================================

sscheduler\sgcs.py
==================================================
#!/usr/bin/env python3
"""
Simplified GCS Scheduler (FOLLOWER) - sscheduler/sgcs.py

REVERSED CONTROL: GCS follows drone commands.
- GCS has control server, waits for drone commands
- GCS starts its proxy when drone says "start"
- GCS runs traffic generator when commanded
- Drone controls suite order, timing, rekey

Usage:
    python -m sscheduler.sgcs [options]

Environment:
    DRONE_HOST          Drone IP (default: from config)
    GCS_HOST            GCS IP (default: from config)
    GCS_CONTROL_HOST    GCS control server bind IP (default: GCS_HOST)
"""

import os
import sys
import time
import json
import socket
import signal
import argparse
import threading
import subprocess
import atexit
from pathlib import Path
from datetime import datetime, timezone

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import CONFIG
from core.suites import get_suite, list_suites
from core.process import ManagedProcess
from tools.mavproxy_manager import MavProxyManager
from sscheduler.gcs_metrics import GcsMetricsCollector

# Extract config values (single source of truth)
DRONE_HOST = str(CONFIG.get("DRONE_HOST"))
GCS_HOST = str(CONFIG.get("GCS_HOST"))
GCS_PLAIN_TX_PORT = int(CONFIG.get("GCS_PLAINTEXT_TX", 47001))
GCS_PLAIN_RX_PORT = int(CONFIG.get("GCS_PLAINTEXT_RX", 47002))
DRONE_PLAIN_RX_PORT = int(CONFIG.get("DRONE_PLAINTEXT_RX", 47004))
GCS_TELEMETRY_PORT = int(CONFIG.get("GCS_TELEMETRY_PORT", 52080))
GCS_TELEMETRY_SNIFF_PORT = 14552
TCP_CTRL_PORT = CONFIG.get("TCP_HANDSHAKE_PORT")

# ============================================================
# Configuration (derived from CONFIG)
# ============================================================

# Bind control server to 0.0.0.0 so Drone can connect in diverse networks
GCS_CONTROL_HOST = str(CONFIG.get("GCS_CONTROL_BIND_HOST", "0.0.0.0"))
# Use configured GCS control port (default 48080)
GCS_CONTROL_PORT = int(CONFIG.get("GCS_CONTROL_PORT", 48080))

# Derived internal proxy control port to avoid collision when ports change
PROXY_INTERNAL_CONTROL_PORT = GCS_CONTROL_PORT + 100

SECRETS_DIR = Path(__file__).parent.parent / "secrets" / "matrix"

# Default traffic settings (can be overridden by drone)
DEFAULT_RATE_MBPS = 110.0
DEFAULT_DURATION = 10.0
PAYLOAD_SIZE = 1200

# --------------------
# Local editable configuration (edit here, no CLI args needed)
# --------------------
LOCAL_RATE_MBPS = None  # e.g. 110.0
LOCAL_DURATION = None  # e.g. 10.0
LOCAL_MAX_SUITES = None
LOCAL_SUITES = None

# Get all suites (list_suites returns dict, convert to list of dicts)
_suites_dict = list_suites()
SUITES = [{"name": k, **v} for k, v in _suites_dict.items()]

# ============================================================
# Logging
# ============================================================

def log(msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{ts}] [sgcs-follow] {msg}", flush=True)

def wait_for_tcp_port(port: int, timeout: float = 5.0) -> bool:
    """Wait for a local TCP port to be listening."""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            with socket.create_connection(("127.0.0.1", port), timeout=0.5):
                return True
        except (ConnectionRefusedError, OSError, socket.timeout):
            time.sleep(0.2)
    return False

# ============================================================
# Traffic Generator
# ============================================================

class TrafficGenerator:
    """Generates UDP traffic from GCS to drone"""
    
    def __init__(self, rate_mbps: float = DEFAULT_RATE_MBPS):
        self.rate_mbps = rate_mbps
        self.tx_sock = None
        self.rx_sock = None
        self.running = False
        self.tx_count = 0
        self.rx_count = 0
        self.tx_bytes = 0
        self.rx_bytes = 0
        self.lock = threading.Lock()
        self.complete = False
    
    def start(self, duration: float):
        """Start traffic generation in background thread"""
        self.tx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.tx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 4 * 1024 * 1024)

        self.rx_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.rx_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 4 * 1024 * 1024)
        # Bind receive socket on the GCS plaintext RX port so echoes return here
        self.rx_sock.bind((GCS_HOST, GCS_PLAIN_RX_PORT))
        self.rx_sock.settimeout(1.0)
        
        self.running = True
        self.complete = False
        self.tx_count = 0
        self.rx_count = 0
        self.tx_bytes = 0
        self.rx_bytes = 0
        
        # Start receiver thread
        self.rx_thread = threading.Thread(target=self._receive_loop, daemon=True)
        self.rx_thread.start()
        
        # Start sender thread
        self.tx_thread = threading.Thread(target=self._send_loop, args=(duration,), daemon=True)
        self.tx_thread.start()
        
        log(f"Traffic started: {self.rate_mbps} Mbps for {duration}s")
    
    def _send_loop(self, duration: float):
        """Send packets at target rate"""
        payload = b"X" * PAYLOAD_SIZE
        packets_per_sec = (self.rate_mbps * 1_000_000) / (8 * PAYLOAD_SIZE)
        interval = 1.0 / packets_per_sec
        batch_size = max(1, int(packets_per_sec / 100))  # Send in batches
        batch_interval = interval * batch_size
        
        start_time = time.time()
        end_time = start_time + duration
        
        while time.time() < end_time and self.running:
            batch_start = time.time()
            
            for _ in range(batch_size):
                try:
                    # Send traffic to the Drone's plaintext receive port
                    self.tx_sock.sendto(payload, (DRONE_HOST, DRONE_PLAIN_RX_PORT))
                    with self.lock:
                        self.tx_count += 1
                        self.tx_bytes += len(payload)
                except Exception:
                    pass
            
            # Rate limiting
            elapsed = time.time() - batch_start
            if elapsed < batch_interval:
                time.sleep(batch_interval - elapsed)
        
        self.complete = True
        log(f"Traffic complete: TX={self.tx_count}, RX={self.rx_count}")
    
    def _receive_loop(self):
        """Receive echo responses"""
        while self.running:
            try:
                data, addr = self.rx_sock.recvfrom(65535)
                with self.lock:
                    self.rx_count += 1
                    self.rx_bytes += len(data)
            except socket.timeout:
                continue
            except Exception:
                if self.running:
                    pass
    
    def get_stats(self):
        with self.lock:
            return {
                "tx_count": self.tx_count,
                "rx_count": self.rx_count,
                "tx_bytes": self.tx_bytes,
                "rx_bytes": self.rx_bytes,
                "complete": self.complete,
            }
    
    def stop(self):
        self.running = False
        if hasattr(self, 'tx_thread'):
            self.tx_thread.join(timeout=2.0)
        if hasattr(self, 'rx_thread'):
            self.rx_thread.join(timeout=2.0)
        if self.tx_sock:
            self.tx_sock.close()
        if self.rx_sock:
            self.rx_sock.close()
    
    def is_complete(self):
        return self.complete

# ============================================================
# GCS Proxy Management
# ============================================================

class GcsProxyManager:
    """Manages GCS proxy subprocess"""
    
    def __init__(self):
        self.managed_proc = None
        self.current_suite = None
    
    def start(self, suite_name: str) -> bool:
        """Start GCS proxy with given suite"""
        if self.managed_proc and self.managed_proc.is_running():
            self.stop()
        
        suite = get_suite(suite_name)
        if not suite:
            log(f"Unknown suite: {suite_name}")
            return False
        
        secret_dir = SECRETS_DIR / suite_name
        gcs_key = secret_dir / "gcs_signing.key"
        
        if not gcs_key.exists():
            log(f"Missing key: {gcs_key}")
            return False
        
        cmd = [
            sys.executable, "-m", "core.run_proxy", "gcs",
            "--suite", suite_name,
            "--gcs-secret-file", str(gcs_key),
            "--quiet"
        ]
        
        log(f"Launching: {' '.join(cmd)}")
        self.managed_proc = ManagedProcess(
            cmd=cmd,
            name=f"proxy-{suite_name}",
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        
        if self.managed_proc.start():
            self.current_suite = suite_name
            time.sleep(2.0)
            if not self.managed_proc.is_running():
                log(f"Proxy exited early")
                return False
            return True
        return False
    
    def stop(self):
        """Stop GCS proxy"""
        if self.managed_proc:
            self.managed_proc.stop()
            self.managed_proc = None
            self.current_suite = None
    
    def is_running(self) -> bool:
        return self.managed_proc is not None and self.managed_proc.is_running()


# MavProxyManager imported from tools.mavproxy_manager


# ============================================================
# Telemetry Sender
# ============================================================

class TelemetrySender:
    """Sends telemetry updates to the Drone via UDP (Fire-and-Forget)"""
    def __init__(self, target_host: str, target_port: int):
        self.target_addr = (target_host, target_port)
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.seq = 0
        self.lock = threading.Lock()

    def send(self, packet: dict):
        """Send a telemetry packet (Schema v1)"""
        with self.lock:
            self.seq += 1
            packet["seq"] = self.seq
        
        try:
            payload = json.dumps(packet).encode('utf-8')
            self.sock.sendto(payload, self.target_addr)
        except Exception:
            # Fire and forget
            pass

    def close(self):
        self.sock.close()

# ============================================================
# Control Server (GCS listens for drone commands)
# ============================================================

class ControlServer:
    """TCP control server - GCS listens for commands from drone"""
    
    def __init__(self, proxy: GcsProxyManager):
        self.proxy = proxy
        self.traffic = None
        self.mavproxy = MavProxyManager("gcs")
        # Persistent mavproxy subprocess handle (if started here)
        self.mavproxy_proc = None
        self.server_sock = None
        self.running = False
        self.thread = None
        self.rate_mbps = DEFAULT_RATE_MBPS
        self.duration = DEFAULT_DURATION
        
        # Telemetry
        self.telemetry = TelemetrySender(DRONE_HOST, GCS_TELEMETRY_PORT)
        self.telemetry_thread = None
        
        # Metrics Collector
        self.metrics_collector = GcsMetricsCollector(
            mavlink_host="127.0.0.1",
            mavlink_port=GCS_TELEMETRY_SNIFF_PORT,
            proxy_manager=self.proxy,
            log_dir=Path(__file__).parent.parent / "logs" / "gcs_telemetry"
        )

    def start_persistent_mavproxy(self):
        """Start a persistent mavproxy subprocess for the lifetime of the scheduler.

        Uses `sys.executable -m MAVProxy.mavproxy` where possible so Windows/sudo
        environments resolve correctly.
        """
        try:
            bind_host = str(CONFIG.get("GCS_PLAINTEXT_BIND", "0.0.0.0"))
            listen_port = int(CONFIG.get("GCS_PLAINTEXT_RX", GCS_PLAIN_RX_PORT))
            tunnel_out_port = int(CONFIG.get("GCS_PLAINTEXT_TX", GCS_PLAIN_TX_PORT))
            QGC_PORT = int(CONFIG.get("QGC_PORT", 14550))

            master_str = f"udpin:{bind_host}:{listen_port}"
            # out_arg = f"udp:127.0.0.1:{tunnel_out_port}"

            # Prefer module invocation to avoid PATH issues on Windows
            python_exe = sys.executable
            
            # Interactive mode requested: Remove --daemon and use CREATE_NEW_CONSOLE on Windows
            # Removed --out to proxy to prevent loops; rely on reply-to-sender from proxy
            # [FIX] Removed --daemon, added --map --console for interactive GUI
            # Added telemetry sniff port output
            cmd = [
                python_exe, "-m", "MAVProxy.mavproxy", 
                f"--master={master_str}", 
                "--dialect=ardupilotmega", 
                "--nowait", 
                "--map", 
                "--console", 
                f"--out=udp:127.0.0.1:{QGC_PORT}",
                f"--out=udp:127.0.0.1:{GCS_TELEMETRY_SNIFF_PORT}"
            ]

            log(f"Starting persistent mavproxy: {' '.join(cmd)}")

            log_dir = Path(__file__).resolve().parents[1] / "logs" / "sscheduler" / "gcs"
            log_dir.mkdir(parents=True, exist_ok=True)
            ts_now = time.strftime("%Y%m%d-%H%M%S")
            log_path = log_dir / f"mavproxy_gcs_{ts_now}.log"
            try:
                fh = open(log_path, "w", encoding="utf-8")
            except Exception:
                fh = subprocess.DEVNULL

            stdout_arg = fh
            stderr_arg = subprocess.STDOUT
            
            if sys.platform == "win32":
                # On Windows, redirecting stdout breaks prompt_toolkit even with new_console=True
                stdout_arg = None
                stderr_arg = None
            
            # Add TERM=dumb to environment to avoid prompt_toolkit crash on Windows
            env = os.environ.copy()
            env["TERM"] = "dumb"

            stdin_arg = subprocess.DEVNULL
            if sys.platform == "win32":
                stdin_arg = None # Allow inheritance/detachment for interactive console

            self.mavproxy_proc = ManagedProcess(
                cmd=cmd,
                name="mavproxy-gcs",
                stdout=stdout_arg,
                stderr=stderr_arg,
                stdin=stdin_arg,
                new_console=True, # Windows requires a console for prompt_toolkit
                env=env
            )
            
            if self.mavproxy_proc.start():
                # Update metrics collector with process handle
                self.metrics_collector.mavproxy_proc = self.mavproxy_proc
                
                if wait_for_tcp_port(TCP_CTRL_PORT, timeout=5.0):
                    log("Persistent mavproxy started (port open)")
                    return True
                elif self.mavproxy_proc.is_running():
                    log("Persistent mavproxy started (process running, but port not yet ready)")
                    return True
                else:
                    log("Persistent mavproxy failed to start")
                    return False
            return False
        except Exception as e:
            log(f"start_persistent_mavproxy exception: {e}")
            return False

    def start(self):
        self.server_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.server_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.server_sock.bind((GCS_CONTROL_HOST, GCS_CONTROL_PORT))
        self.server_sock.listen(5)
        self.server_sock.settimeout(1.0)
        
        self.running = True
        self.thread = threading.Thread(target=self._server_loop, daemon=True)
        self.thread.start()
        
        # Start metrics collector
        self.metrics_collector.start()
        
        # Start telemetry loop
        self.telemetry_thread = threading.Thread(target=self._telemetry_loop, daemon=True)
        self.telemetry_thread.start()
        
        log(f"Control server listening on {GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}")
    
    def _server_loop(self):
        while self.running:
            try:
                client, addr = self.server_sock.accept()
                threading.Thread(target=self._handle_client, args=(client, addr), daemon=True).start()
            except socket.timeout:
                continue
            except Exception as e:
                if self.running:
                    log(f"Server error: {e}")

    def _telemetry_loop(self):
        """Periodically send status to drone"""
        while self.running:
            try:
                # Get latest metrics snapshot
                snapshot = self.metrics_collector.get_snapshot()
                self.telemetry.send(snapshot)
            except Exception:
                pass
            
            time.sleep(0.2)

    def _handle_client(self, client, addr):
        try:
            data = b""
            while True:
                chunk = client.recv(4096)
                if not chunk:
                    break
                data += chunk
                # Assuming simple one-shot JSON command
                if len(data) > 0 and (data.strip().endswith(b"}") or b"\n" in data):
                     break
            
            if data:
                try:
                    request = json.loads(data.decode().strip())
                    response = self._handle_command(request)
                    client.sendall(json.dumps(response).encode() + b"\n")
                except json.JSONDecodeError:
                    pass
        except Exception as e:
            log(f"Client error: {e}")
        finally:
            client.close()
    
    def _handle_command(self, request: dict) -> dict:
        cmd = request.get("cmd", "")
        
        if cmd == "ping":
            return {"status": "ok", "message": "pong", "role": "gcs_follower"}
        
        elif cmd == "status":
            traffic_stats = self.traffic.get_stats() if self.traffic else {}
            return {
                "status": "ok",
                "proxy_running": self.proxy.is_running(),
                "current_suite": self.proxy.current_suite,
                "traffic_complete": traffic_stats.get("complete", False),
                "traffic_stats": traffic_stats,
            }
        
        elif cmd == "configure":
            # Drone tells GCS the traffic parameters
            self.rate_mbps = request.get("rate_mbps", DEFAULT_RATE_MBPS)
            self.duration = request.get("duration", DEFAULT_DURATION)
            log(f"Configured: rate={self.rate_mbps} Mbps, duration={self.duration}s")
            return {"status": "ok", "message": "configured"}
        
        elif cmd == "start":
            # Drone tells GCS to start proxy and begin traffic (combined)
            suite = request.get("suite")
            duration = request.get("duration", self.duration)
            
            if not suite:
                return {"status": "error", "message": "missing suite"}
            
            log(f"Start requested for suite: {suite}")
            
            # Start GCS proxy
            if not self.proxy.start(suite):
                return {"status": "error", "message": "proxy_start_failed"}
            
            # Wait a moment for handshake
            time.sleep(1.0)
            
            # Do NOT spawn a new mavproxy here. MAVProxy should be persistent.
            log("Traffic start requested (MAVProxy is already running)")
            # Check persistent mavproxy health
            if not (self.mavproxy_proc and self.mavproxy_proc.poll() is None):
                return {"status": "error", "message": "mavproxy_not_running"}
            return {"status": "ok", "message": "started"}
        
        elif cmd == "start_proxy":
            # Drone tells GCS to start proxy only (no traffic yet)
            suite = request.get("suite")
            
            if not suite:
                return {"status": "error", "message": "missing suite"}
            
            log(f"Start proxy requested for suite: {suite}")
            
            # Start GCS proxy
            if not self.proxy.start(suite):
                return {"status": "error", "message": "proxy_start_failed"}

            # Persistent MAVProxy should already be running; just acknowledge
            log("Proxy started; persistent MAVProxy assumed running")
            return {"status": "ok", "message": "proxy_started"}
        
        elif cmd == "start_traffic":
            # Drone tells GCS to start traffic (proxy already running)
            duration = request.get("duration", self.duration)
            
            if not self.proxy.is_running():
                return {"status": "error", "message": "proxy_not_running"}
            
            log(f"Starting traffic: {self.rate_mbps} Mbps for {duration}s")
            
            # With persistent MAVProxy there is nothing to spawn here.
            log("Traffic start requested (MAVProxy is already running)")
            if not (self.mavproxy_proc and self.mavproxy_proc.poll() is None):
                return {"status": "error", "message": "mavproxy_not_running"}
            return {"status": "ok", "message": "traffic_started"}
            
            return {"status": "ok", "message": "traffic_started"}
        
        elif cmd == "prepare_rekey":
            # Drone tells GCS to prepare for rekey (stop proxy)
            log("Prepare rekey: stopping proxy...")
            self.proxy.stop()
            
            # DO NOT stop persistent MAVProxy here. It should keep running.
            # if self.mavproxy_proc: ...
            
            if self.traffic:
                try:
                    self.traffic.stop()
                except Exception:
                    pass
                self.traffic = None
            
            return {"status": "ok", "message": "ready_for_rekey"}
        
        elif cmd == "stop":
            log("Stop command received")
            self.proxy.stop()
            # Stop mavproxy and any traffic generator wrapper
            if self.mavproxy_proc:
                try:
                    self.mavproxy_proc.terminate()
                except Exception:
                    pass
                self.mavproxy_proc = None

            if self.traffic:
                try:
                    self.traffic.stop()
                except Exception:
                    pass
                self.traffic = None
            
            return {"status": "ok", "message": "stopped"}
        
        elif cmd == "get_suites":
            return {
                "status": "ok",
                "suites": [s["name"] for s in SUITES],
            }
        
        return {"status": "error", "message": f"unknown command: {cmd}"}
    
    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2.0)
        if self.telemetry_thread:
            self.telemetry_thread.join(timeout=2.0)
        if self.metrics_collector:
            self.metrics_collector.stop()
        if self.telemetry:
            self.telemetry.close()
        if self.server_sock:
            self.server_sock.close()
        if self.traffic:
            try:
                self.traffic.stop()
            except Exception:
                pass
        if self.mavproxy_proc:
            try:
                self.mavproxy_proc.stop()
            except Exception:
                pass
            self.mavproxy_proc = None

# ============================================================
# Main
# ============================================================

def cleanup_environment():
    """Force kill any stale instances of our components."""
    log("Cleaning up stale processes...")
    
    # Current PID to avoid suicide (though unlikely to match targets)
    my_pid = os.getpid()
    
    targets = ["mavproxy", "core.run_proxy"]
    
    if sys.platform.startswith("win"):
        # Windows: Use taskkill for known PIDs if we tracked them, but here we are cleaning up *stale* ones.
        # WMIC is slow but effective for pattern matching.
        for t in targets:
            # Clause: name='python.exe' AND commandline like '%target%' AND ProcessId != my_pid
            query = f"name='python.exe' and commandline like '%{t}%' and ProcessId!={my_pid}"
            cmd = f'wmic process where "{query}" call terminate'
            try:
                subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            except Exception:
                pass
    else:
        # Linux/Posix
        for t in targets:
             subprocess.run(["pkill", "-f", t], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
             
    time.sleep(1.0)

# Register cleanup on exit
atexit.register(cleanup_environment)

def main():
    parser = argparse.ArgumentParser(description="GCS Scheduler (Follower)")
    args = parser.parse_args()
    
    print("=" * 60)
    print("Simplified GCS Scheduler (FOLLOWER) - sscheduler")
    print("=" * 60)
    # Configuration dump for debugging
    cfg = {
        "DRONE_HOST": DRONE_HOST,
        "GCS_HOST": GCS_HOST,
        "GCS_CONTROL_BIND": f"{GCS_CONTROL_HOST}:{GCS_CONTROL_PORT}",
        "PROXY_INTERNAL_CONTROL_PORT": PROXY_INTERNAL_CONTROL_PORT,
        "GCS_PLAINTEXT_RX": GCS_PLAIN_RX_PORT,
        "GCS_PLAINTEXT_TX": GCS_PLAIN_TX_PORT,
        "DRONE_PLAINTEXT_RX": DRONE_PLAIN_RX_PORT,
    }
    log("Configuration Dump:")
    for k, v in cfg.items():
        log(f"  {k}: {v}")
    log("GCS scheduler running. Waiting for commands from drone...")
    log("(Drone will send 'start', 'rekey', 'stop' commands)")
    
    # Cleanup environment before starting
    cleanup_environment()

    # Initialize components
    proxy = GcsProxyManager()
    control = ControlServer(proxy)
    control.start()

    # Start persistent MAVProxy for the scheduler lifetime
    try:
        ok = control.start_persistent_mavproxy()
        if ok:
            log("persistent mavproxy started at scheduler startup")
        else:
            log("persistent mavproxy failed to start at scheduler startup")
    except Exception as _e:
        log(f"persistent mavproxy startup exception: {_e}")

    # Apply local in-file overrides for rate/duration if set
    if LOCAL_RATE_MBPS is not None:
        control.rate_mbps = float(LOCAL_RATE_MBPS)
    if LOCAL_DURATION is not None:
        control.duration = float(LOCAL_DURATION)
    
    # Wait for shutdown
    shutdown = threading.Event()
    
    def signal_handler(sig, frame):
        log("Shutdown signal received")
        shutdown.set()
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        while not shutdown.is_set():
            shutdown.wait(timeout=1.0)
    finally:
        log("Shutting down...")
        control.stop()
        proxy.stop()
    
    log("GCS scheduler stopped")
    return 0

if __name__ == "__main__":
    sys.exit(main())

==================================================

sscheduler\__init__.py
==================================================
# sscheduler - Drone-controlled scheduler
# Reversed control: Drone schedules, GCS follows

==================================================

tests\test_lifecycle.py
==================================================
import sys
import time
import os
import subprocess
from pathlib import Path

# Add root to path
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from core.process import ManagedProcess

def test_lifecycle():
    print("--- Testing ManagedProcess Lifecycle ---")
    
    # 1. Start a sleeper process
    cmd = [sys.executable, "-c", "import time; print('Child running'); time.sleep(30); print('Child done')"]
    
    print(f"Starting child: {cmd}")
    proc = ManagedProcess(cmd, name="test-sleeper", stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    
    if not proc.start():
        print("FAILED: Could not start process")
        return False
    
    print(f"Child started with PID: {proc.process.pid}")
    
    # 2. Verify running
    time.sleep(1.0)
    if not proc.is_running():
        print("FAILED: Process died immediately")
        stdout, stderr = proc.process.communicate()
        print(f"STDOUT: {stdout}")
        print(f"STDERR: {stderr}")
        return False
    
    print("Child is running...")
    
    # 3. Stop
    print("Stopping child...")
    start_stop = time.time()
    proc.stop()
    duration = time.time() - start_stop
    
    # 4. Verify stopped
    if proc.is_running():
        print("FAILED: Process still running after stop()")
        return False
    
    print(f"Child stopped successfully in {duration:.2f}s")
    return True

if __name__ == "__main__":
    if test_lifecycle():
        print("TEST PASSED")
        sys.exit(0)
    else:
        print("TEST FAILED")
        sys.exit(1)

==================================================

tools\blackout_metrics.py
==================================================
from __future__ import annotations

import csv
import math
from pathlib import Path
from typing import Dict, List, Optional


def _read_marks(path: Path) -> List[Dict[str, object]]:
    rows: List[Dict[str, object]] = []
    if not path.exists():
        return rows
    try:
        with path.open("r", encoding="utf-8", newline="") as handle:
            reader = csv.reader(handle)
            for row in reader:
                if not row or row[0] in {"kind", ""}:
                    continue
                kind = row[0].strip().lower()
                try:
                    ts_val = int(row[1])
                except (IndexError, ValueError):
                    continue
                entry: Dict[str, object] = {"kind": kind, "ts": ts_val, "raw": row}
                rows.append(entry)
    except Exception:
        return []
    return rows


def _read_packets(path: Path) -> List[Dict[str, int]]:
    packets: List[Dict[str, int]] = []
    if not path.exists():
        return packets
    try:
        with path.open("r", encoding="utf-8", newline="") as handle:
            reader = csv.reader(handle)
            header = next(reader, None)
            recv_idx = 0
            proc_idx = 2
            if header:
                try:
                    recv_idx = header.index("recv_timestamp_ns")
                except ValueError:
                    recv_idx = 0
                try:
                    proc_idx = header.index("processing_ns")
                except ValueError:
                    proc_idx = 2
            for row in reader:
                try:
                    recv_ns = int(row[recv_idx])
                except (IndexError, ValueError):
                    continue
                proc_ns = 0
                try:
                    proc_ns = int(row[proc_idx])
                except (IndexError, ValueError):
                    proc_ns = 0
                packets.append({"recv_ns": recv_ns, "proc_ns": proc_ns})
    except Exception:
        return []
    packets.sort(key=lambda item: item["recv_ns"])
    return packets


def _percentile(values: List[float], pct: float) -> Optional[float]:
    if not values:
        return None
    if len(values) == 1:
        return values[0]
    ordered = sorted(values)
    rank = pct * (len(ordered) - 1)
    lower = int(math.floor(rank))
    upper = int(math.ceil(rank))
    if lower == upper:
        return ordered[lower]
    fraction = rank - lower
    return ordered[lower] + fraction * (ordered[upper] - ordered[lower])


def _find_mark_pair(
    marks: List[Dict[str, object]],
    window_start: int,
    window_end: int,
) -> Optional[Dict[str, int]]:
    current_start: Optional[Dict[str, object]] = None
    pairs: List[Dict[str, int]] = []
    for entry in marks:
        kind = entry.get("kind")
        if kind == "start":
            current_start = entry
        elif kind == "end" and current_start:
            start_ts = int(current_start.get("ts", 0))
            end_ts = int(entry.get("ts", 0))
            pairs.append({"start": start_ts, "end": end_ts})
            current_start = None
    candidate = None
    for pair in pairs:
        if pair["start"] >= window_start and pair["end"] <= window_end:
            if candidate is None or pair["start"] > candidate["start"]:
                candidate = pair
    if candidate:
        return candidate
    if pairs:
        return pairs[-1]
    return None


def _rate_kpps(packets: List[Dict[str, int]]) -> Optional[float]:
    if len(packets) < 2:
        return None
    duration_ns = packets[-1]["recv_ns"] - packets[0]["recv_ns"]
    if duration_ns <= 0:
        return None
    rate_pps = len(packets) / (duration_ns / 1_000_000_000)
    return rate_pps / 1000.0


def compute_blackout(
    session_dir: Path,
    t_mark_ns: int,
    t_ok_ns: int,
) -> Dict[str, Optional[float]]:
    packets = _read_packets(session_dir / "packet_timing.csv")
    mark_candidates = sorted(session_dir.glob("rekey_marks_*.csv"))
    marks_path = mark_candidates[-1] if mark_candidates else session_dir / "rekey_marks.csv"
    marks = _read_marks(marks_path)
    if not packets:
        return {"blackout_ms": None, "gap_max_ms": None}
    window_start = t_mark_ns - 2_000_000_000
    window_end = t_ok_ns + 2_000_000_000
    window_packets = [pkt for pkt in packets if window_start <= pkt["recv_ns"] <= window_end]
    if len(window_packets) < 3:
        return {"blackout_ms": None, "gap_max_ms": None}
    gaps = [
        (window_packets[i]["recv_ns"] - window_packets[i - 1]["recv_ns"]) / 1_000_000
        for i in range(1, len(window_packets))
    ]
    gap_max = max(gaps)
    gap_p99 = _percentile(gaps, 0.99)
    pre_start = t_mark_ns - 3_000_000_000
    pre_end = t_mark_ns - 500_000_000
    pre_packets = [pkt for pkt in packets if pre_start <= pkt["recv_ns"] < pre_end]
    pre_gaps = [
        (pre_packets[i]["recv_ns"] - pre_packets[i - 1]["recv_ns"]) / 1_000_000
        for i in range(1, len(pre_packets))
    ]
    steady_gap = _percentile(pre_gaps, 0.95) or 0.0
    blackout = max(0.0, gap_max - steady_gap)
    post_end = t_ok_ns + 3_000_000_000
    post_packets = [pkt for pkt in packets if t_ok_ns <= pkt["recv_ns"] <= post_end]
    recv_rate_before = _rate_kpps(pre_packets)
    recv_rate_after = _rate_kpps(post_packets)
    proc_values = [pkt["proc_ns"] for pkt in window_packets if pkt["proc_ns"] > 0]
    proc_p95 = _percentile([val for val in proc_values], 0.95)
    pair = _find_mark_pair(marks, window_start, window_end)
    result: Dict[str, Optional[float]] = {
        "blackout_ms": round(blackout, 3),
        "gap_max_ms": round(gap_max, 3),
        "steady_gap_ms": round(steady_gap, 3) if steady_gap is not None else None,
        "gap_p99_ms": round(gap_p99, 3) if gap_p99 is not None else None,
        "recv_rate_kpps_before": round(recv_rate_before, 3) if recv_rate_before is not None else None,
        "recv_rate_kpps_after": round(recv_rate_after, 3) if recv_rate_after is not None else None,
        "proc_ns_p95": round(proc_p95, 3) if proc_p95 is not None else None,
    }
    if pair:
        result["pair_start_ns"] = pair.get("start")
        result["pair_end_ns"] = pair.get("end")
    return result

==================================================

tools\mavproxy_manager.py
==================================================
#!/usr/bin/env python3
"""Shared MavProxyManager for launching mavproxy subprocesses.

Keeps a minimal API: `start(listen_host, listen_port, peer_host, peer_port) -> bool`,
`stop()`, and `is_running()`.
"""
import signal
from pathlib import Path
import time
import subprocess
from typing import Optional
import sys
import os
from pathlib import Path as _Path

from core.config import CONFIG
from core.process import ManagedProcess


ROOT = Path(__file__).resolve().parents[1]

def _logs_dir_for(role: str) -> Path:
    d = ROOT / "logs" / "sscheduler" / role
    d.mkdir(parents=True, exist_ok=True)
    return d


class MavProxyManager:
    def __init__(self, role: str = "generic") -> None:
        self.role = role
        self.managed_proc: Optional[ManagedProcess] = None
        self._last_log: Optional[Path] = None

    def start(self, master_str_or_listen_host, master_baud_or_listen_port, out_ip=None, out_port=None, extra_args=None) -> bool:
        """Start mavproxy using ManagedProcess."""
        # Backwards compatibility: old callers passed (listen_host, listen_port, peer_host, peer_port)
        if out_ip is None and out_port is None:
            # interpret as old-style
            listen_host = str(master_str_or_listen_host)
            listen_port = int(master_baud_or_listen_port)
            peer_host = None
            peer_port = None
            # We'll require caller to pass peer via extra_args in this case, but try to be helpful
            master_str = f"udpin:{listen_host}:{listen_port}"
            out_ip = "127.0.0.1"
            out_port = listen_port  # fallback
        else:
            master_str = str(master_str_or_listen_host)

        master_baud = master_baud_or_listen_port

        if extra_args is None:
            extra_args = []

        # Determine configured binary or fallback name
        configured = CONFIG.get("MAVPROXY_BINARY")
        # Build base out argument
        out_arg = f"udp:{out_ip}:{int(out_port)}"

        # 1. Determine the path to the python interpreter currently running
        python_exe = sys.executable

        # 2. Find mavproxy.py relative to the python executable
        bin_dir = os.path.dirname(python_exe)
        mavproxy_script = os.path.join(bin_dir, "mavproxy.py")

        # 3. Fallbacks
        if os.path.exists(mavproxy_script):
            base_cmd = [python_exe, mavproxy_script]
        elif configured and _Path(str(configured)).exists() and str(configured).lower().endswith(".py"):
            # If CONFIG points to an explicit .py file, use it via sys.executable
            base_cmd = [python_exe, str(configured)]
        elif sys.platform.startswith("win"):
            # Windows fallback: run as module
            base_cmd = [python_exe, "-m", "MAVProxy.mavproxy"]
        else:
            # Linux / general fallback: rely on executable in PATH
            base_cmd = ["mavproxy.py"]

        # 4. Construct full command with master/out and recommended flags
        cmd = base_cmd + [f"--master={master_str}", f"--out={out_arg}", "--dialect=ardupilotmega", "--nowait"]

        # append any extra args verbatim
        if extra_args:
            cmd.extend(extra_args)

        log_dir = _logs_dir_for(self.role)
        ts_now = time.strftime("%Y%m%d-%H%M%S")
        log_path = log_dir / f"mavproxy_{self.role}_{ts_now}.log"
        
        try:
            # On Windows, we might want a new console for interactive use if requested.
            # But for stability, we default to headless unless debugging.
            # The previous code tried to use CREATE_NEW_CONSOLE on Windows.
            # ManagedProcess supports new_console=True.
            
            # Determine if we want a console. Usually yes for GCS, maybe for Drone.
            # But for "secure-tunnel" stability, headless is safer.
            # Let's stick to headless (redirected logs) for now to ensure we capture output.
            
            log_fh = open(log_path, "w", encoding="utf-8")
            
            stdout_arg = log_fh
            stderr_arg = subprocess.STDOUT
            
            if sys.platform == "win32":
                stdout_arg = None
                stderr_arg = None

            # Add TERM=dumb to environment to avoid prompt_toolkit crash on Windows
            env = os.environ.copy()
            env["TERM"] = "dumb"

            self.managed_proc = ManagedProcess(
                cmd=cmd,
                name=f"mavproxy-{self.role}",
                stdout=stdout_arg,
                stderr=stderr_arg,
                new_console=True, # Windows requires a console
                env=env
            )
            
            if self.managed_proc.start():
                self._last_log = log_path
                time.sleep(0.5)
                if not self.managed_proc.is_running():
                    return False
                return True
            return False
            
        except Exception:
            return False

    def stop(self) -> None:
        if self.managed_proc:
            self.managed_proc.stop()
            self.managed_proc = None

    def is_running(self) -> bool:
        return self.managed_proc is not None and self.managed_proc.is_running()

    def last_log(self) -> Optional[Path]:
        return self._last_log

==================================================

tools\merge_power.py
==================================================
from __future__ import annotations

from typing import Any, Dict


def extract_power_fields(status: Dict[str, Any]) -> Dict[str, Any]:
    summary = status.get("last_summary") or {}
    return {
        "energy_j": summary.get("energy_j"),
        "avg_power_w": summary.get("avg_power_w"),
        "duration_s": summary.get("duration_s"),
        "summary_json_path": summary.get("summary_json_path") or summary.get("csv_path"),
    }

==================================================

tools\net_diag.py
==================================================
#!/usr/bin/env python3
"""
Unified Network Diagnostic Tool for Secure Tunnel
Usage: python tools/net_diag.py

Automatically detects role (Drone vs GCS) based on hostname or IP configuration.
Performs:
0. Clock Synchronization Check: Measures time offset between peers.
1. Firewall/Binding Check: Can we bind to our configured RX ports (UDP & TCP)?
2. Reachability Check: Can we reach the peer's IP?
3. Bidirectional UDP Flow Test:
   - Binds all configured ports (Plaintext RX, Encrypted RX, Control RX)
   - Sends test packets to the peer's expected ports
   - Listens for echoes/responses
   - Reports packet loss and latency
4. TCP Control Port Check:
   - GCS binds/listens on Control Port.
   - Drone attempts to connect to Control Port.

This script is standalone and does NOT use the proxy code. It validates the *physical network layer*
and *OS firewall rules* before the complex crypto tunnel is started.
"""

import sys
import socket
import time
import threading
import json
import platform
import subprocess
import select
from pathlib import Path

# Add parent to path to load config
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from core.config import CONFIG
except ImportError:
    print("Error: Could not import core.config. Run from repository root.")
    sys.exit(1)

# Configuration Constants
TIMEOUT = 2.0

def get_local_ips():
    ips = []
    try:
        # Hack to get preferred outbound IP
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ips.append(s.getsockname()[0])
        s.close()
    except Exception:
        pass
    # Get all interfaces
    try:
        hostname = socket.gethostname()
        for ip in socket.gethostbyname_ex(hostname)[2]:
            if ip not in ips:
                ips.append(ip)
    except Exception:
        pass
    return ips

def detect_role():
    """Heuristic to decide if we are Drone or GCS based on CONFIG IPs and local interfaces."""
    local_ips = get_local_ips()
    drone_ip = CONFIG["DRONE_HOST"]
    gcs_ip = CONFIG["GCS_HOST"]
    
    print(f"[*] Local IPs: {local_ips}")
    print(f"[*] Configured Drone IP: {drone_ip}")
    print(f"[*] Configured GCS IP:   {gcs_ip}")

    if drone_ip in local_ips:
        return "drone"
    if gcs_ip in local_ips:
        return "gcs"
    
    # Fallback: Ask user if ambiguous
    print("[!] Could not auto-detect role from IP match.")
    if sys.platform == "linux": # Assumption: Drone is usually Linux (Pi)
        return "drone"
    return "gcs" # Assumption: GCS is usually Windows

class PortTester:
    def __init__(self, role):
        self.role = role
        self.peer_ip = CONFIG["GCS_HOST"] if role == "drone" else CONFIG["DRONE_HOST"]
        self.sockets = {}
        self.running = True
        self.lock = threading.Lock()
        self.log_msgs = []
        self.tcp_listener = None

    def log(self, msg):
        with self.lock:
            print(msg)
            self.log_msgs.append(msg)

    def check_firewall_binding(self, port, desc, proto="UDP"):
        """Try to bind to a port to verify no other process is using it and firewall allows binding."""
        try:
            if proto == "UDP":
                s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
                s.bind(("0.0.0.0", port))
            else:
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                s.bind(("0.0.0.0", port))
                if desc == "Control TCP": # Keep listener open for test
                    s.listen(1)
                    return s
            
            self.log(f"[PASS] Bind {desc} ({proto}/{port}): OK")
            return s
        except OSError as e:
            self.log(f"[FAIL] Bind {desc} ({proto}/{port}): FAILED ({e})")
            self.log(f"       -> Check if proxy/mavproxy is already running or firewall blocks it.")
            return None

    def start_listener(self, sock, name):
        def _listen():
            while self.running:
                r, _, _ = select.select([sock], [], [], 0.5)
                if r:
                    try:
                        data, addr = sock.recvfrom(4096)
                        now = time.time()
                        
                        if b"DIAG_PING:" in data:
                            try:
                                # Parse timestamp
                                parts = data.split(b":")
                                if len(parts) >= 2:
                                    remote_ts = float(parts[1])
                                    diff = now - remote_ts
                                    self.log(f"[RECV] {name} <- {addr}: Clock Delta={diff:.4f}s")
                                    if abs(diff) > 1.0:
                                        self.log(f"       [WARN] Large clock skew detected! (>1.0s)")
                            except Exception:
                                pass
                            
                            # Echo back
                            resp = b"DIAG_PONG:" + str(now).encode()
                            sock.sendto(resp, addr)
                            
                        elif b"DIAG_PONG:" in data:
                            try:
                                parts = data.split(b":")
                                if len(parts) >= 2:
                                    remote_ts = float(parts[1])
                                    rtt = now - remote_ts # This is actually one-way from remote to here if we trust their clock, 
                                                          # but for PONG it's just a signal.
                                    # Real RTT requires us to track when WE sent the PING.
                                    # For simplicity, just confirm receipt.
                                    self.log(f"[ECHO] {name} <- {addr}: Round-trip confirmed!")
                            except Exception:
                                pass
                    except Exception as e:
                        pass
        t = threading.Thread(target=_listen, daemon=True)
        t.start()

    def start_tcp_acceptor(self):
        """Accept TCP connections for Control Port test (GCS only)"""
        def _accept():
            while self.running:
                try:
                    r, _, _ = select.select([self.tcp_listener], [], [], 0.5)
                    if r:
                        conn, addr = self.tcp_listener.accept()
                        self.log(f"[TCP] Accepted connection from {addr}")
                        conn.close()
                except Exception:
                    pass
        t = threading.Thread(target=_accept, daemon=True)
        t.start()

    def check_tcp_connect(self, port, desc):
        """Try to connect to peer TCP port (Drone only)"""
        target = (self.peer_ip, port)
        self.log(f"[TCP] Connecting to {desc} {target}...")
        try:
            s = socket.create_connection(target, timeout=2.0)
            self.log(f"[PASS] Connect {desc} (TCP/{port}): OK")
            s.close()
        except Exception as e:
            self.log(f"[FAIL] Connect {desc} (TCP/{port}): {e}")

    def run_diagnostics(self):
        print("="*60)
        print(f"Network Diagnostic Tool - Role: {self.role.upper()}")
        print("="*60)

        # 1. Define Ports based on Role
        if self.role == "drone":
            my_rx_ports = {
                "Encrypted RX": CONFIG["UDP_DRONE_RX"],
                "Plaintext RX": CONFIG["DRONE_PLAINTEXT_RX"],
            }
            peer_tx_ports = {
                "Encrypted TX": CONFIG["UDP_GCS_RX"],
                "Plaintext TX": CONFIG["GCS_PLAINTEXT_RX"], 
            }
            # Drone connects to GCS Control
            tcp_connect_ports = {
                "Control Port": CONFIG.get("GCS_CONTROL_PORT", 48080)
            }
            tcp_bind_ports = {} # Drone doesn't bind TCP usually, except maybe internal
        else: # GCS
            my_rx_ports = {
                "Encrypted RX": CONFIG["UDP_GCS_RX"],
                "Plaintext RX": CONFIG["GCS_PLAINTEXT_RX"],
            }
            peer_tx_ports = {
                "Encrypted TX": CONFIG["UDP_DRONE_RX"],
                "Plaintext TX": CONFIG["DRONE_PLAINTEXT_RX"],
            }
            # GCS binds Control Port
            tcp_bind_ports = {
                "Control TCP": CONFIG.get("GCS_CONTROL_PORT", 48080)
            }
            tcp_connect_ports = {}

        # 2. Bind Check & Listeners
        print("\n--- Phase 1: Local Port Binding & Firewall Check ---")
        
        # UDP Binds
        for name, port in my_rx_ports.items():
            s = self.check_firewall_binding(port, name, proto="UDP")
            if s:
                self.sockets[name] = s
                self.start_listener(s, name)
        
        # TCP Binds (GCS)
        for name, port in tcp_bind_ports.items():
            s = self.check_firewall_binding(port, name, proto="TCP")
            if s and name == "Control TCP":
                self.tcp_listener = s
                self.start_tcp_acceptor()
            elif s:
                s.close() # Just a bind check for others

        # 3. Ping Check
        print("\n--- Phase 2: ICMP Reachability ---")
        param = "-n" if platform.system().lower() == "windows" else "-c"
        cmd = ["ping", param, "1", self.peer_ip]
        try:
            ret = subprocess.call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            if ret == 0:
                self.log(f"[PASS] Ping {self.peer_ip}: OK")
            else:
                self.log(f"[WARN] Ping {self.peer_ip}: UNREACHABLE (ICMP might be blocked)")
        except Exception:
            self.log("[WARN] Ping command failed")

        # 4. Active Packet Test (UDP + TCP Connect)
        print("\n--- Phase 3: Active Packet Injection & Clock Check ---")
        print(f"Sending test packets to {self.peer_ip}...")
        
        payload = f"DIAG_PING:{time.time()}".encode()

        # UDP Send
        if "Encrypted RX" in self.sockets:
            sock = self.sockets["Encrypted RX"]
            target_port = peer_tx_ports["Encrypted TX"]
            self.log(f"[SEND] Encrypted Path -> {self.peer_ip}:{target_port}")
            sock.sendto(payload, (self.peer_ip, target_port))

        if "Plaintext RX" in self.sockets:
            sock = self.sockets["Plaintext RX"]
            target_host = CONFIG["GCS_PLAINTEXT_HOST"] if self.role == "gcs" else CONFIG["DRONE_PLAINTEXT_HOST"]
            target_port = CONFIG["GCS_PLAINTEXT_TX"] if self.role == "gcs" else CONFIG["DRONE_PLAINTEXT_TX"]
            
            self.log(f"[SEND] Plaintext Loopback -> {target_host}:{target_port}")
            try:
                sock.sendto(payload, (target_host, target_port))
            except Exception as e:
                self.log(f"[FAIL] Plaintext Send: {e}")

        # TCP Connect (Drone)
        for name, port in tcp_connect_ports.items():
            self.check_tcp_connect(port, name)

        print("\nWaiting for echoes (5s)...")
        time.sleep(5)
        self.running = False
        print("\n--- Diagnostics Complete ---")

if __name__ == "__main__":
    role = detect_role()
    tester = PortTester(role)
    tester.run_diagnostics()

==================================================

tools\power_utils.py
==================================================
"""Utility helpers for power trace analysis on the GCS."""
from __future__ import annotations

import csv
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Iterator, List, Optional, Sequence, Tuple


_TS_FIELDS = ("timestamp_ns", "ts_ns", "time_ns", "timestamp", "ts")
_POWER_FIELDS = ("power_w", "power", "power_watts", "watts")
_CURRENT_FIELDS = ("current_a", "current", "amps", "i_a")
_VOLTAGE_FIELDS = ("voltage_v", "voltage", "volts", "v_v")
_SIGN_FIELDS = ("sign", "sign_factor", "sign_multiplier", "direction")


@dataclass(frozen=True)
class PowerSample:
    """Single power reading expressed in nanoseconds and Watts."""

    ts_ns: int
    power_w: float


def _normalize(field: str) -> str:
    return field.strip().lower().replace("-", "_")


def _detect_header(row: Sequence[str]) -> bool:
    if not row:
        return False
    lowered = [_normalize(cell) for cell in row]
    if any(name in lowered for name in _TS_FIELDS + _POWER_FIELDS):
        return True
    # Heuristic: if any cell contains alphabetic characters, treat as header.
    for cell in row:
        if any(ch.isalpha() for ch in cell):
            return True
    return False


def _row_to_sample(row: Sequence[str], headers: Sequence[str]) -> Optional[PowerSample]:
    mapping = {headers[idx]: value for idx, value in enumerate(row)}
    ts_value: Optional[int] = None
    for field in _TS_FIELDS:
        raw = mapping.get(field)
        if raw is None or raw == "":
            continue
        try:
            ts_value = int(raw)
            break
        except ValueError:
            continue
    if ts_value is None:
        return None

    power_value: Optional[float] = None
    for field in _POWER_FIELDS:
        raw = mapping.get(field)
        if raw is None or raw == "":
            continue
        try:
            power_value = float(raw)
            break
        except ValueError:
            continue

    if power_value is None:
        current = None
        voltage = None
        for field in _CURRENT_FIELDS:
            raw = mapping.get(field)
            if raw is None or raw == "":
                continue
            try:
                current = float(raw)
                break
            except ValueError:
                continue
        for field in _VOLTAGE_FIELDS:
            raw = mapping.get(field)
            if raw is None or raw == "":
                continue
            try:
                voltage = float(raw)
                break
            except ValueError:
                continue
        if current is not None and voltage is not None:
            power_value = current * voltage

    if power_value is None:
        return None

    sign_multiplier = 1.0
    for field in _SIGN_FIELDS:
        raw = mapping.get(field)
        if raw is None or raw == "":
            continue
        try:
            sign_multiplier = float(raw)
            break
        except ValueError:
            continue

    return PowerSample(ts_ns=ts_value, power_w=power_value * sign_multiplier)


def load_power_trace(csv_path: str | Path) -> List[PowerSample]:
    """Load a power CSV and return chronologically sorted samples.

    The loader is tolerant to optional headers and derives ``power_w`` from
    voltage/current columns when an explicit power column is absent.
    """

    path = Path(csv_path)
    if not path.exists():
        raise FileNotFoundError(path)

    samples: List[PowerSample] = []
    with path.open("r", encoding="utf-8", newline="") as handle:
        reader = csv.reader(handle)
        try:
            first_row = next(reader)
        except StopIteration:
            return []

        has_header = _detect_header(first_row)
        headers: List[str]
        data_rows: List[Sequence[str]]
        if has_header:
            headers = [_normalize(cell) for cell in first_row]
            data_rows = list(reader)
        else:
            headers = [f"col_{idx}" for idx in range(len(first_row))]
            data_rows = [first_row] + list(reader)

        # Ensure canonical header names exist even for header-less files.
        if not any(name in headers for name in _TS_FIELDS):
            headers = list(headers)
            headers[0] = "timestamp_ns"
        if not any(name in headers for name in _POWER_FIELDS):
            if len(headers) > 3:
                headers[3] = "power_w"

        for row in data_rows:
            if not row:
                continue
            sample = _row_to_sample(row, headers)
            if sample is not None:
                samples.append(sample)

    samples.sort(key=lambda item: item.ts_ns)
    return samples


def slice_window(samples: Sequence[PowerSample], start_ns: int, end_ns: int) -> List[PowerSample]:
    """Return samples that overlap the window ``[start_ns, end_ns]``."""

    if end_ns <= start_ns:
        return []
    window: List[PowerSample] = []
    for sample in samples:
        if sample.ts_ns < start_ns:
            continue
        if sample.ts_ns > end_ns:
            break
        window.append(sample)
    return window


def integrate_energy_mj(samples: Sequence[PowerSample], start_ns: int, end_ns: int) -> Tuple[float, int]:
    """Integrate energy for ``[start_ns, end_ns]`` using trapezoidal rule."""

    if end_ns <= start_ns:
        return 0.0, 0
    if not samples:
        return 0.0, 0

    total_j = 0.0
    used_segments = 0
    prev: Optional[PowerSample] = None

    for sample in samples:
        if prev is None:
            prev = sample
            continue
        if sample.ts_ns <= prev.ts_ns:
            prev = sample
            continue

        segment_start = max(start_ns, prev.ts_ns)
        segment_end = min(end_ns, sample.ts_ns)
        if segment_end > segment_start:
            span = sample.ts_ns - prev.ts_ns
            ratio_start = (segment_start - prev.ts_ns) / span if span else 0.0
            ratio_end = (segment_end - prev.ts_ns) / span if span else 0.0
            p_start = prev.power_w + (sample.power_w - prev.power_w) * ratio_start
            p_end = prev.power_w + (sample.power_w - prev.power_w) * ratio_end
            dt = (segment_end - segment_start) / 1_000_000_000.0
            total_j += 0.5 * (p_start + p_end) * dt
            used_segments += 1
        if sample.ts_ns >= end_ns:
            break
        prev = sample

    return total_j * 1000.0, used_segments


def align_gcs_to_drone(ts_gcs_ns: int, offset_ns: int) -> int:
    """Convert a GCS timestamp into the drone clock domain."""

    return ts_gcs_ns + offset_ns


def calculate_transient_energy(power_csv_path: str, start_ns: int, end_ns: int) -> float:
    """Backward compatible helper used by legacy callers."""

    samples = load_power_trace(power_csv_path)
    energy_mj, _ = integrate_energy_mj(samples, start_ns, end_ns)
    return energy_mj

==================================================

tools\__init__.py
==================================================
# tools package init (kept minimal to allow imports)

==================================================

